{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16ddccf",
   "metadata": {},
   "source": [
    "# Fine-tune a small LM on a customer-support dataset (Local MacBook)\n",
    "\n",
    "This notebook is adapted to run on a local MacBook (CPU or Apple MPS). It uses a small model (`distilgpt2`) by default and is conservative with batch sizes/epochs so it can run locally for demonstration. If the public dataset is unavailable the notebook falls back to a tiny synthetic dataset.\n",
    "\n",
    "### How to use\n",
    "1. (Optional) In the first code cell uncomment the `!pip install` line to install dependencies.\n",
    "2. Adjust settings in the `User settings` cell (model, dataset, epochs, batch_size).\n",
    "3. Run cells top-to-bottom. Training on CPU/MPS is slow; expect longer runtimes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8154db2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (4.4.1)\n",
      "Requirement already satisfied: accelerate in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (1.11.0)\n",
      "Requirement already satisfied: peft in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (0.17.1)\n",
      "Requirement already satisfied: bitsandbytes in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (0.42.0)\n",
      "Requirement already satisfied: trl in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (0.25.1)\n",
      "Requirement already satisfied: filelock in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from accelerate) (2.9.0)\n",
      "Requirement already satisfied: scipy in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from bitsandbytes) (1.16.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: setuptools in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers datasets accelerate peft bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f295ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Prevent tokenizers parallelism warnings & deadlocks\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "import torch\n",
    "# If using MPS or CPU, pin_memory should be False to avoid warnings\n",
    "# We'll set a flag used in TrainingArguments below\n",
    "use_pin_memory = False if (not torch.cuda.is_available()) else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34002140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps (model dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "# Device selection (robust)\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# Choose dtype depending on device to avoid unsupported dtypes on CPU\n",
    "if device == 'cuda' or device == 'mps':\n",
    "    model_dtype = torch.float16\n",
    "else:\n",
    "    model_dtype = torch.float32\n",
    "\n",
    "print(f'Using device: {device} (model dtype={model_dtype})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d520af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      " MODEL_ID=microsoft/Phi-3-mini-4k-instruct\n",
      " DATASET_ID=bitext/Bitext-customer-support-llm-chatbot-training-dataset\n",
      " OUTPUT_DIR=./local_ft_output\n",
      " EPOCHS=3, BATCH_SIZE=1, MAX_LENGTH=256, USE_LORA=True\n"
     ]
    }
   ],
   "source": [
    "# ----------------- User settings (adjust before run) -----------------\n",
    "MODEL_ID = os.environ.get('MODEL_ID', 'microsoft/Phi-3-mini-4k-instruct') # CHANGED\n",
    "DATASET_ID = os.environ.get('DATASET_ID', 'bitext/Bitext-customer-support-llm-chatbot-training-dataset')\n",
    "OUTPUT_DIR = os.environ.get('OUTPUT_DIR', './local_ft_output') # CHANGED OUTPUT DIR to avoid mixing checkpoints\n",
    "EPOCHS = int(os.environ.get('EPOCHS', '3'))\n",
    "BATCH_SIZE = int(os.environ.get('BATCH_SIZE', '1')) # Adjusted BATCH_SIZE for larger model\n",
    "MAX_LENGTH = int(os.environ.get('MAX_LENGTH', '256')) # Increased length for modern model\n",
    "USE_LORA = os.environ.get('USE_LORA', 'true').lower() in ('1', 'true', 'yes')\n",
    "\n",
    "print('Settings:')\n",
    "print(f' MODEL_ID={MODEL_ID}')\n",
    "print(f' DATASET_ID={DATASET_ID}')\n",
    "print(f' OUTPUT_DIR={OUTPUT_DIR}')\n",
    "print(f' EPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}, MAX_LENGTH={MAX_LENGTH}, USE_LORA={USE_LORA}')\n",
    "# --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d02fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def safe_load_customer_dataset(dataset_id):\n",
    "    try:\n",
    "        ds = load_dataset(dataset_id)\n",
    "        if isinstance(ds, dict) and 'train' in ds:\n",
    "            return ds['train']\n",
    "        return ds\n",
    "    except Exception as e:\n",
    "        print(f'Could not load dataset {dataset_id}: {e}')\n",
    "        print('Falling back to a tiny synthetic customer support dataset for demo.')\n",
    "        samples = [\n",
    "            {'customer': \"My order hasn't arrived, it's been 10 days.\", 'agent': \"I'm sorry. Can you share your order id?\"},\n",
    "            {'customer': 'I was charged twice for the same order.', 'agent': \"I can help. Please share the transaction id.\"},\n",
    "            {'customer': 'How do I return an item?', 'agent': \"You can start a return from your orders page.\"},\n",
    "        ]\n",
    "        return Dataset.from_list(samples)\n",
    "\n",
    "def build_prompt(row):\n",
    "    if 'customer' in row and 'agent' in row:\n",
    "        return f\"Human: {row['customer']}\\nAssistant: {row['agent']}\\n\"\n",
    "    if 'input' in row and 'output' in row:\n",
    "        return f\"Human: {row['input']}\\nAssistant: {row['output']}\\n\"\n",
    "    if 'text' in row:\n",
    "        return row['text'] + \"\\n\"\n",
    "    return str(row)\n",
    "\n",
    "print('Helper functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9be4cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset size: 26872 (showing first 2 examples)\n",
      "\n",
      "--- example 0 ---\n",
      "flags\n",
      "\n",
      "--- example 1 ---\n",
      "instruction\n",
      "\n",
      "--- example 2 ---\n",
      "category\n",
      "\n",
      "--- example 3 ---\n",
      "intent\n",
      "\n",
      "--- example 4 ---\n",
      "response\n",
      "Train size: 4096, Eval size: 128\n"
     ]
    }
   ],
   "source": [
    "raw_ds = safe_load_customer_dataset(DATASET_ID)\n",
    "print(f'Loaded dataset size: {len(raw_ds)} (showing first 2 examples)')\n",
    "for i,ex in enumerate(raw_ds[:2]):\n",
    "    print('\\n--- example', i, '---')\n",
    "    print(ex)\n",
    "\n",
    "# Map to text prompts\n",
    "if isinstance(raw_ds[0], dict):\n",
    "    def map_to_prompt(example):\n",
    "        return {'text': build_prompt(example)}\n",
    "    ds = raw_ds.map(map_to_prompt)\n",
    "else:\n",
    "    ds = raw_ds.map(lambda x: {'text': str(x)})\n",
    "\n",
    "# Split and reduce for local run\n",
    "if len(ds) > 2000:\n",
    "    ds = ds.train_test_split(test_size=0.05, shuffle=True, seed=42)\n",
    "    train_ds = ds['train'].select(range(4096))\n",
    "    eval_ds = ds['test'].select(range(128))\n",
    "else:\n",
    "    split = ds.train_test_split(test_size=0.1, seed=42)\n",
    "    train_ds = split['train']\n",
    "    eval_ds = split['test']\n",
    "\n",
    "print(f'Train size: {len(train_ds)}, Eval size: {len(eval_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00f04d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping dataset to single 'text' column using TEMPLATE = A\n",
      "Mapping complete. Columns: ['flags', 'instruction', 'category', 'intent', 'response', 'text']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Prompt template setup (inserted) ---\n",
    "TEMPLATE = \"A\"  # \"A\" = Human/Assistant, \"B\" = Instruction/Input/Response\n",
    "\n",
    "def build_prompt_for_training(row):\n",
    "    if TEMPLATE == \"A\":\n",
    "        if isinstance(row, dict) and 'customer' in row and 'agent' in row:\n",
    "            prompt = f\"Human: {row['customer']}\\nAssistant: {row['agent']}\\n\"\n",
    "        elif isinstance(row, dict) and 'input' in row and 'output' in row:\n",
    "            prompt = f\"Human: {row['input']}\\nAssistant: {row['output']}\\n\"\n",
    "        elif isinstance(row, dict) and 'text' in row:\n",
    "            prompt = row['text']\n",
    "        else:\n",
    "            prompt = str(row)\n",
    "    else:\n",
    "        instruction = row.get('instruction', \"Answer the customer support query.\") if isinstance(row, dict) else \"Answer the customer support query.\"\n",
    "        input_text = row.get('customer') or row.get('input') or row.get('context') or \"\" if isinstance(row, dict) else \"\"\n",
    "        response = row.get('agent') or row.get('output') or row.get('response') or \"\" if isinstance(row, dict) else \"\"\n",
    "        prompt = f\"Instruction: {instruction}\\nInput: {input_text}\\nResponse: {response}\\n\"\n",
    "    return {'text': prompt}\n",
    "\n",
    "# Example and mapping (only run this mapping when raw_ds exists)\n",
    "try:\n",
    "    print(\"Mapping dataset to single 'text' column using TEMPLATE =\", TEMPLATE)\n",
    "    ds = raw_ds.map(build_prompt_for_training)\n",
    "    print(\"Mapping complete. Columns:\", ds.column_names)\n",
    "except NameError:\n",
    "    print(\"raw_ds not found in this scope; ensure you run the cell after loading raw_ds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02a791e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded.\n",
      "Sample ds[0]['text'] type: <class 'str'>\n",
      "Sample text preview: '{\\'flags\\': \\'B\\', \\'instruction\\': \\'question about cancelling order {{Order Number}}\\', \\'category\\': \\'ORDER\\', \\'intent\\': \\'cancel_order\\', \\'response\\': \"I\\'ve understood you have a question re\n",
      "Tokenizing datasets (this may take a bit)...\n",
      "Tokenization complete. Examples:\n",
      "{'flags': 'BILQ', 'instruction': 'what do i have to do to recover my profile key', 'category': 'ACCOUNT', 'intent': 'recover_password', 'response': 'Indeed! I\\'m here to assist you in recovering your profile key. Let\\'s tackle this together:\\n\\n1. Access our platform\\'s \"{{Login Page URL}}\" to initiate the recovery process.\\n2. Locate the \"{{Forgot Password}}\" option and select it to proceed.\\n3. You will be prompted to provide your email address associated with your profile. Kindly input the relevant information.\\n4. Keep an eye on your inbox as you should receive an email containing detailed instructions on how to recover your profile key. In case you don\\'t find it in your primary inbox, please check your spam or other folders.\\n5. Follow the instructions provided in the email carefully to regain access to your profile key.\\n\\nShould you face any hurdles or experience delays in receiving the email, don\\'t hesitate to reach out. Remember, I\\'m here to support you every step of the way!'}\n",
      "{'flags': 'BL', 'instruction': \"I don't know how I can change to the {{Account Category}} account\", 'category': 'ACCOUNT', 'intent': 'switch_account', 'response': 'I appreciate your interest in upgrading to the {{Account Category}} account! Let me guide you through the process step by step:\\n\\n1. Access Your Account: Begin by logging into our platform using your credentials.\\n2. Navigate to Account or Profile Settings: Look for the \"{{Settings}}\" or \"{{Profile}}\" section within your account.\\n3. Explore Upgrade Options: Within the Account/Profile Settings, you should find an option labeled \"{{Upgrade Account}}\" or \"{{Account Change}}\".\\n4. Choose {{Account Category}}: Select the \"{{Account Type}}\" option from the list of available upgrades.\\n5. Follow the Prompts: Once you\\'ve made your selection, follow the on-screen instructions to finalize the switch to the {{Account Category}} account.\\n\\nIf you encounter any difficulties during the process or have any additional questions, please don\\'t hesitate to reach out. We\\'re here to ensure a smooth and enjoyable transition to your new {{Account Category}} account.'}\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Tokenization and Model Loading -----------------\n",
    "# Load tokenizer and model with a dtype appropriate for the device\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "# ensure pad token exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|padding|>'})\n",
    "\n",
    "# Load the model. Avoid forcing float16 on CPU.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=model_dtype,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Resize token embeddings if we added special tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print('Model and tokenizer loaded.')\n",
    "\n",
    "# Tokenization helper that uses the 'text' column (created earlier by map_to_prompt)\n",
    "# --- Robust tokenize_for_lm (inserted) ---\n",
    "def tokenize_for_lm(examples):\n",
    "    # examples['text'] should be a list[str] after mapping\n",
    "    texts = examples.get('text', [])\n",
    "    normalized = []\n",
    "    for t in texts:\n",
    "        if t is None:\n",
    "            normalized.append(\"\")\n",
    "        elif isinstance(t, str):\n",
    "            normalized.append(t)\n",
    "        elif isinstance(t, (list, tuple)):\n",
    "            flat = []\n",
    "            for x in t:\n",
    "                if isinstance(x, (list, tuple)):\n",
    "                    flat += [str(y) for y in x]\n",
    "                else:\n",
    "                    flat.append(str(x))\n",
    "            normalized.append(\" \".join(flat))\n",
    "        else:\n",
    "            normalized.append(str(t))\n",
    "    outputs = tokenizer(\n",
    "        normalized,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "    return outputs\n",
    "\n",
    "# Sanity check helper\n",
    "try:\n",
    "    print(\"Sample ds[0]['text'] type:\", type(ds[0]['text']))\n",
    "    print(\"Sample text preview:\", repr(ds[0]['text'])[:200])\n",
    "except NameError:\n",
    "    print(\"ds not available yet - run this after mapping the dataset.\")\n",
    "\n",
    "\n",
    "print('Tokenizing datasets (this may take a bit)...')\n",
    "train_tok = train_ds.map(tokenize_for_lm, batched=True, remove_columns=[c for c in train_ds.column_names if c!='text'])\n",
    "eval_tok = eval_ds.map(tokenize_for_lm, batched=True, remove_columns=[c for c in eval_ds.column_names if c!='text'])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "print('Tokenization complete. Examples:')\n",
    "for i in range(min(2, len(train_tok))):\n",
    "    print(train_tok[i]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf7d276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for training...\n",
      "Using device=mps, dtype=torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded and moved to mps\n"
     ]
    }
   ],
   "source": [
    "# --- Safe model initialization before training ---\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print(\"Preparing model for training...\")\n",
    "\n",
    "if 'MODEL_ID' not in globals():\n",
    "    raise RuntimeError(\"Please define MODEL_ID (e.g. 'gpt2', 'distilgpt2', etc.) before running this cell.\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "\n",
    "# Pick device and dtype\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\")\n",
    "dtype = torch.float16 if device in (\"cuda\", \"mps\") else torch.float32\n",
    "print(f\"Using device={device}, dtype={dtype}\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Resize embeddings (in case tokenizer was modified)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "print(\"✅ Model loaded and moved to\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2043dda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "Applied LoRA adapter to model (PEFT).\n",
      "use_peft = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "# ----------------- PEFT / LoRA Configuration (optional) -----------------\n",
    "use_peft = False\n",
    "if USE_LORA:\n",
    "    try:\n",
    "        from peft import LoraConfig, get_peft_model\n",
    "        use_peft = True\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        print('Applied LoRA adapter to model (PEFT).')\n",
    "    except Exception as e:\n",
    "        print(f'PEFT/LoRA unavailable or failed: {e}. Continuing without LoRA.')\n",
    "\n",
    "print('use_peft =', use_peft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc7badfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model from MODEL_ID = microsoft/Phi-3-mini-4k-instruct\n",
      "Device: mps dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer are ready. Model on device: mps:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Ensure model & tokenizer exist before Trainer ---\n",
    "# This cell is inserted to guarantee `model` and `tokenizer` are defined.\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "if 'MODEL_ID' not in globals():\n",
    "    raise RuntimeError(\"Please define MODEL_ID before running this cell (e.g., MODEL_ID='gpt2').\")\n",
    "\n",
    "print(\"Preparing model from MODEL_ID =\", MODEL_ID)\n",
    "\n",
    "# Load tokenizer (and ensure pad token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<|pad|>\"})\n",
    "\n",
    "# Choose device and dtype\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\")\n",
    "dtype = torch.float16 if device in (\"cuda\",\"mps\") else torch.float32\n",
    "print(\"Device:\", device, \"dtype:\", dtype)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Resize embeddings to tokenizer (just in case)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model and tokenizer are ready. Model on device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f2ff4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training... (this may be slow on CPU/MPS)\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Training Arguments and Trainer -----------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    fp16=(device in ('cuda', 'mps')),\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=use_pin_memory\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print('Starting training... (this may be slow on CPU/MPS)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59c556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps dtype: torch.float16\n",
      "Tokenizer size: 50258\n",
      "Base config.vocab_size (original): 50257\n",
      "Loading base model with ignore_mismatched_sizes=True ...\n",
      "Base model loaded. Embedding shape (before resize): torch.Size([50257, 768])\n",
      "Resizing token embeddings: 50257 -> 50258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape (after resize): torch.Size([50258, 768])\n",
      "Attaching adapter from OUTPUT_DIR ...\n",
      "Adapter attached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on device: mps:0\n",
      "=== Generated ===\n",
      "Human: I haven't received my refund after 10 days. What should I do?\n",
      "Assistant: I have to answer the following questions. First, I have to provide a proof of account information. If you have a refund, I will provide it to you as soon as I have provided the information to you. Second, I am not responsible for any inconvenience. I would appreciate any inconvenience. Third, I will provide you with the information I have provided to you in advance of my refund. I will not be responsible for any inconvenience. Fourth, I will not be responsible for any inconvenience. I will not be responsible for any inconvenience. Fifth, I will not be responsible for any inconvenience. If\n"
     ]
    }
   ],
   "source": [
    "# Better generation: multi-sample + repetition controls\n",
    "from transformers import AutoTokenizer\n",
    "import torch, textwrap\n",
    "\n",
    "tokenizer = tokenizer  # already loaded in notebook\n",
    "model = model          # already attached and on device\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "print(\"Model device:\", device)\n",
    "\n",
    "# Use instruction-style prompt for clearer behaviour\n",
    "prompt = (\n",
    "    \"Instruction: You are a friendly, helpful customer support assistant.\\n\"\n",
    "    \"Input: I haven't received my refund after 10 days. What should I do?\\n\"\n",
    "    \"Response:\"\n",
    ")\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=150,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.92,\n",
    "    repetition_penalty=1.2,\n",
    "    no_repeat_ngram_size=3,\n",
    "    num_return_sequences=3,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=getattr(tokenizer, \"pad_token_id\", tokenizer.eos_token_id),\n",
    ")\n",
    "\n",
    "outputs = model.generate(input_ids, **gen_kwargs)\n",
    "\n",
    "print(\"\\n=== Candidates ===\\n\")\n",
    "for i, out in enumerate(outputs):\n",
    "    txt = tokenizer.decode(out, skip_special_tokens=True)\n",
    "    # trim to response portion (after \"Response:\" or \"Assistant:\")\n",
    "    if \"Response:\" in txt:\n",
    "        txt = txt.split(\"Response:\",1)[1].strip()\n",
    "    elif \"Assistant:\" in txt:\n",
    "        txt = txt.split(\"Assistant:\",1)[1].strip()\n",
    "    print(f\"--- Candidate {i+1} ---\\n{txt}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
