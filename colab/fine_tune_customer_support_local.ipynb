{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16ddccf",
   "metadata": {},
   "source": [
    "# Fine-tune a small LM on a customer-support dataset (Local MacBook)\n",
    "\n",
    "This notebook is adapted to run on a local MacBook (CPU or Apple MPS). It uses a small model (`distilgpt2`) by default and is conservative with batch sizes/epochs so it can run locally for demonstration. If the public dataset is unavailable the notebook falls back to a tiny synthetic dataset.\n",
    "\n",
    "### How to use\n",
    "1. (Optional) In the first code cell uncomment the `!pip install` line to install dependencies.\n",
    "2. Adjust settings in the `User settings` cell (model, dataset, epochs, batch_size).\n",
    "3. Run cells top-to-bottom. Training on CPU/MPS is slow; expect longer runtimes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8154db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade transformers datasets accelerate peft bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34002140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "# Device selection\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d520af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- User settings (adjust before run) -----------------\n",
    "MODEL_ID = os.environ.get('MODEL_ID', 'microsoft/Phi-3-mini-4k-instruct') # CHANGED\n",
    "DATASET_ID = os.environ.get('DATASET_ID', 'bitext/Bitext-customer-support-llm-chatbot-training-dataset')\n",
    "OUTPUT_DIR = os.environ.get('OUTPUT_DIR', './local_ft_output_phi3') # CHANGED OUTPUT DIR to avoid mixing checkpoints\n",
    "EPOCHS = int(os.environ.get('EPOCHS', '3'))\n",
    "BATCH_SIZE = int(os.environ.get('BATCH_SIZE', '1')) # Adjusted BATCH_SIZE for larger model\n",
    "MAX_LENGTH = int(os.environ.get('MAX_LENGTH', '256')) # Increased length for modern model\n",
    "USE_LORA = os.environ.get('USE_LORA', 'true').lower() in ('1', 'true', 'yes')\n",
    "\n",
    "print('Settings:')\n",
    "print(f' MODEL_ID={MODEL_ID}')\n",
    "print(f' DATASET_ID={DATASET_ID}')\n",
    "print(f' OUTPUT_DIR={OUTPUT_DIR}')\n",
    "print(f' EPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}, MAX_LENGTH={MAX_LENGTH}, USE_LORA={USE_LORA}')\n",
    "# --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d02fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_load_customer_dataset(dataset_id):\n",
    "    try:\n",
    "        ds = load_dataset(dataset_id)\n",
    "        if isinstance(ds, dict) and 'train' in ds:\n",
    "            return ds['train']\n",
    "        return ds\n",
    "    except Exception as e:\n",
    "        print(f'Could not load dataset {dataset_id}: {e}')\n",
    "        print('Falling back to a tiny synthetic customer support dataset for demo.')\n",
    "        samples = [\n",
    "            {'customer': \"My order hasn't arrived, it's been 10 days.\", 'agent': \"I'm sorry. Can you share your order id?\"},\n",
    "            {'customer': 'I was charged twice for the same order.', 'agent': \"I can help. Please share the transaction id.\"},\n",
    "            {'customer': 'How do I return an item?', 'agent': \"You can start a return from your orders page.\"},\n",
    "        ]\n",
    "        return Dataset.from_list(samples)\n",
    "\n",
    "def build_prompt(row):\n",
    "    if 'customer' in row and 'agent' in row:\n",
    "        return f\"Human: {row['customer']}\\nAssistant: {row['agent']}\\n\"\n",
    "    if 'input' in row and 'output' in row:\n",
    "        return f\"Human: {row['input']}\\nAssistant: {row['output']}\\n\"\n",
    "    if 'text' in row:\n",
    "        return row['text'] + \"\\n\"\n",
    "    return str(row)\n",
    "\n",
    "print('Helper functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be4cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = safe_load_customer_dataset(DATASET_ID)\n",
    "print(f'Loaded dataset size: {len(raw_ds)} (showing first 2 examples)')\n",
    "for i,ex in enumerate(raw_ds[:2]):\n",
    "    print('\\n--- example', i, '---')\n",
    "    print(ex)\n",
    "\n",
    "# Map to text prompts\n",
    "if isinstance(raw_ds[0], dict):\n",
    "    def map_to_prompt(example):\n",
    "        return {'text': build_prompt(example)}\n",
    "    ds = raw_ds.map(map_to_prompt)\n",
    "else:\n",
    "    ds = raw_ds.map(lambda x: {'text': str(x)})\n",
    "\n",
    "# Split and reduce for local run\n",
    "if len(ds) > 2000:\n",
    "    ds = ds.train_test_split(test_size=0.05, shuffle=True, seed=42)\n",
    "    train_ds = ds['train'].select(range(4096))\n",
    "    eval_ds = ds['test'].select(range(128))\n",
    "else:\n",
    "    split = ds.train_test_split(test_size=0.1, seed=42)\n",
    "    train_ds = split['train']\n",
    "    eval_ds = split['test']\n",
    "\n",
    "print(f'Train size: {len(train_ds)}, Eval size: {len(eval_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92315426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Tokenization and Model Loading (Cell 124) -----------------\n",
    "\n",
    "# NOTE: BitsAndBytes is removed here because it requires CUDA, which is not available on MPS.\n",
    "# We use torch.float16 instead for memory saving on MPS.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|padding|>'})\n",
    "\n",
    "# Load the model directly in half-precision (float16) to save memory (~7.6 GB)\n",
    "# This is the primary fix for the MPS memory error.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    # Use dtype=torch.float16 for memory efficiency on MPS\n",
    "    dtype=torch.float16,                 # Replaced torch_dtype (deprecated) and BitsAndBytes\n",
    "    trust_remote_code=True,\n",
    "    ignore_mismatched_sizes=True,        # Keep this for token resizing\n",
    ")\n",
    "\n",
    "# NOTE: The model is moved to the MPS device implicitly when it is used by the Trainer.\n",
    "# We still resize the embeddings here before PEFT is applied in the next step.\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# model.to(device) <-- REMOVED: Keep the model on CPU/MPS before PEFT is applied later.\n",
    "\n",
    "print('Tokenizing datasets...')\n",
    "\n",
    "def tokenize_for_lm(examples):\n",
    "    outputs = tokenizer(examples['text'], truncation=True, max_length=MAX_LENGTH, padding='max_length')\n",
    "    outputs['labels'] = outputs['input_ids'].copy()\n",
    "    return outputs\n",
    "\n",
    "train_tok = train_ds.map(tokenize_for_lm, batched=True, remove_columns=train_ds.column_names)\n",
    "eval_tok = eval_ds.map(tokenize_for_lm, batched=True, remove_columns=eval_ds.column_names)\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "print('Tokenization complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- PEFT Configuration (Cell 95) -----------------\n",
    "use_peft = False\n",
    "if USE_LORA:\n",
    "    try:\n",
    "        from peft import LoraConfig, get_peft_model\n",
    "        use_peft = True\n",
    "        lora_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=16,\n",
    "            # CHANGED: Target modules for Phi-3 (often similar to Llama models)\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], \n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        print('Applied LoRA adapter to model (PEFT).')\n",
    "    except Exception as e:\n",
    "        print(f'PEFT/LoRA unavailable or failed: {e}. Continuing without LoRA.')\n",
    "\n",
    "print('use_peft =', use_peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ff4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print('Starting training... (this may be slow on CPU/MPS)')\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "print(f'Saved fine-tuned model to {OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dynamic Checkpoint Selection and Setup (Cell at Index 9) ---\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "TOKENIZER_PATH = OUTPUT_DIR\n",
    "BASE_MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Determine the device (keeps your existing MPS logic)\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# 1. List all checkpoint directories within the output folder\n",
    "checkpoint_dirs = glob.glob(f\"{OUTPUT_DIR}/checkpoint-*\")\n",
    "\n",
    "if not checkpoint_dirs:\n",
    "    PEFT_CHECKPOINT = OUTPUT_DIR\n",
    "    print(f\"Warning: No explicit checkpoints found. Using base path: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    # 2. Extract the step number from the directory name and find the max\n",
    "    def extract_step(path):\n",
    "        match = re.search(r'checkpoint-(\\d+)', path)\n",
    "        return int(match.group(1)) if match else 0\n",
    "\n",
    "    latest_checkpoint_dir = max(checkpoint_dirs, key=extract_step)\n",
    "    PEFT_CHECKPOINT = latest_checkpoint_dir\n",
    "    print(f\"Automatically selected latest checkpoint: {PEFT_CHECKPOINT}\")\n",
    "\n",
    "# --- Model Loading ---\n",
    "\n",
    "# 1) Load tokenizer from checkpoint dir (ensures we use the same vocab)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, trust_remote_code=True)\n",
    "print(\"tokenizer vocab size:\", len(tokenizer))\n",
    "\n",
    "# 2) Load base model (original base) allowing mismatched sizes, then resize embeddings\n",
    "# FIX: Use 'dtype' instead of 'torch_dtype' and include 'trust_remote_code'\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    dtype=torch.float32,                # <-- FIX: Use 'dtype'\n",
    "    trust_remote_code=True              # <-- Added for Phi-3\n",
    ")\n",
    "print(\"base model embedding rows before resize:\", base.get_input_embeddings().weight.shape[0])\n",
    "\n",
    "# resize base model embeddings to tokenizer length\n",
    "base.resize_token_embeddings(len(tokenizer))\n",
    "print(\"base model embedding rows after resize:\", base.get_input_embeddings().weight.shape[0])\n",
    "\n",
    "# 3) Load PEFT adapter on top of the resized base model\n",
    "model = PeftModel.from_pretrained(base, PEFT_CHECKPOINT, torch_dtype=None)\n",
    "model.to(device)\n",
    "\n",
    "# --- Inference Test ---\n",
    "\n",
    "# Set padding token ID for stable generation\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Structured prompt for Instruction-tuned model\n",
    "prompt = \"Instruction: You are a friendly customer support assistant. Provide a concise answer to the user's request.\\nHuman: I have not received my refund after 10 days. What can I do?\\nAssistant:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_length=inputs['input_ids'].shape[1] + 100,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        top_k=0,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "    print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
