{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16ddccf",
   "metadata": {},
   "source": [
    "# Fine-tune a small LM on a customer-support dataset (Local MacBook)\n",
    "\n",
    "This notebook is adapted to run on a local MacBook (CPU or Apple MPS). It uses a small model (`distilgpt2`) by default and is conservative with batch sizes/epochs so it can run locally for demonstration. If the public dataset is unavailable the notebook falls back to a tiny synthetic dataset.\n",
    "\n",
    "### How to use\n",
    "1. (Optional) In the first code cell uncomment the `!pip install` line to install dependencies.\n",
    "2. Adjust settings in the `User settings` cell (model, dataset, epochs, batch_size).\n",
    "3. Run cells top-to-bottom. Training on CPU/MPS is slow; expect longer runtimes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8154db2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (4.4.1)\n",
      "Requirement already satisfied: accelerate in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (1.11.0)\n",
      "Requirement already satisfied: peft in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (0.17.1)\n",
      "Requirement already satisfied: bitsandbytes in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (0.42.0)\n",
      "Requirement already satisfied: trl in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (0.25.1)\n",
      "Requirement already satisfied: filelock in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from accelerate) (2.9.0)\n",
      "Requirement already satisfied: scipy in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from bitsandbytes) (1.16.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: setuptools in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers datasets accelerate peft bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4f295ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Prevent tokenizers parallelism warnings & deadlocks\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "import torch\n",
    "# If using MPS or CPU, pin_memory should be False to avoid warnings\n",
    "# We'll set a flag used in TrainingArguments below\n",
    "use_pin_memory = False if (not torch.cuda.is_available()) else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34002140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps (model dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "# Device selection (robust)\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# Choose dtype depending on device to avoid unsupported dtypes on CPU\n",
    "if device == 'cuda' or device == 'mps':\n",
    "    model_dtype = torch.float16\n",
    "else:\n",
    "    model_dtype = torch.float32\n",
    "\n",
    "print(f'Using device: {device} (model dtype={model_dtype})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d520af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      " MODEL_ID=microsoft/Phi-3-mini-4k-instruct\n",
      " DATASET_ID=bitext/Bitext-customer-support-llm-chatbot-training-dataset\n",
      " OUTPUT_DIR=./local_ft_output\n",
      " EPOCHS=3, BATCH_SIZE=1, MAX_LENGTH=256, USE_LORA=True\n"
     ]
    }
   ],
   "source": [
    "# ----------------- User settings (adjust before run) -----------------\n",
    "MODEL_ID = os.environ.get('MODEL_ID', 'microsoft/Phi-3-mini-4k-instruct') # CHANGED\n",
    "DATASET_ID = os.environ.get('DATASET_ID', 'bitext/Bitext-customer-support-llm-chatbot-training-dataset')\n",
    "OUTPUT_DIR = os.environ.get('OUTPUT_DIR', './local_ft_output') # CHANGED OUTPUT DIR to avoid mixing checkpoints\n",
    "EPOCHS = int(os.environ.get('EPOCHS', '3'))\n",
    "BATCH_SIZE = int(os.environ.get('BATCH_SIZE', '1')) # Adjusted BATCH_SIZE for larger model\n",
    "MAX_LENGTH = int(os.environ.get('MAX_LENGTH', '256')) # Increased length for modern model\n",
    "USE_LORA = os.environ.get('USE_LORA', 'true').lower() in ('1', 'true', 'yes')\n",
    "\n",
    "print('Settings:')\n",
    "print(f' MODEL_ID={MODEL_ID}')\n",
    "print(f' DATASET_ID={DATASET_ID}')\n",
    "print(f' OUTPUT_DIR={OUTPUT_DIR}')\n",
    "print(f' EPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}, MAX_LENGTH={MAX_LENGTH}, USE_LORA={USE_LORA}')\n",
    "# --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15d02fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def safe_load_customer_dataset(dataset_id):\n",
    "    try:\n",
    "        ds = load_dataset(dataset_id)\n",
    "        if isinstance(ds, dict) and 'train' in ds:\n",
    "            return ds['train']\n",
    "        return ds\n",
    "    except Exception as e:\n",
    "        print(f'Could not load dataset {dataset_id}: {e}')\n",
    "        print('Falling back to a tiny synthetic customer support dataset for demo.')\n",
    "        samples = [\n",
    "            {'customer': \"My order hasn't arrived, it's been 10 days.\", 'agent': \"I'm sorry. Can you share your order id?\"},\n",
    "            {'customer': 'I was charged twice for the same order.', 'agent': \"I can help. Please share the transaction id.\"},\n",
    "            {'customer': 'How do I return an item?', 'agent': \"You can start a return from your orders page.\"},\n",
    "        ]\n",
    "        return Dataset.from_list(samples)\n",
    "\n",
    "def build_prompt(row):\n",
    "    if 'customer' in row and 'agent' in row:\n",
    "        return f\"Human: {row['customer']}\\nAssistant: {row['agent']}\\n\"\n",
    "    if 'input' in row and 'output' in row:\n",
    "        return f\"Human: {row['input']}\\nAssistant: {row['output']}\\n\"\n",
    "    if 'text' in row:\n",
    "        return row['text'] + \"\\n\"\n",
    "    return str(row)\n",
    "\n",
    "print('Helper functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9be4cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset size: 26872 (showing first 2 examples)\n",
      "\n",
      "--- example 0 ---\n",
      "flags\n",
      "\n",
      "--- example 1 ---\n",
      "instruction\n",
      "\n",
      "--- example 2 ---\n",
      "category\n",
      "\n",
      "--- example 3 ---\n",
      "intent\n",
      "\n",
      "--- example 4 ---\n",
      "response\n",
      "Train size: 4096, Eval size: 128\n"
     ]
    }
   ],
   "source": [
    "raw_ds = safe_load_customer_dataset(DATASET_ID)\n",
    "print(f'Loaded dataset size: {len(raw_ds)} (showing first 2 examples)')\n",
    "for i,ex in enumerate(raw_ds[:2]):\n",
    "    print('\\n--- example', i, '---')\n",
    "    print(ex)\n",
    "\n",
    "# Map to text prompts\n",
    "if isinstance(raw_ds[0], dict):\n",
    "    def map_to_prompt(example):\n",
    "        return {'text': build_prompt(example)}\n",
    "    ds = raw_ds.map(map_to_prompt)\n",
    "else:\n",
    "    ds = raw_ds.map(lambda x: {'text': str(x)})\n",
    "\n",
    "# Split and reduce for local run\n",
    "if len(ds) > 2000:\n",
    "    ds = ds.train_test_split(test_size=0.05, shuffle=True, seed=42)\n",
    "    train_ds = ds['train'].select(range(4096))\n",
    "    eval_ds = ds['test'].select(range(128))\n",
    "else:\n",
    "    split = ds.train_test_split(test_size=0.1, seed=42)\n",
    "    train_ds = split['train']\n",
    "    eval_ds = split['test']\n",
    "\n",
    "print(f'Train size: {len(train_ds)}, Eval size: {len(eval_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92315426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded.\n",
      "Tokenizing datasets (this may take a bit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 128/128 [00:00<00:00, 2301.66 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Examples:\n",
      "{'flags': 'BILQ', 'instruction': 'what do i have to do to recover my profile key', 'category': 'ACCOUNT', 'intent': 'recover_password', 'response': 'Indeed! I\\'m here to assist you in recovering your profile key. Let\\'s tackle this together:\\n\\n1. Access our platform\\'s \"{{Login Page URL}}\" to initiate the recovery process.\\n2. Locate the \"{{Forgot Password}}\" option and select it to proceed.\\n3. You will be prompted to provide your email address associated with your profile. Kindly input the relevant information.\\n4. Keep an eye on your inbox as you should receive an email containing detailed instructions on how to recover your profile key. In case you don\\'t find it in your primary inbox, please check your spam or other folders.\\n5. Follow the instructions provided in the email carefully to regain access to your profile key.\\n\\nShould you face any hurdles or experience delays in receiving the email, don\\'t hesitate to reach out. Remember, I\\'m here to support you every step of the way!'}\n",
      "{'flags': 'BL', 'instruction': \"I don't know how I can change to the {{Account Category}} account\", 'category': 'ACCOUNT', 'intent': 'switch_account', 'response': 'I appreciate your interest in upgrading to the {{Account Category}} account! Let me guide you through the process step by step:\\n\\n1. Access Your Account: Begin by logging into our platform using your credentials.\\n2. Navigate to Account or Profile Settings: Look for the \"{{Settings}}\" or \"{{Profile}}\" section within your account.\\n3. Explore Upgrade Options: Within the Account/Profile Settings, you should find an option labeled \"{{Upgrade Account}}\" or \"{{Account Change}}\".\\n4. Choose {{Account Category}}: Select the \"{{Account Type}}\" option from the list of available upgrades.\\n5. Follow the Prompts: Once you\\'ve made your selection, follow the on-screen instructions to finalize the switch to the {{Account Category}} account.\\n\\nIf you encounter any difficulties during the process or have any additional questions, please don\\'t hesitate to reach out. We\\'re here to ensure a smooth and enjoyable transition to your new {{Account Category}} account.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Tokenization and Model Loading -----------------\n",
    "# Load tokenizer and model with a dtype appropriate for the device\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "# ensure pad token exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|padding|>'})\n",
    "\n",
    "# Load the model. Avoid forcing float16 on CPU.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=model_dtype,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Resize token embeddings if we added special tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print('Model and tokenizer loaded.')\n",
    "\n",
    "# Tokenization helper that uses the 'text' column (created earlier by map_to_prompt)\n",
    "def tokenize_for_lm(examples):\n",
    "    \"\"\"Robust tokenizer that normalizes nested text structures to flat strings.\"\"\"\n",
    "    raw_texts = examples.get('text')\n",
    "    normalized = []\n",
    "    for t in raw_texts:\n",
    "        if t is None:\n",
    "            normalized.append(\"\")\n",
    "        elif isinstance(t, str):\n",
    "            normalized.append(t)\n",
    "        elif isinstance(t, (list, tuple)):\n",
    "            # Flatten nested lists or tokens into a single string\n",
    "            flat = []\n",
    "            for x in t:\n",
    "                if isinstance(x, (list, tuple)):\n",
    "                    flat += [str(y) for y in x]\n",
    "                else:\n",
    "                    flat.append(str(x))\n",
    "            normalized.append(\" \".join(flat))\n",
    "        else:\n",
    "            normalized.append(str(t))\n",
    "\n",
    "    # Now safely tokenize normalized strings\n",
    "    outputs = tokenizer(\n",
    "        normalized,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\"  # ensures consistent tensor shapes\n",
    "    )\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "    return outputs\n",
    "\n",
    "\n",
    "print('Tokenizing datasets (this may take a bit)...')\n",
    "train_tok = train_ds.map(tokenize_for_lm, batched=True, remove_columns=[c for c in train_ds.column_names if c!='text'])\n",
    "eval_tok = eval_ds.map(tokenize_for_lm, batched=True, remove_columns=[c for c in eval_ds.column_names if c!='text'])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "print('Tokenization complete. Examples:')\n",
    "for i in range(min(2, len(train_tok))):\n",
    "    print(train_tok[i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2043dda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied LoRA adapter to model (PEFT).\n",
      "use_peft = True\n"
     ]
    }
   ],
   "source": [
    "# ----------------- PEFT / LoRA Configuration (optional) -----------------\n",
    "use_peft = False\n",
    "if USE_LORA:\n",
    "    try:\n",
    "        from peft import LoraConfig, get_peft_model\n",
    "        use_peft = True\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        print('Applied LoRA adapter to model (PEFT).')\n",
    "    except Exception as e:\n",
    "        print(f'PEFT/LoRA unavailable or failed: {e}. Continuing without LoRA.')\n",
    "\n",
    "print('use_peft =', use_peft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f2ff4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training... (this may be slow on CPU/MPS)\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Training Arguments and Trainer -----------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    fp16=(device in ('cuda', 'mps')),\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=use_pin_memory\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print('Starting training... (this may be slow on CPU/MPS)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f59c556",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPT2LMHeadModel:\n\tsize mismatch for transformer.wte.weight: copying a param with shape torch.Size([50258, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([50258, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m gen_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load model with ignore_mismatched_sizes so from_pretrained won't error on mismatch.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m gen_model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# allow different embedding sizes to be adjusted\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Ensure model embedding matrix matches tokenizer size\u001b[39;00m\n\u001b[32m     16\u001b[39m gen_model.resize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(gen_tokenizer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:5148\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _adapter_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5147\u001b[39m     adapter_kwargs[\u001b[33m\"\u001b[39m\u001b[33mkey_mapping\u001b[39m\u001b[33m\"\u001b[39m] = key_mapping\n\u001b[32m-> \u001b[39m\u001b[32m5148\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5149\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_adapter_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5150\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5152\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5153\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_loading_info:\n\u001b[32m   5156\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m from_pt:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages/transformers/integrations/peft.py:261\u001b[39m, in \u001b[36mPeftAdapterMixin.load_adapter\u001b[39m\u001b[34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, low_cpu_mem_usage, is_trainable, adapter_kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m     processed_adapter_state_dict[new_key] = value\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# Load state dict\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m incompatible_keys = \u001b[43mset_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_adapter_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpeft_load_kwargs\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m incompatible_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    266\u001b[39m     err_msg = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages/peft/utils/save_and_load.py:524\u001b[39m, in \u001b[36mset_peft_model_state_dict\u001b[39m\u001b[34m(model, peft_model_state_dict, adapter_name, ignore_mismatched_sizes, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    522\u001b[39m             module._move_adapter_to_device_of_base_layer(adapter_name)\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m     load_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.is_prompt_learning:\n\u001b[32m    527\u001b[39m     model.prompt_encoder[adapter_name].embedding.load_state_dict(\n\u001b[32m    528\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m: peft_model_state_dict[\u001b[33m\"\u001b[39m\u001b[33mprompt_embeddings\u001b[39m\u001b[33m\"\u001b[39m]}, strict=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    529\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for GPT2LMHeadModel:\n\tsize mismatch for transformer.wte.weight: copying a param with shape torch.Size([50258, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([50258, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768])."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Load tokenizer from the OUTPUT_DIR first (this contains any special tokens you saved)\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR, trust_remote_code=True)\n",
    "\n",
    "# Load model with ignore_mismatched_sizes so from_pretrained won't error on mismatch.\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    OUTPUT_DIR,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=model_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    ignore_mismatched_sizes=True  # allow different embedding sizes to be adjusted\n",
    ")\n",
    "\n",
    "# Ensure model embedding matrix matches tokenizer size\n",
    "gen_model.resize_token_embeddings(len(gen_tokenizer))\n",
    "\n",
    "# Move to device and build pipeline\n",
    "gen_model.to(device)\n",
    "device_id = 0 if device == 'cuda' else -1\n",
    "generator = pipeline('text-generation', model=gen_model, tokenizer=gen_tokenizer, device=device_id)\n",
    "\n",
    "prompt = \"Human: I haven't received my refund after 10 days. What can I do?\\nAssistant:\"\n",
    "print('Generating an example response:')\n",
    "print(generator(prompt, max_length=200, do_sample=True, top_p=0.9, num_return_sequences=1)[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
