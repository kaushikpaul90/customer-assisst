{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16ddccf",
   "metadata": {},
   "source": [
    "# Fine-tune a small LM on a customer-support dataset (Local MacBook)\n",
    "\n",
    "This notebook is adapted to run on a local MacBook (CPU or Apple MPS). It uses a small model (`distilgpt2`) by default and is conservative with batch sizes/epochs so it can run locally for demonstration. If the public dataset is unavailable the notebook falls back to a tiny synthetic dataset.\n",
    "\n",
    "### How to use\n",
    "1. (Optional) In the first code cell uncomment the `!pip install` line to install dependencies.\n",
    "2. Adjust settings in the `User settings` cell (model, dataset, epochs, batch_size).\n",
    "3. Run cells top-to-bottom. Training on CPU/MPS is slow; expect longer runtimes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ce7e1987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_MODEL_ID: microsoft/Phi-3-mini-4k-instruct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- BASE model helper ---\n",
    "# Ensure BASE_MODEL_ID is defined. Set this to the original base model used for fine-tuning.\n",
    "# Example: BASE_MODEL_ID = \"distilgpt2\"\n",
    "if 'BASE_MODEL_ID' not in globals():\n",
    "    try:\n",
    "        BASE_MODEL_ID = MODEL_ID\n",
    "    except NameError:\n",
    "        BASE_MODEL_ID = None\n",
    "print(\"BASE_MODEL_ID:\", BASE_MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8154db2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (4.4.1)\n",
      "Requirement already satisfied: accelerate in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (1.11.0)\n",
      "Requirement already satisfied: peft in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (0.17.1)\n",
      "Requirement already satisfied: bitsandbytes in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (0.42.0)\n",
      "Requirement already satisfied: trl in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (0.25.1)\n",
      "Requirement already satisfied: filelock in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from accelerate) (2.9.0)\n",
      "Requirement already satisfied: scipy in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from bitsandbytes) (1.16.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: setuptools in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers datasets accelerate peft bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c4f295ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Prevent tokenizers parallelism warnings & deadlocks\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "import torch\n",
    "# If using MPS or CPU, pin_memory should be False to avoid warnings\n",
    "# We'll set a flag used in TrainingArguments below\n",
    "use_pin_memory = False if (not torch.cuda.is_available()) else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "34002140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps (model dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "# Device selection (robust)\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# Choose dtype depending on device to avoid unsupported dtypes on CPU\n",
    "if device == 'cuda' or device == 'mps':\n",
    "    model_dtype = torch.float16\n",
    "else:\n",
    "    model_dtype = torch.float32\n",
    "\n",
    "print(f'Using device: {device} (model dtype={model_dtype})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0d520af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      " MODEL_ID=microsoft/Phi-3-mini-4k-instruct\n",
      " DATASET_ID=bitext/Bitext-customer-support-llm-chatbot-training-dataset\n",
      " OUTPUT_DIR=./local_ft_output\n",
      " EPOCHS=3, BATCH_SIZE=1, MAX_LENGTH=256, USE_LORA=True\n"
     ]
    }
   ],
   "source": [
    "# ----------------- User settings (adjust before run) -----------------\n",
    "MODEL_ID = os.environ.get('MODEL_ID', 'microsoft/Phi-3-mini-4k-instruct') # CHANGED\n",
    "DATASET_ID = os.environ.get('DATASET_ID', 'bitext/Bitext-customer-support-llm-chatbot-training-dataset')\n",
    "OUTPUT_DIR = os.environ.get('OUTPUT_DIR', './local_ft_output') # CHANGED OUTPUT DIR to avoid mixing checkpoints\n",
    "EPOCHS = int(os.environ.get('EPOCHS', '3'))\n",
    "BATCH_SIZE = int(os.environ.get('BATCH_SIZE', '1')) # Adjusted BATCH_SIZE for larger model\n",
    "MAX_LENGTH = int(os.environ.get('MAX_LENGTH', '256')) # Increased length for modern model\n",
    "USE_LORA = os.environ.get('USE_LORA', 'true').lower() in ('1', 'true', 'yes')\n",
    "\n",
    "print('Settings:')\n",
    "print(f' MODEL_ID={MODEL_ID}')\n",
    "print(f' DATASET_ID={DATASET_ID}')\n",
    "print(f' OUTPUT_DIR={OUTPUT_DIR}')\n",
    "print(f' EPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}, MAX_LENGTH={MAX_LENGTH}, USE_LORA={USE_LORA}')\n",
    "# --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "15d02fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def safe_load_customer_dataset(dataset_id):\n",
    "    try:\n",
    "        ds = load_dataset(dataset_id)\n",
    "        if isinstance(ds, dict) and 'train' in ds:\n",
    "            return ds['train']\n",
    "        return ds\n",
    "    except Exception as e:\n",
    "        print(f'Could not load dataset {dataset_id}: {e}')\n",
    "        print('Falling back to a tiny synthetic customer support dataset for demo.')\n",
    "        samples = [\n",
    "            {'customer': \"My order hasn't arrived, it's been 10 days.\", 'agent': \"I'm sorry. Can you share your order id?\"},\n",
    "            {'customer': 'I was charged twice for the same order.', 'agent': \"I can help. Please share the transaction id.\"},\n",
    "            {'customer': 'How do I return an item?', 'agent': \"You can start a return from your orders page.\"},\n",
    "        ]\n",
    "        return Dataset.from_list(samples)\n",
    "\n",
    "def build_prompt(row):\n",
    "    if 'customer' in row and 'agent' in row:\n",
    "        return f\"Human: {row['customer']}\\nAssistant: {row['agent']}\\n\"\n",
    "    if 'input' in row and 'output' in row:\n",
    "        return f\"Human: {row['input']}\\nAssistant: {row['output']}\\n\"\n",
    "    if 'text' in row:\n",
    "        return row['text'] + \"\\n\"\n",
    "    return str(row)\n",
    "\n",
    "print('Helper functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c9be4cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset size: 26872 (showing first 2 examples)\n",
      "\n",
      "--- example 0 ---\n",
      "flags\n",
      "\n",
      "--- example 1 ---\n",
      "instruction\n",
      "\n",
      "--- example 2 ---\n",
      "category\n",
      "\n",
      "--- example 3 ---\n",
      "intent\n",
      "\n",
      "--- example 4 ---\n",
      "response\n",
      "Train size: 4096, Eval size: 128\n"
     ]
    }
   ],
   "source": [
    "raw_ds = safe_load_customer_dataset(DATASET_ID)\n",
    "print(f'Loaded dataset size: {len(raw_ds)} (showing first 2 examples)')\n",
    "for i,ex in enumerate(raw_ds[:2]):\n",
    "    print('\\n--- example', i, '---')\n",
    "    print(ex)\n",
    "\n",
    "# Map to text prompts\n",
    "if isinstance(raw_ds[0], dict):\n",
    "    def map_to_prompt(example):\n",
    "        return {'text': build_prompt(example)}\n",
    "    ds = raw_ds.map(map_to_prompt)\n",
    "else:\n",
    "    ds = raw_ds.map(lambda x: {'text': str(x)})\n",
    "\n",
    "# Split and reduce for local run\n",
    "if len(ds) > 2000:\n",
    "    ds = ds.train_test_split(test_size=0.05, shuffle=True, seed=42)\n",
    "    train_ds = ds['train'].select(range(4096))\n",
    "    eval_ds = ds['test'].select(range(128))\n",
    "else:\n",
    "    split = ds.train_test_split(test_size=0.1, seed=42)\n",
    "    train_ds = split['train']\n",
    "    eval_ds = split['test']\n",
    "\n",
    "print(f'Train size: {len(train_ds)}, Eval size: {len(eval_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d00f04d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping dataset to single 'text' column using TEMPLATE = A\n",
      "Mapping complete. Columns: ['flags', 'instruction', 'category', 'intent', 'response', 'text']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Prompt template setup (inserted) ---\n",
    "TEMPLATE = \"A\"  # \"A\" = Human/Assistant, \"B\" = Instruction/Input/Response\n",
    "\n",
    "def build_prompt_for_training(row):\n",
    "    if TEMPLATE == \"A\":\n",
    "        if isinstance(row, dict) and 'customer' in row and 'agent' in row:\n",
    "            prompt = f\"Human: {row['customer']}\\nAssistant: {row['agent']}\\n\"\n",
    "        elif isinstance(row, dict) and 'input' in row and 'output' in row:\n",
    "            prompt = f\"Human: {row['input']}\\nAssistant: {row['output']}\\n\"\n",
    "        elif isinstance(row, dict) and 'text' in row:\n",
    "            prompt = row['text']\n",
    "        else:\n",
    "            prompt = str(row)\n",
    "    else:\n",
    "        instruction = row.get('instruction', \"Answer the customer support query.\") if isinstance(row, dict) else \"Answer the customer support query.\"\n",
    "        input_text = row.get('customer') or row.get('input') or row.get('context') or \"\" if isinstance(row, dict) else \"\"\n",
    "        response = row.get('agent') or row.get('output') or row.get('response') or \"\" if isinstance(row, dict) else \"\"\n",
    "        prompt = f\"Instruction: {instruction}\\nInput: {input_text}\\nResponse: {response}\\n\"\n",
    "    return {'text': prompt}\n",
    "\n",
    "# Example and mapping (only run this mapping when raw_ds exists)\n",
    "try:\n",
    "    print(\"Mapping dataset to single 'text' column using TEMPLATE =\", TEMPLATE)\n",
    "    ds = raw_ds.map(build_prompt_for_training)\n",
    "    print(\"Mapping complete. Columns:\", ds.column_names)\n",
    "except NameError:\n",
    "    print(\"raw_ds not found in this scope; ensure you run the cell after loading raw_ds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "02a791e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded.\n",
      "Sample ds[0]['text'] type: <class 'str'>\n",
      "Sample text preview: '{\\'flags\\': \\'B\\', \\'instruction\\': \\'question about cancelling order {{Order Number}}\\', \\'category\\': \\'ORDER\\', \\'intent\\': \\'cancel_order\\', \\'response\\': \"I\\'ve understood you have a question re\n",
      "Tokenizing datasets (this may take a bit)...\n",
      "Tokenization complete. Examples:\n",
      "{'flags': 'BILQ', 'instruction': 'what do i have to do to recover my profile key', 'category': 'ACCOUNT', 'intent': 'recover_password', 'response': 'Indeed! I\\'m here to assist you in recovering your profile key. Let\\'s tackle this together:\\n\\n1. Access our platform\\'s \"{{Login Page URL}}\" to initiate the recovery process.\\n2. Locate the \"{{Forgot Password}}\" option and select it to proceed.\\n3. You will be prompted to provide your email address associated with your profile. Kindly input the relevant information.\\n4. Keep an eye on your inbox as you should receive an email containing detailed instructions on how to recover your profile key. In case you don\\'t find it in your primary inbox, please check your spam or other folders.\\n5. Follow the instructions provided in the email carefully to regain access to your profile key.\\n\\nShould you face any hurdles or experience delays in receiving the email, don\\'t hesitate to reach out. Remember, I\\'m here to support you every step of the way!'}\n",
      "{'flags': 'BL', 'instruction': \"I don't know how I can change to the {{Account Category}} account\", 'category': 'ACCOUNT', 'intent': 'switch_account', 'response': 'I appreciate your interest in upgrading to the {{Account Category}} account! Let me guide you through the process step by step:\\n\\n1. Access Your Account: Begin by logging into our platform using your credentials.\\n2. Navigate to Account or Profile Settings: Look for the \"{{Settings}}\" or \"{{Profile}}\" section within your account.\\n3. Explore Upgrade Options: Within the Account/Profile Settings, you should find an option labeled \"{{Upgrade Account}}\" or \"{{Account Change}}\".\\n4. Choose {{Account Category}}: Select the \"{{Account Type}}\" option from the list of available upgrades.\\n5. Follow the Prompts: Once you\\'ve made your selection, follow the on-screen instructions to finalize the switch to the {{Account Category}} account.\\n\\nIf you encounter any difficulties during the process or have any additional questions, please don\\'t hesitate to reach out. We\\'re here to ensure a smooth and enjoyable transition to your new {{Account Category}} account.'}\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Tokenization and Model Loading -----------------\n",
    "# Load tokenizer and model with a dtype appropriate for the device\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "# ensure pad token exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|padding|>'})\n",
    "\n",
    "# Load the model. Avoid forcing float16 on CPU.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=model_dtype,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Resize token embeddings if we added special tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print('Model and tokenizer loaded.')\n",
    "\n",
    "# Tokenization helper that uses the 'text' column (created earlier by map_to_prompt)\n",
    "# --- Robust tokenize_for_lm (inserted) ---\n",
    "def tokenize_for_lm(examples):\n",
    "    # examples['text'] should be a list[str] after mapping\n",
    "    texts = examples.get('text', [])\n",
    "    normalized = []\n",
    "    for t in texts:\n",
    "        if t is None:\n",
    "            normalized.append(\"\")\n",
    "        elif isinstance(t, str):\n",
    "            normalized.append(t)\n",
    "        elif isinstance(t, (list, tuple)):\n",
    "            flat = []\n",
    "            for x in t:\n",
    "                if isinstance(x, (list, tuple)):\n",
    "                    flat += [str(y) for y in x]\n",
    "                else:\n",
    "                    flat.append(str(x))\n",
    "            normalized.append(\" \".join(flat))\n",
    "        else:\n",
    "            normalized.append(str(t))\n",
    "    outputs = tokenizer(\n",
    "        normalized,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "    return outputs\n",
    "\n",
    "# Sanity check helper\n",
    "try:\n",
    "    print(\"Sample ds[0]['text'] type:\", type(ds[0]['text']))\n",
    "    print(\"Sample text preview:\", repr(ds[0]['text'])[:200])\n",
    "except NameError:\n",
    "    print(\"ds not available yet - run this after mapping the dataset.\")\n",
    "\n",
    "\n",
    "print('Tokenizing datasets (this may take a bit)...')\n",
    "train_tok = train_ds.map(tokenize_for_lm, batched=True, remove_columns=[c for c in train_ds.column_names if c!='text'])\n",
    "eval_tok = eval_ds.map(tokenize_for_lm, batched=True, remove_columns=[c for c in eval_ds.column_names if c!='text'])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "print('Tokenization complete. Examples:')\n",
    "for i in range(min(2, len(train_tok))):\n",
    "    print(train_tok[i]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7d276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for training...\n",
      "Using device=mps, dtype=torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.20s/it]"
     ]
    }
   ],
   "source": [
    "# --- Safe model initialization before training ---\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print(\"Preparing model for training...\")\n",
    "\n",
    "if 'MODEL_ID' not in globals():\n",
    "    raise RuntimeError(\"Please define MODEL_ID (e.g. 'gpt2', 'distilgpt2', etc.) before running this cell.\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "\n",
    "# Pick device and dtype\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\")\n",
    "dtype = torch.float16 if device in (\"cuda\", \"mps\") else torch.float32\n",
    "print(f\"Using device={device}, dtype={dtype}\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Resize embeddings (in case tokenizer was modified)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "print(\"✅ Model loaded and moved to\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043dda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied LoRA adapter to model (PEFT).\n",
      "use_peft = True\n"
     ]
    }
   ],
   "source": [
    "# ----------------- PEFT / LoRA Configuration (optional) -----------------\n",
    "use_peft = False\n",
    "if USE_LORA:\n",
    "    try:\n",
    "        from peft import LoraConfig, get_peft_model\n",
    "        use_peft = True\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        print('Applied LoRA adapter to model (PEFT).')\n",
    "    except Exception as e:\n",
    "        print(f'PEFT/LoRA unavailable or failed: {e}. Continuing without LoRA.')\n",
    "\n",
    "print('use_peft =', use_peft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7badfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model from MODEL_ID = microsoft/Phi-3-mini-4k-instruct\n",
      "Device: mps dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer are ready. Model on device: mps:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Ensure model & tokenizer exist before Trainer ---\n",
    "# This cell is inserted to guarantee `model` and `tokenizer` are defined.\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "if 'MODEL_ID' not in globals():\n",
    "    raise RuntimeError(\"Please define MODEL_ID before running this cell (e.g., MODEL_ID='gpt2').\")\n",
    "\n",
    "print(\"Preparing model from MODEL_ID =\", MODEL_ID)\n",
    "\n",
    "# Load tokenizer (and ensure pad token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<|pad|>\"})\n",
    "\n",
    "# Choose device and dtype\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\")\n",
    "dtype = torch.float16 if device in (\"cuda\",\"mps\") else torch.float32\n",
    "print(\"Device:\", device, \"dtype:\", dtype)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Resize embeddings to tokenizer (just in case)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model and tokenizer are ready. Model on device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ff4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training... (this may be slow on CPU/MPS)\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Training Arguments and Trainer -----------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    fp16=(device in ('cuda', 'mps')),\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=use_pin_memory\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print('Starting training... (this may be slow on CPU/MPS)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59c556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT_DIR: ./local_ft_output\n",
      "Loading tokenizer from OUTPUT_DIR...\n",
      "Tokenizer vocab size: 50258\n",
      "adapter_config.json found. Reading adapter metadata...\n",
      "Adapter base_model_name_or_path: distilgpt2\n",
      "Adapter target_modules: ['v_attn', 'c_attn', 'q_attn']\n",
      "Base model chosen for loading: distilgpt2\n",
      "Device: mps dtype: torch.float16\n",
      "Loading config from base model (safe)...\n",
      "Base config.vocab_size (orig): 50257\n",
      "Base looks GPT-2-like; using GPT2LMHeadModel explicit class for loading.\n",
      "Loading base model distilgpt2 with ignore_mismatched_sizes=True ...\n",
      "Base model loaded. Embedding shape (before resize): torch.Size([50257, 768])\n",
      "Resizing token embeddings: 50257 -> 50258\n",
      "Embedding shape (after): torch.Size([50258, 768])\n",
      "\n",
      "Sample attention-related parameter names (first 50):\n",
      "  transformer.h.0.attn.c_attn.weight\n",
      "  transformer.h.0.attn.c_attn.bias\n",
      "  transformer.h.0.attn.c_proj.weight\n",
      "  transformer.h.0.attn.c_proj.bias\n",
      "  transformer.h.0.mlp.c_proj.weight\n",
      "  transformer.h.0.mlp.c_proj.bias\n",
      "  transformer.h.1.attn.c_attn.weight\n",
      "  transformer.h.1.attn.c_attn.bias\n",
      "  transformer.h.1.attn.c_proj.weight\n",
      "  transformer.h.1.attn.c_proj.bias\n",
      "  transformer.h.1.mlp.c_proj.weight\n",
      "  transformer.h.1.mlp.c_proj.bias\n",
      "  transformer.h.2.attn.c_attn.weight\n",
      "  transformer.h.2.attn.c_attn.bias\n",
      "  transformer.h.2.attn.c_proj.weight\n",
      "  transformer.h.2.attn.c_proj.bias\n",
      "  transformer.h.2.mlp.c_proj.weight\n",
      "  transformer.h.2.mlp.c_proj.bias\n",
      "  transformer.h.3.attn.c_attn.weight\n",
      "  transformer.h.3.attn.c_attn.bias\n",
      "  transformer.h.3.attn.c_proj.weight\n",
      "  transformer.h.3.attn.c_proj.bias\n",
      "  transformer.h.3.mlp.c_proj.weight\n",
      "  transformer.h.3.mlp.c_proj.bias\n",
      "  transformer.h.4.attn.c_attn.weight\n",
      "  transformer.h.4.attn.c_attn.bias\n",
      "  transformer.h.4.attn.c_proj.weight\n",
      "  transformer.h.4.attn.c_proj.bias\n",
      "  transformer.h.4.mlp.c_proj.weight\n",
      "  transformer.h.4.mlp.c_proj.bias\n",
      "  transformer.h.5.attn.c_attn.weight\n",
      "  transformer.h.5.attn.c_attn.bias\n",
      "  transformer.h.5.attn.c_proj.weight\n",
      "  transformer.h.5.attn.c_proj.bias\n",
      "  transformer.h.5.mlp.c_proj.weight\n",
      "  transformer.h.5.mlp.c_proj.bias\n",
      "\n",
      "Adapter target module match summary:\n",
      "  v_attn: NOT FOUND\n",
      "  c_attn: FOUND\n",
      "  q_attn: NOT FOUND\n",
      "Adapter target modules do NOT match base model. Skipping adapter attach to avoid ValueError.\n",
      "If you want to force attach, set BASE_MODEL_ID to the exact model used during adapter training (e.g., 'distilgpt2').\n",
      "Final model on device: mps:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Robust safe model & adapter loader (guarded, non-destructive) ---\n",
    "# This cell will:\n",
    "# 1. Load tokenizer from OUTPUT_DIR (must exist)\n",
    "# 2. Determine BASE model (adapter metadata or MODEL_ID/BASE_MODEL_ID)\n",
    "# 3. Load base model safely (use explicit GPT2 class for GPT-like bases)\n",
    "# 4. Check adapter target_modules vs base model parameter names\n",
    "# 5. Attach adapter only if target modules match; otherwise keep base_model only and print diagnostics\n",
    "# 6. Expose `model` (either base_model or model_with_adapter) and `tokenizer` for downstream cells\n",
    "\n",
    "import os, json, torch, traceback\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel\n",
    "\n",
    "OUTPUT_DIR = globals().get(\"OUTPUT_DIR\", \"./local_ft_output\")\n",
    "BASE_MODEL_ID = globals().get(\"BASE_MODEL_ID\", None) or globals().get(\"MODEL_ID\", None)\n",
    "\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "outp = Path(OUTPUT_DIR)\n",
    "if not outp.exists():\n",
    "    raise RuntimeError(f\"OUTPUT_DIR {OUTPUT_DIR} does not exist. Set OUTPUT_DIR correctly.\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer from OUTPUT_DIR...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR, trust_remote_code=False)\n",
    "tokenizer_vocab_size = len(tokenizer)\n",
    "print(\"Tokenizer vocab size:\", tokenizer_vocab_size)\n",
    "\n",
    "# Inspect OUTPUT_DIR for adapter/config\n",
    "adapter_cfg_path = outp / \"adapter_config.json\"\n",
    "has_adapter = adapter_cfg_path.exists()\n",
    "adapter_cfg = None\n",
    "adapter_base = None\n",
    "adapter_targets = None\n",
    "\n",
    "if has_adapter:\n",
    "    print(\"adapter_config.json found. Reading adapter metadata...\")\n",
    "    try:\n",
    "        adapter_cfg = json.loads(adapter_cfg_path.read_text())\n",
    "        adapter_base = adapter_cfg.get(\"base_model_name_or_path\")\n",
    "        adapter_targets = adapter_cfg.get(\"target_modules\", adapter_cfg.get(\"target_module\", None))\n",
    "        print(\"Adapter base_model_name_or_path:\", adapter_base)\n",
    "        print(\"Adapter target_modules:\", adapter_targets)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to read adapter_config.json:\", e)\n",
    "        adapter_cfg = None\n",
    "else:\n",
    "    print(\"No adapter_config.json present in OUTPUT_DIR. Proceeding with base model only.\")\n",
    "\n",
    "# Decide base model to load\n",
    "base_to_load = adapter_base or BASE_MODEL_ID\n",
    "if base_to_load is None:\n",
    "    raise RuntimeError(\"Could not determine base model. Set BASE_MODEL_ID or ensure adapter_config.json has base_model_name_or_path.\")\n",
    "\n",
    "print(\"Base model chosen for loading:\", base_to_load)\n",
    "\n",
    "# Device & dtype\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\")\n",
    "dtype = torch.float16 if device in (\"cuda\",\"mps\") else torch.float32\n",
    "print(\"Device:\", device, \"dtype:\", dtype)\n",
    "\n",
    "# Load config from base_to_load\n",
    "print(\"Loading config from base model (safe)...\")\n",
    "config = AutoConfig.from_pretrained(base_to_load, trust_remote_code=False)\n",
    "print(\"Base config.vocab_size (orig):\", getattr(config, \"vocab_size\", None))\n",
    "\n",
    "# Load base model safely. Use explicit GPT2 class for GPT-like bases for parameter name consistency.\n",
    "use_gpt2_class = any(k in base_to_load.lower() for k in (\"gpt2\",\"distilgpt\",\"gpt-\"))\n",
    "if use_gpt2_class:\n",
    "    print(\"Base looks GPT-2-like; using GPT2LMHeadModel explicit class for loading.\")\n",
    "    BaseClass = GPT2LMHeadModel\n",
    "else:\n",
    "    BaseClass = AutoModelForCausalLM\n",
    "\n",
    "print(f\"Loading base model {base_to_load} with ignore_mismatched_sizes=True ...\")\n",
    "base_model = BaseClass.from_pretrained(\n",
    "    base_to_load,\n",
    "    config=config,\n",
    "    dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "print(\"Base model loaded. Embedding shape (before resize):\", base_model.get_input_embeddings().weight.shape)\n",
    "\n",
    "# Resize embeddings to tokenizer size if needed\n",
    "if base_model.get_input_embeddings().weight.shape[0] != tokenizer_vocab_size:\n",
    "    print(\"Resizing token embeddings:\", base_model.get_input_embeddings().weight.shape[0], \"->\", tokenizer_vocab_size)\n",
    "    base_model.resize_token_embeddings(tokenizer_vocab_size)\n",
    "print(\"Embedding shape (after):\", base_model.get_input_embeddings().weight.shape)\n",
    "\n",
    "# Diagnostic: collect attention/module name substrings\n",
    "param_names = [n for n, _ in base_model.named_parameters()]\n",
    "attn_related = [n for n in param_names if any(x in n for x in ['attn','c_attn','q_attn','v_attn','q_proj','k_proj','v_proj','c_proj'])]\n",
    "print(\"\\nSample attention-related parameter names (first 50):\")\n",
    "for n in attn_related[:50]:\n",
    "    print(\" \", n)\n",
    "\n",
    "# Attempt to attach adapter only if safe (target module match)\n",
    "model = base_model\n",
    "if has_adapter and adapter_targets:\n",
    "    # normalize adapter targets to list of substrings\n",
    "    adapter_targets_list = adapter_targets if isinstance(adapter_targets, (list,tuple)) else [adapter_targets]\n",
    "    # check match: any adapter target substring appears in any base model param names\n",
    "    matches = {t: any(t in name for name in param_names) for t in adapter_targets_list}\n",
    "    print(\"\\nAdapter target module match summary:\")\n",
    "    for t, m in matches.items():\n",
    "        print(f\"  {t}: {'FOUND' if m else 'NOT FOUND'}\")\n",
    "    if all(matches.values()):\n",
    "        # safe to attach\n",
    "        try:\n",
    "            print(\"All adapter target modules found in base model. Attaching adapter via PeftModel.from_pretrained...\")\n",
    "            from peft import PeftModel\n",
    "            model = PeftModel.from_pretrained(base_model, OUTPUT_DIR, torch_dtype=dtype, device_map=None)\n",
    "            print(\"Adapter attached successfully.\")\n",
    "        except Exception as e:\n",
    "            print(\"Adapter attach failed with exception; falling back to base_model. Exception:\")\n",
    "            traceback.print_exc()\n",
    "            model = base_model\n",
    "    else:\n",
    "        print(\"Adapter target modules do NOT match base model. Skipping adapter attach to avoid ValueError.\")\n",
    "        print(\"If you want to force attach, set BASE_MODEL_ID to the exact model used during adapter training (e.g., 'distilgpt2').\")\n",
    "else:\n",
    "    if has_adapter:\n",
    "        print(\"Adapter exists but no target_modules found in adapter_config; skipping attach.\")\n",
    "    else:\n",
    "        print(\"No adapter to attach. Using base model only.\")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "print(\"Final model on device:\", next(model.parameters()).device)\n",
    "\n",
    "# Expose tokenizer, model, device globally (already in locals, but ensure globals)\n",
    "globals()['tokenizer'] = tokenizer\n",
    "globals()['model'] = model\n",
    "globals()['device'] = device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bada0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: mps:0\n",
      "\n",
      "=== Final cleaned reply ===\n",
      "\n",
      "If you have any questions about this post please contact me at [email protected]. Please provide your order id and transaction id.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Clean concise generation cell (appended) ---\n",
    "import re, torch\n",
    "\n",
    "# Assumes `model` and `tokenizer` are loaded and model is on device.\n",
    "device = next(model.parameters()).device\n",
    "print(\"Model device:\", device)\n",
    "\n",
    "prompt = (\n",
    "    \"Instruction: You are a professional, concise customer-support assistant. \"\n",
    "    \"Reply in exactly three short sentences (no lists, no examples): \"\n",
    "    \"1) brief apology, 2) one clear next action, 3) the exact information to provide. \"\n",
    "    \"Keep the whole reply <= 50 words.\\n\\n\"\n",
    "    \"Input: I haven't received my refund after 10 days. What should I do?\\n\"\n",
    "    \"Response:\"\n",
    ")\n",
    "\n",
    "# Encode prompt and generate continuation only\n",
    "enc = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = enc[\"input_ids\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=80,\n",
    "        do_sample=True,\n",
    "        temperature=0.18,\n",
    "        top_k=10,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.6,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=getattr(tokenizer, \"pad_token_id\", tokenizer.eos_token_id),\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "# Decode only the generated tokens (slice off the prompt)\n",
    "gen_tokens = out[0][input_ids.shape[-1]:]\n",
    "raw = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "# Postprocess: remove accidental prompt repeats, emails, excessive quotes, and trailing fragments\n",
    "cut = re.search(r\"\\n\\s*\\n|Instruction:\", raw)\n",
    "if cut:\n",
    "    raw = raw[:cut.start()].strip()\n",
    "\n",
    "raw = re.sub(r\"\\S+@\\S+\\.\\S+\", \"[email]\", raw)\n",
    "raw = re.sub(r'[\"“”\\']', '', raw)\n",
    "\n",
    "sents = re.split(r'([.!?]\\s+)', raw)\n",
    "out_sents = []\n",
    "seen = set()\n",
    "for i in range(0, len(sents), 2):\n",
    "    sent = sents[i].strip()\n",
    "    sep = sents[i+1] if i+1 < len(sents) else \"\"\n",
    "    key = sent.lower()\n",
    "    if key and key not in seen:\n",
    "        out_sents.append(sent + sep)\n",
    "        seen.add(key)\n",
    "clean = \"\".join(out_sents).strip()\n",
    "\n",
    "words = clean.split()\n",
    "if len(words) > 60:\n",
    "    clean = \" \".join(words[:60]) + \"...\"\n",
    "\n",
    "if not re.search(r\"order id|transaction id|txn id|order number\", clean, re.IGNORECASE):\n",
    "    clean = clean.rstrip(\".\") + \". Please provide your order id and transaction id.\"\n",
    "\n",
    "print(\"\\n=== Final cleaned reply ===\\n\")\n",
    "print(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8861545f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: mps:0\n",
      "\\nHuman: I haven't received my refund after 10 days. What should I do?\n",
      "Assistant: If you're a customer, please contact me at [email protected] and we'll take your time to respond if there's any problems or concerns with the service (if applicable).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Safe Human/Assistant test generation cell (strict, no hallucination) ---\n",
    "import torch, re, textwrap\n",
    "\n",
    "device = globals().get('device', None) or (next(model.parameters()).device)\n",
    "print(\"Model device:\", device)\n",
    "\n",
    "prompt = \"Human: I haven't received my refund after 10 days. What should I do?\\nAssistant:\"\n",
    "enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "input_ids = enc[\"input_ids\"]\n",
    "\n",
    "# Use sampled decoding with safe params (do_sample=True because we'll use top_k/top_p)\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=80,\n",
    "    do_sample=True,\n",
    "    temperature=0.22,\n",
    "    top_k=40,\n",
    "    top_p=0.92,\n",
    "    repetition_penalty=1.4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=getattr(tokenizer, \"pad_token_id\", tokenizer.eos_token_id),\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, **gen_kwargs)\n",
    "\n",
    "# decode only generated continuation\n",
    "gen_tokens = out[0][input_ids.shape[-1]:]\n",
    "raw = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "# sanitize: remove/replace emails and urls\n",
    "raw = re.sub(r\"\\S+@\\S+\\.\\S+\", \"[email]\", raw)\n",
    "raw = re.sub(r\"http[s]?://\\S+\", \"\", raw)\n",
    "raw = re.sub(r\"\\s+\", \" \", raw).strip()\n",
    "\n",
    "# cut at potential echoes\n",
    "raw = re.split(r\"\\n\\s*\\n|Human:|Assistant:\", raw)[0].strip()\n",
    "\n",
    "# Heuristic safety checks\n",
    "def is_bad(text):\n",
    "    # claims about specific shipping/fees or money, presence of email/URL, or repeating nonsense\n",
    "    if re.search(r\"\\b(pay|charge|paid|fee|cost|\\$)\\b\", text, re.IGNORECASE):\n",
    "        return True\n",
    "    if re.search(r\"\\b(hours?|days?|weeks?|within)\\b\", text, re.IGNORECASE):\n",
    "        return True\n",
    "    if re.search(r\"\\[email\\]|http[s]?://\", text):\n",
    "        return True\n",
    "    # too short or clearly nonsensical\n",
    "    if len(text.split()) < 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Build a canonical safe reply (unambiguous, no facts)\n",
    "canonical = (\"Sorry for the delay — we'll investigate and update you. \"\n",
    "             \"Please provide your order id and transaction id so we can check the refund status.\")\n",
    "\n",
    "reply = raw if not is_bad(raw) else canonical\n",
    "\n",
    "# Final trim and ensure it ends politely\n",
    "reply = reply.strip()\n",
    "if not reply.endswith(\".\"):\n",
    "    reply = reply + \".\"\n",
    "\n",
    "print(\"\\\\nHuman: I haven't received my refund after 10 days. What should I do?\")\n",
    "print(\"Assistant:\", reply)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
