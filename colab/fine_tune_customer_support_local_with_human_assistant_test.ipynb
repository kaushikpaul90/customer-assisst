{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16ddccf",
   "metadata": {},
   "source": [
    "# Fine-tune a small LM on a customer-support dataset (Local MacBook)\n",
    "\n",
    "This notebook is adapted to run on a local MacBook (CPU or Apple MPS). It uses a small model (`distilgpt2`) by default and is conservative with batch sizes/epochs so it can run locally for demonstration. If the public dataset is unavailable the notebook falls back to a tiny synthetic dataset.\n",
    "\n",
    "### How to use\n",
    "1. (Optional) In the first code cell uncomment the `!pip install` line to install dependencies.\n",
    "2. Adjust settings in the `User settings` cell (model, dataset, epochs, batch_size).\n",
    "3. Run cells top-to-bottom. Training on CPU/MPS is slow; expect longer runtimes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce7e1987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_MODEL_ID: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- BASE model helper ---\n",
    "# Ensure BASE_MODEL_ID is defined. Set this to the original base model used for fine-tuning.\n",
    "# Example: BASE_MODEL_ID = \"distilgpt2\"\n",
    "if 'BASE_MODEL_ID' not in globals():\n",
    "    try:\n",
    "        BASE_MODEL_ID = MODEL_ID\n",
    "    except NameError:\n",
    "        BASE_MODEL_ID = None\n",
    "print(\"BASE_MODEL_ID:\", BASE_MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8154db2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (4.4.1)\n",
      "Requirement already satisfied: accelerate in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (1.11.0)\n",
      "Requirement already satisfied: peft in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (0.17.1)\n",
      "Requirement already satisfied: bitsandbytes in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (0.42.0)\n",
      "Requirement already satisfied: trl in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (0.25.1)\n",
      "Requirement already satisfied: filelock in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from accelerate) (2.9.0)\n",
      "Requirement already satisfied: scipy in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from bitsandbytes) (1.16.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: setuptools in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers datasets accelerate peft bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f295ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Prevent tokenizers parallelism warnings & deadlocks\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "import torch\n",
    "# If using MPS or CPU, pin_memory should be False to avoid warnings\n",
    "# We'll set a flag used in TrainingArguments below\n",
    "use_pin_memory = False if (not torch.cuda.is_available()) else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34002140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps (model dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "# Device selection (robust)\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif getattr(torch.backends, 'mps', None) is not None and torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# Choose dtype depending on device to avoid unsupported dtypes on CPU\n",
    "if device == 'cuda' or device == 'mps':\n",
    "    model_dtype = torch.float16\n",
    "else:\n",
    "    model_dtype = torch.float32\n",
    "\n",
    "print(f'Using device: {device} (model dtype={model_dtype})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d520af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      " MODEL_ID=microsoft/Phi-3-mini-4k-instruct\n",
      " DATASET_ID=bitext/Bitext-customer-support-llm-chatbot-training-dataset\n",
      " OUTPUT_DIR=./local_ft_output\n",
      " EPOCHS=3, BATCH_SIZE=1, MAX_LENGTH=256, USE_LORA=True\n"
     ]
    }
   ],
   "source": [
    "# ----------------- User settings (adjust before run) -----------------\n",
    "MODEL_ID = os.environ.get('MODEL_ID', 'microsoft/Phi-3-mini-4k-instruct') # CHANGED\n",
    "DATASET_ID = os.environ.get('DATASET_ID', 'bitext/Bitext-customer-support-llm-chatbot-training-dataset')\n",
    "OUTPUT_DIR = os.environ.get('OUTPUT_DIR', './local_ft_output') # CHANGED OUTPUT DIR to avoid mixing checkpoints\n",
    "EPOCHS = int(os.environ.get('EPOCHS', '3'))\n",
    "BATCH_SIZE = int(os.environ.get('BATCH_SIZE', '1')) # Adjusted BATCH_SIZE for larger model\n",
    "MAX_LENGTH = int(os.environ.get('MAX_LENGTH', '256')) # Increased length for modern model\n",
    "USE_LORA = os.environ.get('USE_LORA', 'true').lower() in ('1', 'true', 'yes')\n",
    "\n",
    "print('Settings:')\n",
    "print(f' MODEL_ID={MODEL_ID}')\n",
    "print(f' DATASET_ID={DATASET_ID}')\n",
    "print(f' OUTPUT_DIR={OUTPUT_DIR}')\n",
    "print(f' EPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}, MAX_LENGTH={MAX_LENGTH}, USE_LORA={USE_LORA}')\n",
    "# --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d02fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def safe_load_customer_dataset(dataset_id):\n",
    "    try:\n",
    "        ds = load_dataset(dataset_id)\n",
    "        if isinstance(ds, dict) and 'train' in ds:\n",
    "            return ds['train']\n",
    "        return ds\n",
    "    except Exception as e:\n",
    "        print(f'Could not load dataset {dataset_id}: {e}')\n",
    "        print('Falling back to a tiny synthetic customer support dataset for demo.')\n",
    "        samples = [\n",
    "            {'customer': \"My order hasn't arrived, it's been 10 days.\", 'agent': \"I'm sorry. Can you share your order id?\"},\n",
    "            {'customer': 'I was charged twice for the same order.', 'agent': \"I can help. Please share the transaction id.\"},\n",
    "            {'customer': 'How do I return an item?', 'agent': \"You can start a return from your orders page.\"},\n",
    "        ]\n",
    "        return Dataset.from_list(samples)\n",
    "\n",
    "def build_prompt(row):\n",
    "    if 'customer' in row and 'agent' in row:\n",
    "        return f\"Human: {row['customer']}\\nAssistant: {row['agent']}\\n\"\n",
    "    if 'input' in row and 'output' in row:\n",
    "        return f\"Human: {row['input']}\\nAssistant: {row['output']}\\n\"\n",
    "    if 'text' in row:\n",
    "        return row['text'] + \"\\n\"\n",
    "    return str(row)\n",
    "\n",
    "print('Helper functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9be4cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset size: 26872 (showing first 2 examples)\n",
      "\n",
      "--- example 0 ---\n",
      "flags\n",
      "\n",
      "--- example 1 ---\n",
      "instruction\n",
      "\n",
      "--- example 2 ---\n",
      "category\n",
      "\n",
      "--- example 3 ---\n",
      "intent\n",
      "\n",
      "--- example 4 ---\n",
      "response\n",
      "Train size: 4096, Eval size: 128\n"
     ]
    }
   ],
   "source": [
    "raw_ds = safe_load_customer_dataset(DATASET_ID)\n",
    "print(f'Loaded dataset size: {len(raw_ds)} (showing first 2 examples)')\n",
    "for i,ex in enumerate(raw_ds[:2]):\n",
    "    print('\\n--- example', i, '---')\n",
    "    print(ex)\n",
    "\n",
    "# Map to text prompts\n",
    "if isinstance(raw_ds[0], dict):\n",
    "    def map_to_prompt(example):\n",
    "        return {'text': build_prompt(example)}\n",
    "    ds = raw_ds.map(map_to_prompt)\n",
    "else:\n",
    "    ds = raw_ds.map(lambda x: {'text': str(x)})\n",
    "\n",
    "# Split and reduce for local run\n",
    "if len(ds) > 2000:\n",
    "    ds = ds.train_test_split(test_size=0.05, shuffle=True, seed=42)\n",
    "    train_ds = ds['train'].select(range(4096))\n",
    "    eval_ds = ds['test'].select(range(128))\n",
    "else:\n",
    "    split = ds.train_test_split(test_size=0.1, seed=42)\n",
    "    train_ds = split['train']\n",
    "    eval_ds = split['test']\n",
    "\n",
    "print(f'Train size: {len(train_ds)}, Eval size: {len(eval_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d00f04d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping dataset to single 'text' column using TEMPLATE = A\n",
      "Mapping complete. Columns: ['flags', 'instruction', 'category', 'intent', 'response', 'text']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Prompt template setup (inserted) ---\n",
    "TEMPLATE = \"A\"  # \"A\" = Human/Assistant, \"B\" = Instruction/Input/Response\n",
    "\n",
    "def build_prompt_for_training(row):\n",
    "    if TEMPLATE == \"A\":\n",
    "        if isinstance(row, dict) and 'customer' in row and 'agent' in row:\n",
    "            prompt = f\"Human: {row['customer']}\\nAssistant: {row['agent']}\\n\"\n",
    "        elif isinstance(row, dict) and 'input' in row and 'output' in row:\n",
    "            prompt = f\"Human: {row['input']}\\nAssistant: {row['output']}\\n\"\n",
    "        elif isinstance(row, dict) and 'text' in row:\n",
    "            prompt = row['text']\n",
    "        else:\n",
    "            prompt = str(row)\n",
    "    else:\n",
    "        instruction = row.get('instruction', \"Answer the customer support query.\") if isinstance(row, dict) else \"Answer the customer support query.\"\n",
    "        input_text = row.get('customer') or row.get('input') or row.get('context') or \"\" if isinstance(row, dict) else \"\"\n",
    "        response = row.get('agent') or row.get('output') or row.get('response') or \"\" if isinstance(row, dict) else \"\"\n",
    "        prompt = f\"Instruction: {instruction}\\nInput: {input_text}\\nResponse: {response}\\n\"\n",
    "    return {'text': prompt}\n",
    "\n",
    "# Example and mapping (only run this mapping when raw_ds exists)\n",
    "try:\n",
    "    print(\"Mapping dataset to single 'text' column using TEMPLATE =\", TEMPLATE)\n",
    "    ds = raw_ds.map(build_prompt_for_training)\n",
    "    print(\"Mapping complete. Columns:\", ds.column_names)\n",
    "except NameError:\n",
    "    print(\"raw_ds not found in this scope; ensure you run the cell after loading raw_ds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02a791e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded.\n",
      "Sample ds[0]['text'] type: <class 'str'>\n",
      "Sample text preview: '{\\'flags\\': \\'B\\', \\'instruction\\': \\'question about cancelling order {{Order Number}}\\', \\'category\\': \\'ORDER\\', \\'intent\\': \\'cancel_order\\', \\'response\\': \"I\\'ve understood you have a question re\n",
      "Tokenizing datasets (this may take a bit)...\n",
      "Tokenization complete. Examples:\n",
      "{'flags': 'BILQ', 'instruction': 'what do i have to do to recover my profile key', 'category': 'ACCOUNT', 'intent': 'recover_password', 'response': 'Indeed! I\\'m here to assist you in recovering your profile key. Let\\'s tackle this together:\\n\\n1. Access our platform\\'s \"{{Login Page URL}}\" to initiate the recovery process.\\n2. Locate the \"{{Forgot Password}}\" option and select it to proceed.\\n3. You will be prompted to provide your email address associated with your profile. Kindly input the relevant information.\\n4. Keep an eye on your inbox as you should receive an email containing detailed instructions on how to recover your profile key. In case you don\\'t find it in your primary inbox, please check your spam or other folders.\\n5. Follow the instructions provided in the email carefully to regain access to your profile key.\\n\\nShould you face any hurdles or experience delays in receiving the email, don\\'t hesitate to reach out. Remember, I\\'m here to support you every step of the way!'}\n",
      "{'flags': 'BL', 'instruction': \"I don't know how I can change to the {{Account Category}} account\", 'category': 'ACCOUNT', 'intent': 'switch_account', 'response': 'I appreciate your interest in upgrading to the {{Account Category}} account! Let me guide you through the process step by step:\\n\\n1. Access Your Account: Begin by logging into our platform using your credentials.\\n2. Navigate to Account or Profile Settings: Look for the \"{{Settings}}\" or \"{{Profile}}\" section within your account.\\n3. Explore Upgrade Options: Within the Account/Profile Settings, you should find an option labeled \"{{Upgrade Account}}\" or \"{{Account Change}}\".\\n4. Choose {{Account Category}}: Select the \"{{Account Type}}\" option from the list of available upgrades.\\n5. Follow the Prompts: Once you\\'ve made your selection, follow the on-screen instructions to finalize the switch to the {{Account Category}} account.\\n\\nIf you encounter any difficulties during the process or have any additional questions, please don\\'t hesitate to reach out. We\\'re here to ensure a smooth and enjoyable transition to your new {{Account Category}} account.'}\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Tokenization and Model Loading -----------------\n",
    "# Load tokenizer and model with a dtype appropriate for the device\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "# ensure pad token exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|padding|>'})\n",
    "\n",
    "# Load the model. Avoid forcing float16 on CPU.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=model_dtype,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Resize token embeddings if we added special tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print('Model and tokenizer loaded.')\n",
    "\n",
    "# Tokenization helper that uses the 'text' column (created earlier by map_to_prompt)\n",
    "# --- Robust tokenize_for_lm (inserted) ---\n",
    "def tokenize_for_lm(examples):\n",
    "    # examples['text'] should be a list[str] after mapping\n",
    "    texts = examples.get('text', [])\n",
    "    normalized = []\n",
    "    for t in texts:\n",
    "        if t is None:\n",
    "            normalized.append(\"\")\n",
    "        elif isinstance(t, str):\n",
    "            normalized.append(t)\n",
    "        elif isinstance(t, (list, tuple)):\n",
    "            flat = []\n",
    "            for x in t:\n",
    "                if isinstance(x, (list, tuple)):\n",
    "                    flat += [str(y) for y in x]\n",
    "                else:\n",
    "                    flat.append(str(x))\n",
    "            normalized.append(\" \".join(flat))\n",
    "        else:\n",
    "            normalized.append(str(t))\n",
    "    outputs = tokenizer(\n",
    "        normalized,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "    return outputs\n",
    "\n",
    "# Sanity check helper\n",
    "try:\n",
    "    print(\"Sample ds[0]['text'] type:\", type(ds[0]['text']))\n",
    "    print(\"Sample text preview:\", repr(ds[0]['text'])[:200])\n",
    "except NameError:\n",
    "    print(\"ds not available yet - run this after mapping the dataset.\")\n",
    "\n",
    "\n",
    "print('Tokenizing datasets (this may take a bit)...')\n",
    "train_tok = train_ds.map(tokenize_for_lm, batched=True, remove_columns=[c for c in train_ds.column_names if c!='text'])\n",
    "eval_tok = eval_ds.map(tokenize_for_lm, batched=True, remove_columns=[c for c in eval_ds.column_names if c!='text'])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "print('Tokenization complete. Examples:')\n",
    "for i in range(min(2, len(train_tok))):\n",
    "    print(train_tok[i]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf7d276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for training...\n",
      "Using device=mps, dtype=torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded and moved to mps\n"
     ]
    }
   ],
   "source": [
    "# --- Safe model initialization before training ---\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print(\"Preparing model for training...\")\n",
    "\n",
    "if 'MODEL_ID' not in globals():\n",
    "    raise RuntimeError(\"Please define MODEL_ID (e.g. 'gpt2', 'distilgpt2', etc.) before running this cell.\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "\n",
    "# Pick device and dtype\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\")\n",
    "dtype = torch.float16 if device in (\"cuda\", \"mps\") else torch.float32\n",
    "print(f\"Using device={device}, dtype={dtype}\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Resize embeddings (in case tokenizer was modified)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "print(\"✅ Model loaded and moved to\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2043dda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "Applied LoRA adapter to model (PEFT).\n",
      "use_peft = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaushikpaul90/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "# ----------------- PEFT / LoRA Configuration (optional) -----------------\n",
    "use_peft = False\n",
    "if USE_LORA:\n",
    "    try:\n",
    "        from peft import LoraConfig, get_peft_model\n",
    "        use_peft = True\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        print('Applied LoRA adapter to model (PEFT).')\n",
    "    except Exception as e:\n",
    "        print(f'PEFT/LoRA unavailable or failed: {e}. Continuing without LoRA.')\n",
    "\n",
    "print('use_peft =', use_peft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc7badfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model from MODEL_ID = microsoft/Phi-3-mini-4k-instruct\n",
      "Device: mps dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer are ready. Model on device: mps:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Ensure model & tokenizer exist before Trainer ---\n",
    "# This cell is inserted to guarantee `model` and `tokenizer` are defined.\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "if 'MODEL_ID' not in globals():\n",
    "    raise RuntimeError(\"Please define MODEL_ID before running this cell (e.g., MODEL_ID='gpt2').\")\n",
    "\n",
    "print(\"Preparing model from MODEL_ID =\", MODEL_ID)\n",
    "\n",
    "# Load tokenizer (and ensure pad token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<|pad|>\"})\n",
    "\n",
    "# Choose device and dtype\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\")\n",
    "dtype = torch.float16 if device in (\"cuda\",\"mps\") else torch.float32\n",
    "print(\"Device:\", device, \"dtype:\", dtype)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Resize embeddings to tokenizer (just in case)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model and tokenizer are ready. Model on device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f2ff4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training... (this may be slow on CPU/MPS)\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Training Arguments and Trainer -----------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    fp16=(device in ('cuda', 'mps')),\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=use_pin_memory\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print('Starting training... (this may be slow on CPU/MPS)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f59c556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OUTPUT_DIR: ./local_ft_output\n",
      "Using BASE_MODEL_ID: microsoft/Phi-3-mini-4k-instruct\n",
      "Tokenizer size: 50258\n",
      "Original base config.vocab_size: 32064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded. Embedding shape (before): torch.Size([32064, 3072])\n",
      "Resizing token embeddings: 32064 -> 50258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape (after): torch.Size([50258, 3072])\n",
      "PEFT adapter detected in OUTPUT_DIR. Attaching adapter...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target modules {'v_attn', 'c_attn', 'q_attn'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(os.path.join(OUTPUT_DIR, \u001b[33m\"\u001b[39m\u001b[33madapter_config.json\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPEFT adapter detected in OUTPUT_DIR. Attaching adapter...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     model = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAdapter attached.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages/peft/peft_model.py:547\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m    539\u001b[39m     model = \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m    540\u001b[39m         model,\n\u001b[32m    541\u001b[39m         config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    544\u001b[39m         low_cpu_mem_usage=low_cpu_mem_usage,\n\u001b[32m    545\u001b[39m     )\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m     model = \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m load_result = model.load_adapter(\n\u001b[32m    556\u001b[39m     model_id,\n\u001b[32m    557\u001b[39m     adapter_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    562\u001b[39m     **kwargs,\n\u001b[32m    563\u001b[39m )\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# 1. Remove VB-LoRA vector bank, since it's a shared parameter set via the VBLoRAModel\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m# 2. Remove the prompt encoder, as it does not need to be part of the checkpoint\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages/peft/peft_model.py:1815\u001b[39m, in \u001b[36mPeftModelForCausalLM.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, **kwargs)\u001b[39m\n\u001b[32m   1812\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m   1813\u001b[39m     \u001b[38;5;28mself\u001b[39m, model: torch.nn.Module, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m, **kwargs\n\u001b[32m   1814\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1815\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1816\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model_prepare_inputs_for_generation = \u001b[38;5;28mself\u001b[39m.base_model.prepare_inputs_for_generation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages/peft/peft_model.py:130\u001b[39m, in \u001b[36mPeftModel.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    128\u001b[39m     ctx = init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m         \u001b[38;5;28mself\u001b[39m.base_model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_model, \u001b[33m\"\u001b[39m\u001b[33m_cast_adapter_dtype\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    133\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model._cast_adapter_dtype(\n\u001b[32m    134\u001b[39m         adapter_name=adapter_name, autocast_adapter_dtype=autocast_adapter_dtype\n\u001b[32m    135\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:209\u001b[39m, in \u001b[36mBaseTuner.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, low_cpu_mem_usage, state_dict)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28mself\u001b[39m._pre_injection_hook(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.peft_config[adapter_name], adapter_name)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config != PeftType.XLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] != PeftType.XLORA:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;28mself\u001b[39m.model.peft_config = \u001b[38;5;28mself\u001b[39m.peft_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/API-Driven Cloud Native Solutions/customer-assist/.venv/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:654\u001b[39m, in \u001b[36mBaseTuner.inject_adapter\u001b[39m\u001b[34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage, state_dict)\u001b[39m\n\u001b[32m    652\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(peft_config, \u001b[33m\"\u001b[39m\u001b[33mlayers_pattern\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    653\u001b[39m         error_msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m You also specified \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlayers_pattern\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config.layers_pattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    656\u001b[39m     \u001b[38;5;66;03m# Some modules did not match and some matched but were excluded\u001b[39;00m\n\u001b[32m    657\u001b[39m     error_msg = (\n\u001b[32m    658\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo modules were targeted for adaptation. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    659\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis might be caused by a combination of mismatched target modules and excluded modules. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    660\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check your `target_modules` and `exclude_modules` configuration. You may also have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    661\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33monly targeted modules that are marked to be saved (`modules_to_save`).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    662\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Target modules {'v_attn', 'c_attn', 'q_attn'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Robust config+model loader (automatically inserted) ---\n",
    "# This cell replaces previous AutoConfig.from_pretrained calls to avoid loading broken config from OUTPUT_DIR.\n",
    "import os, torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "OUTPUT_DIR = globals().get(\"OUTPUT_DIR\", \"./local_ft_output\")\n",
    "BASE_MODEL_ID = globals().get(\"BASE_MODEL_ID\", None) or globals().get(\"MODEL_ID\", None)\n",
    "if BASE_MODEL_ID is None:\n",
    "    raise RuntimeError(\"BASE_MODEL_ID or MODEL_ID must be set before this cell. Set MODEL_ID = 'distilgpt2' or similar.\")\n",
    "\n",
    "print(\"Using OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "print(\"Using BASE_MODEL_ID:\", BASE_MODEL_ID)\n",
    "\n",
    "# Load tokenizer from OUTPUT_DIR (must exist)\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR, trust_remote_code=False)\n",
    "tok_size = len(tokenizer)\n",
    "print(\"Tokenizer size:\", tok_size)\n",
    "\n",
    "# Load base config from BASE_MODEL_ID (safe)\n",
    "config = AutoConfig.from_pretrained(BASE_MODEL_ID, trust_remote_code=False)\n",
    "print(\"Original base config.vocab_size:\", getattr(config, \"vocab_size\", None))\n",
    "# Do not force config.vocab_size before loading weights; we'll allow mismatched sizes and resize after loading.\n",
    "\n",
    "# Load base model with ignore_mismatched_sizes to avoid embedding errors\n",
    "dtype = torch.float16 if torch.backends.mps.is_available() or torch.cuda.is_available() else torch.float32\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    config=config,\n",
    "    dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "print(\"Base model loaded. Embedding shape (before):\", base_model.get_input_embeddings().weight.shape)\n",
    "\n",
    "# Resize token embeddings to tokenizer size if needed\n",
    "if base_model.get_input_embeddings().weight.shape[0] != tok_size:\n",
    "    print(\"Resizing token embeddings:\", base_model.get_input_embeddings().weight.shape[0], \"->\", tok_size)\n",
    "    base_model.resize_token_embeddings(tok_size)\n",
    "print(\"Embedding shape (after):\", base_model.get_input_embeddings().weight.shape)\n",
    "\n",
    "# If OUTPUT_DIR contains a PEFT adapter, attach it\n",
    "if os.path.exists(os.path.join(OUTPUT_DIR, \"adapter_config.json\")):\n",
    "    print(\"PEFT adapter detected in OUTPUT_DIR. Attaching adapter...\")\n",
    "    model = PeftModel.from_pretrained(base_model, OUTPUT_DIR, torch_dtype=dtype, device_map=None)\n",
    "    print(\"Adapter attached.\")\n",
    "else:\n",
    "    model = base_model\n",
    "    print(\"No adapter detected; using base model.\")\n",
    "\n",
    "# Move model to device (cpu/mps/cuda)\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Model is on device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bada0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Clean concise generation cell (appended) ---\n",
    "import re, torch\n",
    "\n",
    "# Assumes `model` and `tokenizer` are loaded and model is on device.\n",
    "device = next(model.parameters()).device\n",
    "print(\"Model device:\", device)\n",
    "\n",
    "prompt = (\n",
    "    \"Instruction: You are a professional, concise customer-support assistant. \"\n",
    "    \"Reply in exactly three short sentences (no lists, no examples): \"\n",
    "    \"1) brief apology, 2) one clear next action, 3) the exact information to provide. \"\n",
    "    \"Keep the whole reply <= 50 words.\\n\\n\"\n",
    "    \"Input: I haven't received my refund after 10 days. What should I do?\\n\"\n",
    "    \"Response:\"\n",
    ")\n",
    "\n",
    "# Encode prompt and generate continuation only\n",
    "enc = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "input_ids = enc[\"input_ids\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=80,\n",
    "        do_sample=True,\n",
    "        temperature=0.18,\n",
    "        top_k=10,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.6,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=getattr(tokenizer, \"pad_token_id\", tokenizer.eos_token_id),\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "# Decode only the generated tokens (slice off the prompt)\n",
    "gen_tokens = out[0][input_ids.shape[-1]:]\n",
    "raw = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "# Postprocess: remove accidental prompt repeats, emails, excessive quotes, and trailing fragments\n",
    "cut = re.search(r\"\\n\\s*\\n|Instruction:\", raw)\n",
    "if cut:\n",
    "    raw = raw[:cut.start()].strip()\n",
    "\n",
    "raw = re.sub(r\"\\S+@\\S+\\.\\S+\", \"[email]\", raw)\n",
    "raw = re.sub(r'[\"“”\\']', '', raw)\n",
    "\n",
    "sents = re.split(r'([.!?]\\s+)', raw)\n",
    "out_sents = []\n",
    "seen = set()\n",
    "for i in range(0, len(sents), 2):\n",
    "    sent = sents[i].strip()\n",
    "    sep = sents[i+1] if i+1 < len(sents) else \"\"\n",
    "    key = sent.lower()\n",
    "    if key and key not in seen:\n",
    "        out_sents.append(sent + sep)\n",
    "        seen.add(key)\n",
    "clean = \"\".join(out_sents).strip()\n",
    "\n",
    "words = clean.split()\n",
    "if len(words) > 60:\n",
    "    clean = \" \".join(words[:60]) + \"...\"\n",
    "\n",
    "if not re.search(r\"order id|transaction id|txn id|order number\", clean, re.IGNORECASE):\n",
    "    clean = clean.rstrip(\".\") + \". Please provide your order id and transaction id.\"\n",
    "\n",
    "print(\"\\n=== Final cleaned reply ===\\n\")\n",
    "print(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65892127",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Human / Assistant test generation cell ---\n",
    "import torch, re\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "print(\"Model device:\", device)\n",
    "\n",
    "prompt = \"Human: I haven't received my refund after 10 days. What should I do?\\nAssistant:\"\n",
    "\n",
    "enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "input_ids = enc[\"input_ids\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=120,\n",
    "        do_sample=True,\n",
    "        temperature=0.25,\n",
    "        top_k=40,\n",
    "        top_p=0.92,\n",
    "        repetition_penalty=1.5,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=getattr(tokenizer, \"pad_token_id\", tokenizer.eos_token_id),\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "gen_tokens = out[0][input_ids.shape[-1]:]\n",
    "reply = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "cut = re.search(r\"\\n\\s*\\n|Human:|Assistant:\", reply)\n",
    "if cut:\n",
    "    reply = reply[:cut.start()].strip()\n",
    "\n",
    "reply = re.sub(r\"\\S+@\\S+\\.\\S+\", \"[email]\", reply)\n",
    "reply = reply.replace('\"', \"\").replace(\"“\", \"\").replace(\"”\", \"\")\n",
    "\n",
    "words = reply.split()\n",
    "if len(words) > 80:\n",
    "    reply = \" \".join(words[:80]) + \"...\"\n",
    "\n",
    "print(\"\\nHuman: I haven't received my refund after 10 days. What should I do?\")\n",
    "print(\"Assistant:\", reply)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
