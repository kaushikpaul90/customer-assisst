{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 12288,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00244140625,
      "grad_norm": 0.3111059069633484,
      "learning_rate": 4.996337890625e-05,
      "loss": 3.4873,
      "step": 10
    },
    {
      "epoch": 0.0048828125,
      "grad_norm": 0.699802041053772,
      "learning_rate": 4.992268880208333e-05,
      "loss": 3.3692,
      "step": 20
    },
    {
      "epoch": 0.00732421875,
      "grad_norm": 0.7378807663917542,
      "learning_rate": 4.988199869791667e-05,
      "loss": 3.2839,
      "step": 30
    },
    {
      "epoch": 0.009765625,
      "grad_norm": 1.0126392841339111,
      "learning_rate": 4.984130859375e-05,
      "loss": 3.1271,
      "step": 40
    },
    {
      "epoch": 0.01220703125,
      "grad_norm": 1.289618968963623,
      "learning_rate": 4.980061848958333e-05,
      "loss": 3.1195,
      "step": 50
    },
    {
      "epoch": 0.0146484375,
      "grad_norm": 1.4598439931869507,
      "learning_rate": 4.975992838541667e-05,
      "loss": 3.2411,
      "step": 60
    },
    {
      "epoch": 0.01708984375,
      "grad_norm": 1.6261401176452637,
      "learning_rate": 4.971923828125e-05,
      "loss": 3.1667,
      "step": 70
    },
    {
      "epoch": 0.01953125,
      "grad_norm": 1.7387584447860718,
      "learning_rate": 4.967854817708334e-05,
      "loss": 3.1108,
      "step": 80
    },
    {
      "epoch": 0.02197265625,
      "grad_norm": 1.9674650430679321,
      "learning_rate": 4.963785807291667e-05,
      "loss": 3.0814,
      "step": 90
    },
    {
      "epoch": 0.0244140625,
      "grad_norm": 1.2315795421600342,
      "learning_rate": 4.959716796875e-05,
      "loss": 2.9784,
      "step": 100
    },
    {
      "epoch": 0.02685546875,
      "grad_norm": 1.4959311485290527,
      "learning_rate": 4.955647786458334e-05,
      "loss": 2.7674,
      "step": 110
    },
    {
      "epoch": 0.029296875,
      "grad_norm": 2.3956661224365234,
      "learning_rate": 4.951578776041667e-05,
      "loss": 2.9595,
      "step": 120
    },
    {
      "epoch": 0.03173828125,
      "grad_norm": 1.7064237594604492,
      "learning_rate": 4.947509765625e-05,
      "loss": 2.8742,
      "step": 130
    },
    {
      "epoch": 0.0341796875,
      "grad_norm": 1.8425489664077759,
      "learning_rate": 4.943440755208334e-05,
      "loss": 3.0237,
      "step": 140
    },
    {
      "epoch": 0.03662109375,
      "grad_norm": 2.25722336769104,
      "learning_rate": 4.939371744791667e-05,
      "loss": 2.8702,
      "step": 150
    },
    {
      "epoch": 0.0390625,
      "grad_norm": 2.754899501800537,
      "learning_rate": 4.935302734375e-05,
      "loss": 2.7283,
      "step": 160
    },
    {
      "epoch": 0.04150390625,
      "grad_norm": 2.643195390701294,
      "learning_rate": 4.931233723958334e-05,
      "loss": 2.8469,
      "step": 170
    },
    {
      "epoch": 0.0439453125,
      "grad_norm": 2.1124322414398193,
      "learning_rate": 4.927164713541667e-05,
      "loss": 2.8981,
      "step": 180
    },
    {
      "epoch": 0.04638671875,
      "grad_norm": 2.69604754447937,
      "learning_rate": 4.923095703125e-05,
      "loss": 2.801,
      "step": 190
    },
    {
      "epoch": 0.048828125,
      "grad_norm": 2.0242786407470703,
      "learning_rate": 4.919026692708334e-05,
      "loss": 2.9299,
      "step": 200
    },
    {
      "epoch": 0.05126953125,
      "grad_norm": 3.1908087730407715,
      "learning_rate": 4.914957682291667e-05,
      "loss": 2.9233,
      "step": 210
    },
    {
      "epoch": 0.0537109375,
      "grad_norm": 2.5421218872070312,
      "learning_rate": 4.9108886718750006e-05,
      "loss": 2.8465,
      "step": 220
    },
    {
      "epoch": 0.05615234375,
      "grad_norm": 2.414931297302246,
      "learning_rate": 4.906819661458334e-05,
      "loss": 2.7444,
      "step": 230
    },
    {
      "epoch": 0.05859375,
      "grad_norm": 2.128598928451538,
      "learning_rate": 4.902750651041667e-05,
      "loss": 2.8778,
      "step": 240
    },
    {
      "epoch": 0.06103515625,
      "grad_norm": 2.8255598545074463,
      "learning_rate": 4.8986816406250006e-05,
      "loss": 2.7458,
      "step": 250
    },
    {
      "epoch": 0.0634765625,
      "grad_norm": 2.1792643070220947,
      "learning_rate": 4.8946126302083337e-05,
      "loss": 2.6267,
      "step": 260
    },
    {
      "epoch": 0.06591796875,
      "grad_norm": 2.6833062171936035,
      "learning_rate": 4.890543619791667e-05,
      "loss": 2.6941,
      "step": 270
    },
    {
      "epoch": 0.068359375,
      "grad_norm": 2.922833204269409,
      "learning_rate": 4.8864746093750005e-05,
      "loss": 2.7359,
      "step": 280
    },
    {
      "epoch": 0.07080078125,
      "grad_norm": 3.291627883911133,
      "learning_rate": 4.8824055989583336e-05,
      "loss": 2.7615,
      "step": 290
    },
    {
      "epoch": 0.0732421875,
      "grad_norm": 3.6004858016967773,
      "learning_rate": 4.878336588541667e-05,
      "loss": 2.7379,
      "step": 300
    },
    {
      "epoch": 0.07568359375,
      "grad_norm": 3.2444565296173096,
      "learning_rate": 4.8742675781250005e-05,
      "loss": 2.8828,
      "step": 310
    },
    {
      "epoch": 0.078125,
      "grad_norm": 2.683443069458008,
      "learning_rate": 4.8701985677083336e-05,
      "loss": 2.6959,
      "step": 320
    },
    {
      "epoch": 0.08056640625,
      "grad_norm": 2.652325391769409,
      "learning_rate": 4.8661295572916674e-05,
      "loss": 2.7435,
      "step": 330
    },
    {
      "epoch": 0.0830078125,
      "grad_norm": 2.806241512298584,
      "learning_rate": 4.8620605468750005e-05,
      "loss": 2.653,
      "step": 340
    },
    {
      "epoch": 0.08544921875,
      "grad_norm": 3.2999930381774902,
      "learning_rate": 4.8579915364583336e-05,
      "loss": 2.7788,
      "step": 350
    },
    {
      "epoch": 0.087890625,
      "grad_norm": 2.356250047683716,
      "learning_rate": 4.8539225260416674e-05,
      "loss": 2.6608,
      "step": 360
    },
    {
      "epoch": 0.09033203125,
      "grad_norm": 2.6179981231689453,
      "learning_rate": 4.8498535156250005e-05,
      "loss": 2.5191,
      "step": 370
    },
    {
      "epoch": 0.0927734375,
      "grad_norm": 3.043982982635498,
      "learning_rate": 4.8457845052083336e-05,
      "loss": 2.7129,
      "step": 380
    },
    {
      "epoch": 0.09521484375,
      "grad_norm": 3.1430516242980957,
      "learning_rate": 4.8417154947916674e-05,
      "loss": 2.7099,
      "step": 390
    },
    {
      "epoch": 0.09765625,
      "grad_norm": 3.4783427715301514,
      "learning_rate": 4.8376464843750005e-05,
      "loss": 2.7241,
      "step": 400
    },
    {
      "epoch": 0.10009765625,
      "grad_norm": 3.7419843673706055,
      "learning_rate": 4.8335774739583336e-05,
      "loss": 2.6095,
      "step": 410
    },
    {
      "epoch": 0.1025390625,
      "grad_norm": 5.229518413543701,
      "learning_rate": 4.8295084635416674e-05,
      "loss": 2.5536,
      "step": 420
    },
    {
      "epoch": 0.10498046875,
      "grad_norm": 4.020813465118408,
      "learning_rate": 4.8254394531250005e-05,
      "loss": 2.6601,
      "step": 430
    },
    {
      "epoch": 0.107421875,
      "grad_norm": 4.373058795928955,
      "learning_rate": 4.8213704427083336e-05,
      "loss": 2.6476,
      "step": 440
    },
    {
      "epoch": 0.10986328125,
      "grad_norm": 3.5219244956970215,
      "learning_rate": 4.8173014322916674e-05,
      "loss": 2.6395,
      "step": 450
    },
    {
      "epoch": 0.1123046875,
      "grad_norm": 3.918148994445801,
      "learning_rate": 4.8132324218750005e-05,
      "loss": 2.6661,
      "step": 460
    },
    {
      "epoch": 0.11474609375,
      "grad_norm": 3.7259254455566406,
      "learning_rate": 4.8091634114583336e-05,
      "loss": 2.5997,
      "step": 470
    },
    {
      "epoch": 0.1171875,
      "grad_norm": 3.2514607906341553,
      "learning_rate": 4.8050944010416674e-05,
      "loss": 2.6514,
      "step": 480
    },
    {
      "epoch": 0.11962890625,
      "grad_norm": 3.2634646892547607,
      "learning_rate": 4.8010253906250005e-05,
      "loss": 2.617,
      "step": 490
    },
    {
      "epoch": 0.1220703125,
      "grad_norm": 2.7733287811279297,
      "learning_rate": 4.7969563802083336e-05,
      "loss": 2.5848,
      "step": 500
    },
    {
      "epoch": 0.12451171875,
      "grad_norm": 4.51951265335083,
      "learning_rate": 4.7928873697916674e-05,
      "loss": 2.5793,
      "step": 510
    },
    {
      "epoch": 0.126953125,
      "grad_norm": 3.01727557182312,
      "learning_rate": 4.7888183593750005e-05,
      "loss": 2.5902,
      "step": 520
    },
    {
      "epoch": 0.12939453125,
      "grad_norm": 3.562633752822876,
      "learning_rate": 4.7847493489583336e-05,
      "loss": 2.4404,
      "step": 530
    },
    {
      "epoch": 0.1318359375,
      "grad_norm": 3.8373162746429443,
      "learning_rate": 4.7806803385416673e-05,
      "loss": 2.544,
      "step": 540
    },
    {
      "epoch": 0.13427734375,
      "grad_norm": 4.17971134185791,
      "learning_rate": 4.7766113281250004e-05,
      "loss": 2.6492,
      "step": 550
    },
    {
      "epoch": 0.13671875,
      "grad_norm": 3.39935564994812,
      "learning_rate": 4.7725423177083336e-05,
      "loss": 2.5309,
      "step": 560
    },
    {
      "epoch": 0.13916015625,
      "grad_norm": 2.572211980819702,
      "learning_rate": 4.768473307291667e-05,
      "loss": 2.6019,
      "step": 570
    },
    {
      "epoch": 0.1416015625,
      "grad_norm": 2.6188619136810303,
      "learning_rate": 4.7644042968750004e-05,
      "loss": 2.5279,
      "step": 580
    },
    {
      "epoch": 0.14404296875,
      "grad_norm": 2.7749295234680176,
      "learning_rate": 4.7603352864583335e-05,
      "loss": 2.5698,
      "step": 590
    },
    {
      "epoch": 0.146484375,
      "grad_norm": 2.7501258850097656,
      "learning_rate": 4.756266276041667e-05,
      "loss": 2.5226,
      "step": 600
    },
    {
      "epoch": 0.14892578125,
      "grad_norm": 2.8758223056793213,
      "learning_rate": 4.7521972656250004e-05,
      "loss": 2.5698,
      "step": 610
    },
    {
      "epoch": 0.1513671875,
      "grad_norm": 4.9029693603515625,
      "learning_rate": 4.7481282552083335e-05,
      "loss": 2.7009,
      "step": 620
    },
    {
      "epoch": 0.15380859375,
      "grad_norm": 3.0405139923095703,
      "learning_rate": 4.744059244791667e-05,
      "loss": 2.46,
      "step": 630
    },
    {
      "epoch": 0.15625,
      "grad_norm": 3.264448404312134,
      "learning_rate": 4.7399902343750004e-05,
      "loss": 2.5387,
      "step": 640
    },
    {
      "epoch": 0.15869140625,
      "grad_norm": 3.9247677326202393,
      "learning_rate": 4.7359212239583335e-05,
      "loss": 2.5791,
      "step": 650
    },
    {
      "epoch": 0.1611328125,
      "grad_norm": 4.585292816162109,
      "learning_rate": 4.731852213541667e-05,
      "loss": 2.5694,
      "step": 660
    },
    {
      "epoch": 0.16357421875,
      "grad_norm": 4.5278520584106445,
      "learning_rate": 4.7277832031250004e-05,
      "loss": 2.6511,
      "step": 670
    },
    {
      "epoch": 0.166015625,
      "grad_norm": 3.4392807483673096,
      "learning_rate": 4.7237141927083335e-05,
      "loss": 2.4759,
      "step": 680
    },
    {
      "epoch": 0.16845703125,
      "grad_norm": 2.4472033977508545,
      "learning_rate": 4.719645182291667e-05,
      "loss": 2.4105,
      "step": 690
    },
    {
      "epoch": 0.1708984375,
      "grad_norm": 3.2311949729919434,
      "learning_rate": 4.7155761718750004e-05,
      "loss": 2.5412,
      "step": 700
    },
    {
      "epoch": 0.17333984375,
      "grad_norm": 2.689896583557129,
      "learning_rate": 4.7115071614583335e-05,
      "loss": 2.4449,
      "step": 710
    },
    {
      "epoch": 0.17578125,
      "grad_norm": 3.285594940185547,
      "learning_rate": 4.707438151041667e-05,
      "loss": 2.4019,
      "step": 720
    },
    {
      "epoch": 0.17822265625,
      "grad_norm": 3.530689239501953,
      "learning_rate": 4.7033691406250004e-05,
      "loss": 2.5695,
      "step": 730
    },
    {
      "epoch": 0.1806640625,
      "grad_norm": 2.9108471870422363,
      "learning_rate": 4.6993001302083335e-05,
      "loss": 2.6007,
      "step": 740
    },
    {
      "epoch": 0.18310546875,
      "grad_norm": 3.615347385406494,
      "learning_rate": 4.695231119791667e-05,
      "loss": 2.5783,
      "step": 750
    },
    {
      "epoch": 0.185546875,
      "grad_norm": 2.908064842224121,
      "learning_rate": 4.6911621093750004e-05,
      "loss": 2.3925,
      "step": 760
    },
    {
      "epoch": 0.18798828125,
      "grad_norm": 3.7778632640838623,
      "learning_rate": 4.6870930989583335e-05,
      "loss": 2.412,
      "step": 770
    },
    {
      "epoch": 0.1904296875,
      "grad_norm": 3.9234654903411865,
      "learning_rate": 4.683024088541667e-05,
      "loss": 2.5504,
      "step": 780
    },
    {
      "epoch": 0.19287109375,
      "grad_norm": 5.0740437507629395,
      "learning_rate": 4.6789550781250004e-05,
      "loss": 2.4312,
      "step": 790
    },
    {
      "epoch": 0.1953125,
      "grad_norm": 4.026914119720459,
      "learning_rate": 4.6748860677083335e-05,
      "loss": 2.4404,
      "step": 800
    },
    {
      "epoch": 0.19775390625,
      "grad_norm": 3.501081943511963,
      "learning_rate": 4.670817057291667e-05,
      "loss": 2.603,
      "step": 810
    },
    {
      "epoch": 0.2001953125,
      "grad_norm": 2.6033387184143066,
      "learning_rate": 4.6667480468750004e-05,
      "loss": 2.3688,
      "step": 820
    },
    {
      "epoch": 0.20263671875,
      "grad_norm": 4.149310111999512,
      "learning_rate": 4.6626790364583335e-05,
      "loss": 2.5254,
      "step": 830
    },
    {
      "epoch": 0.205078125,
      "grad_norm": 4.296835899353027,
      "learning_rate": 4.658610026041667e-05,
      "loss": 2.7155,
      "step": 840
    },
    {
      "epoch": 0.20751953125,
      "grad_norm": 3.6802451610565186,
      "learning_rate": 4.6545410156250003e-05,
      "loss": 2.4471,
      "step": 850
    },
    {
      "epoch": 0.2099609375,
      "grad_norm": 3.661694049835205,
      "learning_rate": 4.6504720052083334e-05,
      "loss": 2.5943,
      "step": 860
    },
    {
      "epoch": 0.21240234375,
      "grad_norm": 4.2954182624816895,
      "learning_rate": 4.646402994791667e-05,
      "loss": 2.5855,
      "step": 870
    },
    {
      "epoch": 0.21484375,
      "grad_norm": 2.5833022594451904,
      "learning_rate": 4.642333984375e-05,
      "loss": 2.2714,
      "step": 880
    },
    {
      "epoch": 0.21728515625,
      "grad_norm": 4.358097553253174,
      "learning_rate": 4.6382649739583334e-05,
      "loss": 2.3881,
      "step": 890
    },
    {
      "epoch": 0.2197265625,
      "grad_norm": 2.9257829189300537,
      "learning_rate": 4.634195963541667e-05,
      "loss": 2.3834,
      "step": 900
    },
    {
      "epoch": 0.22216796875,
      "grad_norm": 3.530578374862671,
      "learning_rate": 4.630126953125e-05,
      "loss": 2.4119,
      "step": 910
    },
    {
      "epoch": 0.224609375,
      "grad_norm": 3.171644687652588,
      "learning_rate": 4.6260579427083334e-05,
      "loss": 2.442,
      "step": 920
    },
    {
      "epoch": 0.22705078125,
      "grad_norm": 4.902461528778076,
      "learning_rate": 4.621988932291667e-05,
      "loss": 2.4439,
      "step": 930
    },
    {
      "epoch": 0.2294921875,
      "grad_norm": 4.000381946563721,
      "learning_rate": 4.617919921875e-05,
      "loss": 2.5961,
      "step": 940
    },
    {
      "epoch": 0.23193359375,
      "grad_norm": 3.1894123554229736,
      "learning_rate": 4.6138509114583334e-05,
      "loss": 2.5192,
      "step": 950
    },
    {
      "epoch": 0.234375,
      "grad_norm": 4.37168025970459,
      "learning_rate": 4.609781901041667e-05,
      "loss": 2.4385,
      "step": 960
    },
    {
      "epoch": 0.23681640625,
      "grad_norm": 2.4957494735717773,
      "learning_rate": 4.605712890625e-05,
      "loss": 2.4125,
      "step": 970
    },
    {
      "epoch": 0.2392578125,
      "grad_norm": 3.4708855152130127,
      "learning_rate": 4.6016438802083334e-05,
      "loss": 2.4581,
      "step": 980
    },
    {
      "epoch": 0.24169921875,
      "grad_norm": 5.037795066833496,
      "learning_rate": 4.597574869791667e-05,
      "loss": 2.4506,
      "step": 990
    },
    {
      "epoch": 0.244140625,
      "grad_norm": 4.0844268798828125,
      "learning_rate": 4.593505859375e-05,
      "loss": 2.4501,
      "step": 1000
    },
    {
      "epoch": 0.24658203125,
      "grad_norm": 4.839343547821045,
      "learning_rate": 4.5894368489583334e-05,
      "loss": 2.515,
      "step": 1010
    },
    {
      "epoch": 0.2490234375,
      "grad_norm": 2.9237327575683594,
      "learning_rate": 4.585367838541667e-05,
      "loss": 2.4958,
      "step": 1020
    },
    {
      "epoch": 0.25146484375,
      "grad_norm": 8.428648948669434,
      "learning_rate": 4.581298828125e-05,
      "loss": 2.5603,
      "step": 1030
    },
    {
      "epoch": 0.25390625,
      "grad_norm": 3.3337242603302,
      "learning_rate": 4.5772298177083334e-05,
      "loss": 2.3717,
      "step": 1040
    },
    {
      "epoch": 0.25634765625,
      "grad_norm": 3.7785565853118896,
      "learning_rate": 4.573160807291667e-05,
      "loss": 2.4688,
      "step": 1050
    },
    {
      "epoch": 0.2587890625,
      "grad_norm": 4.209226608276367,
      "learning_rate": 4.569091796875e-05,
      "loss": 2.5303,
      "step": 1060
    },
    {
      "epoch": 0.26123046875,
      "grad_norm": 3.4098105430603027,
      "learning_rate": 4.5650227864583334e-05,
      "loss": 2.3753,
      "step": 1070
    },
    {
      "epoch": 0.263671875,
      "grad_norm": 2.367611885070801,
      "learning_rate": 4.560953776041667e-05,
      "loss": 2.3163,
      "step": 1080
    },
    {
      "epoch": 0.26611328125,
      "grad_norm": 2.5845208168029785,
      "learning_rate": 4.556884765625e-05,
      "loss": 2.4167,
      "step": 1090
    },
    {
      "epoch": 0.2685546875,
      "grad_norm": 2.671804666519165,
      "learning_rate": 4.5528157552083334e-05,
      "loss": 2.5398,
      "step": 1100
    },
    {
      "epoch": 0.27099609375,
      "grad_norm": 4.045773506164551,
      "learning_rate": 4.548746744791667e-05,
      "loss": 2.3413,
      "step": 1110
    },
    {
      "epoch": 0.2734375,
      "grad_norm": 3.876657247543335,
      "learning_rate": 4.544677734375e-05,
      "loss": 2.5652,
      "step": 1120
    },
    {
      "epoch": 0.27587890625,
      "grad_norm": 3.615650177001953,
      "learning_rate": 4.5406087239583333e-05,
      "loss": 2.3938,
      "step": 1130
    },
    {
      "epoch": 0.2783203125,
      "grad_norm": 4.27980899810791,
      "learning_rate": 4.536539713541667e-05,
      "loss": 2.2369,
      "step": 1140
    },
    {
      "epoch": 0.28076171875,
      "grad_norm": 3.04549241065979,
      "learning_rate": 4.532470703125e-05,
      "loss": 2.4822,
      "step": 1150
    },
    {
      "epoch": 0.283203125,
      "grad_norm": 2.735980987548828,
      "learning_rate": 4.528401692708333e-05,
      "loss": 2.2624,
      "step": 1160
    },
    {
      "epoch": 0.28564453125,
      "grad_norm": 4.745007038116455,
      "learning_rate": 4.524332682291667e-05,
      "loss": 2.3445,
      "step": 1170
    },
    {
      "epoch": 0.2880859375,
      "grad_norm": 4.275007247924805,
      "learning_rate": 4.520263671875e-05,
      "loss": 2.4177,
      "step": 1180
    },
    {
      "epoch": 0.29052734375,
      "grad_norm": 4.263153076171875,
      "learning_rate": 4.516194661458333e-05,
      "loss": 2.3037,
      "step": 1190
    },
    {
      "epoch": 0.29296875,
      "grad_norm": 2.990480422973633,
      "learning_rate": 4.512125651041667e-05,
      "loss": 2.2993,
      "step": 1200
    },
    {
      "epoch": 0.29541015625,
      "grad_norm": 3.2064573764801025,
      "learning_rate": 4.508056640625e-05,
      "loss": 2.4169,
      "step": 1210
    },
    {
      "epoch": 0.2978515625,
      "grad_norm": 5.100698471069336,
      "learning_rate": 4.503987630208333e-05,
      "loss": 2.4885,
      "step": 1220
    },
    {
      "epoch": 0.30029296875,
      "grad_norm": 3.789025068283081,
      "learning_rate": 4.499918619791667e-05,
      "loss": 2.4046,
      "step": 1230
    },
    {
      "epoch": 0.302734375,
      "grad_norm": 4.445727348327637,
      "learning_rate": 4.495849609375e-05,
      "loss": 2.5423,
      "step": 1240
    },
    {
      "epoch": 0.30517578125,
      "grad_norm": 3.099520683288574,
      "learning_rate": 4.491780598958333e-05,
      "loss": 2.3777,
      "step": 1250
    },
    {
      "epoch": 0.3076171875,
      "grad_norm": 4.508594036102295,
      "learning_rate": 4.487711588541667e-05,
      "loss": 2.4718,
      "step": 1260
    },
    {
      "epoch": 0.31005859375,
      "grad_norm": 4.315597057342529,
      "learning_rate": 4.483642578125e-05,
      "loss": 2.3831,
      "step": 1270
    },
    {
      "epoch": 0.3125,
      "grad_norm": 4.827917098999023,
      "learning_rate": 4.479573567708333e-05,
      "loss": 2.407,
      "step": 1280
    },
    {
      "epoch": 0.31494140625,
      "grad_norm": 3.8323841094970703,
      "learning_rate": 4.475504557291667e-05,
      "loss": 2.3199,
      "step": 1290
    },
    {
      "epoch": 0.3173828125,
      "grad_norm": 4.2397589683532715,
      "learning_rate": 4.471435546875e-05,
      "loss": 2.2887,
      "step": 1300
    },
    {
      "epoch": 0.31982421875,
      "grad_norm": 2.682105302810669,
      "learning_rate": 4.467366536458333e-05,
      "loss": 2.3901,
      "step": 1310
    },
    {
      "epoch": 0.322265625,
      "grad_norm": 3.2388916015625,
      "learning_rate": 4.463297526041667e-05,
      "loss": 2.4058,
      "step": 1320
    },
    {
      "epoch": 0.32470703125,
      "grad_norm": 4.244274139404297,
      "learning_rate": 4.459228515625e-05,
      "loss": 2.3144,
      "step": 1330
    },
    {
      "epoch": 0.3271484375,
      "grad_norm": 5.112141132354736,
      "learning_rate": 4.455159505208333e-05,
      "loss": 2.4435,
      "step": 1340
    },
    {
      "epoch": 0.32958984375,
      "grad_norm": 3.28250789642334,
      "learning_rate": 4.451090494791667e-05,
      "loss": 2.273,
      "step": 1350
    },
    {
      "epoch": 0.33203125,
      "grad_norm": 3.9927282333374023,
      "learning_rate": 4.447021484375e-05,
      "loss": 2.5005,
      "step": 1360
    },
    {
      "epoch": 0.33447265625,
      "grad_norm": 3.9892280101776123,
      "learning_rate": 4.442952473958333e-05,
      "loss": 2.3748,
      "step": 1370
    },
    {
      "epoch": 0.3369140625,
      "grad_norm": 2.6989405155181885,
      "learning_rate": 4.438883463541667e-05,
      "loss": 2.2748,
      "step": 1380
    },
    {
      "epoch": 0.33935546875,
      "grad_norm": 3.220551013946533,
      "learning_rate": 4.434814453125e-05,
      "loss": 2.3594,
      "step": 1390
    },
    {
      "epoch": 0.341796875,
      "grad_norm": 3.9431748390197754,
      "learning_rate": 4.430745442708333e-05,
      "loss": 2.3541,
      "step": 1400
    },
    {
      "epoch": 0.34423828125,
      "grad_norm": 4.030378818511963,
      "learning_rate": 4.426676432291667e-05,
      "loss": 2.225,
      "step": 1410
    },
    {
      "epoch": 0.3466796875,
      "grad_norm": 4.670886516571045,
      "learning_rate": 4.422607421875e-05,
      "loss": 2.2985,
      "step": 1420
    },
    {
      "epoch": 0.34912109375,
      "grad_norm": 4.357716083526611,
      "learning_rate": 4.418538411458333e-05,
      "loss": 2.1677,
      "step": 1430
    },
    {
      "epoch": 0.3515625,
      "grad_norm": 3.73991060256958,
      "learning_rate": 4.414469401041667e-05,
      "loss": 2.4138,
      "step": 1440
    },
    {
      "epoch": 0.35400390625,
      "grad_norm": 3.129877805709839,
      "learning_rate": 4.410400390625e-05,
      "loss": 2.3,
      "step": 1450
    },
    {
      "epoch": 0.3564453125,
      "grad_norm": 3.7127649784088135,
      "learning_rate": 4.406331380208333e-05,
      "loss": 2.1864,
      "step": 1460
    },
    {
      "epoch": 0.35888671875,
      "grad_norm": 3.435805320739746,
      "learning_rate": 4.402262369791667e-05,
      "loss": 2.4163,
      "step": 1470
    },
    {
      "epoch": 0.361328125,
      "grad_norm": 3.052504301071167,
      "learning_rate": 4.398193359375e-05,
      "loss": 2.3581,
      "step": 1480
    },
    {
      "epoch": 0.36376953125,
      "grad_norm": 3.6790127754211426,
      "learning_rate": 4.394124348958333e-05,
      "loss": 2.2644,
      "step": 1490
    },
    {
      "epoch": 0.3662109375,
      "grad_norm": 5.667440414428711,
      "learning_rate": 4.390055338541667e-05,
      "loss": 2.342,
      "step": 1500
    },
    {
      "epoch": 0.36865234375,
      "grad_norm": 4.9568891525268555,
      "learning_rate": 4.385986328125e-05,
      "loss": 2.3813,
      "step": 1510
    },
    {
      "epoch": 0.37109375,
      "grad_norm": 5.31986665725708,
      "learning_rate": 4.381917317708333e-05,
      "loss": 2.3684,
      "step": 1520
    },
    {
      "epoch": 0.37353515625,
      "grad_norm": 3.2843596935272217,
      "learning_rate": 4.377848307291667e-05,
      "loss": 2.2372,
      "step": 1530
    },
    {
      "epoch": 0.3759765625,
      "grad_norm": 4.163515090942383,
      "learning_rate": 4.373779296875e-05,
      "loss": 2.274,
      "step": 1540
    },
    {
      "epoch": 0.37841796875,
      "grad_norm": 3.7494959831237793,
      "learning_rate": 4.369710286458333e-05,
      "loss": 2.3041,
      "step": 1550
    },
    {
      "epoch": 0.380859375,
      "grad_norm": 3.2276370525360107,
      "learning_rate": 4.365641276041667e-05,
      "loss": 2.2464,
      "step": 1560
    },
    {
      "epoch": 0.38330078125,
      "grad_norm": 3.236684560775757,
      "learning_rate": 4.361572265625e-05,
      "loss": 2.2587,
      "step": 1570
    },
    {
      "epoch": 0.3857421875,
      "grad_norm": 4.113574981689453,
      "learning_rate": 4.357503255208333e-05,
      "loss": 2.2935,
      "step": 1580
    },
    {
      "epoch": 0.38818359375,
      "grad_norm": 3.84293270111084,
      "learning_rate": 4.353434244791667e-05,
      "loss": 2.3374,
      "step": 1590
    },
    {
      "epoch": 0.390625,
      "grad_norm": 5.381175518035889,
      "learning_rate": 4.349365234375e-05,
      "loss": 2.4342,
      "step": 1600
    },
    {
      "epoch": 0.39306640625,
      "grad_norm": 4.3663811683654785,
      "learning_rate": 4.345296223958333e-05,
      "loss": 2.5031,
      "step": 1610
    },
    {
      "epoch": 0.3955078125,
      "grad_norm": 2.7070999145507812,
      "learning_rate": 4.341227213541667e-05,
      "loss": 2.2991,
      "step": 1620
    },
    {
      "epoch": 0.39794921875,
      "grad_norm": 3.0868141651153564,
      "learning_rate": 4.337158203125e-05,
      "loss": 2.3942,
      "step": 1630
    },
    {
      "epoch": 0.400390625,
      "grad_norm": 4.248469829559326,
      "learning_rate": 4.333089192708333e-05,
      "loss": 2.361,
      "step": 1640
    },
    {
      "epoch": 0.40283203125,
      "grad_norm": 3.8593525886535645,
      "learning_rate": 4.329020182291667e-05,
      "loss": 2.2649,
      "step": 1650
    },
    {
      "epoch": 0.4052734375,
      "grad_norm": 5.070921897888184,
      "learning_rate": 4.324951171875e-05,
      "loss": 2.4503,
      "step": 1660
    },
    {
      "epoch": 0.40771484375,
      "grad_norm": 3.509281873703003,
      "learning_rate": 4.320882161458333e-05,
      "loss": 2.3413,
      "step": 1670
    },
    {
      "epoch": 0.41015625,
      "grad_norm": 3.441580057144165,
      "learning_rate": 4.316813151041667e-05,
      "loss": 2.1563,
      "step": 1680
    },
    {
      "epoch": 0.41259765625,
      "grad_norm": 4.274981498718262,
      "learning_rate": 4.312744140625e-05,
      "loss": 2.1705,
      "step": 1690
    },
    {
      "epoch": 0.4150390625,
      "grad_norm": 2.855509042739868,
      "learning_rate": 4.308675130208333e-05,
      "loss": 2.2232,
      "step": 1700
    },
    {
      "epoch": 0.41748046875,
      "grad_norm": 4.821456432342529,
      "learning_rate": 4.304606119791667e-05,
      "loss": 2.3467,
      "step": 1710
    },
    {
      "epoch": 0.419921875,
      "grad_norm": 3.890895366668701,
      "learning_rate": 4.300537109375e-05,
      "loss": 2.4248,
      "step": 1720
    },
    {
      "epoch": 0.42236328125,
      "grad_norm": 3.6453864574432373,
      "learning_rate": 4.296468098958333e-05,
      "loss": 2.1868,
      "step": 1730
    },
    {
      "epoch": 0.4248046875,
      "grad_norm": 6.564476490020752,
      "learning_rate": 4.292399088541667e-05,
      "loss": 2.2978,
      "step": 1740
    },
    {
      "epoch": 0.42724609375,
      "grad_norm": 5.131775379180908,
      "learning_rate": 4.288330078125e-05,
      "loss": 2.2962,
      "step": 1750
    },
    {
      "epoch": 0.4296875,
      "grad_norm": 4.114429473876953,
      "learning_rate": 4.284261067708333e-05,
      "loss": 2.2069,
      "step": 1760
    },
    {
      "epoch": 0.43212890625,
      "grad_norm": 6.428245544433594,
      "learning_rate": 4.280192057291667e-05,
      "loss": 2.3785,
      "step": 1770
    },
    {
      "epoch": 0.4345703125,
      "grad_norm": 3.0784308910369873,
      "learning_rate": 4.276123046875e-05,
      "loss": 2.2307,
      "step": 1780
    },
    {
      "epoch": 0.43701171875,
      "grad_norm": 4.017691135406494,
      "learning_rate": 4.272054036458333e-05,
      "loss": 2.2075,
      "step": 1790
    },
    {
      "epoch": 0.439453125,
      "grad_norm": 2.848897695541382,
      "learning_rate": 4.267985026041667e-05,
      "loss": 2.3718,
      "step": 1800
    },
    {
      "epoch": 0.44189453125,
      "grad_norm": 3.234182119369507,
      "learning_rate": 4.263916015625e-05,
      "loss": 2.1819,
      "step": 1810
    },
    {
      "epoch": 0.4443359375,
      "grad_norm": 3.758141040802002,
      "learning_rate": 4.259847005208333e-05,
      "loss": 2.4782,
      "step": 1820
    },
    {
      "epoch": 0.44677734375,
      "grad_norm": 5.495974540710449,
      "learning_rate": 4.255777994791667e-05,
      "loss": 2.2944,
      "step": 1830
    },
    {
      "epoch": 0.44921875,
      "grad_norm": 4.938762187957764,
      "learning_rate": 4.251708984375e-05,
      "loss": 2.2986,
      "step": 1840
    },
    {
      "epoch": 0.45166015625,
      "grad_norm": 5.306542873382568,
      "learning_rate": 4.247639973958333e-05,
      "loss": 2.471,
      "step": 1850
    },
    {
      "epoch": 0.4541015625,
      "grad_norm": 5.328232765197754,
      "learning_rate": 4.243570963541667e-05,
      "loss": 2.3268,
      "step": 1860
    },
    {
      "epoch": 0.45654296875,
      "grad_norm": 4.2969970703125,
      "learning_rate": 4.239501953125e-05,
      "loss": 2.2688,
      "step": 1870
    },
    {
      "epoch": 0.458984375,
      "grad_norm": 5.923913478851318,
      "learning_rate": 4.235432942708333e-05,
      "loss": 2.4028,
      "step": 1880
    },
    {
      "epoch": 0.46142578125,
      "grad_norm": 4.031136989593506,
      "learning_rate": 4.231363932291667e-05,
      "loss": 2.4188,
      "step": 1890
    },
    {
      "epoch": 0.4638671875,
      "grad_norm": 4.935835361480713,
      "learning_rate": 4.227294921875e-05,
      "loss": 2.2338,
      "step": 1900
    },
    {
      "epoch": 0.46630859375,
      "grad_norm": 5.8332648277282715,
      "learning_rate": 4.223225911458333e-05,
      "loss": 2.3449,
      "step": 1910
    },
    {
      "epoch": 0.46875,
      "grad_norm": 5.30702543258667,
      "learning_rate": 4.219156901041667e-05,
      "loss": 2.2119,
      "step": 1920
    },
    {
      "epoch": 0.47119140625,
      "grad_norm": 5.041561603546143,
      "learning_rate": 4.215087890625e-05,
      "loss": 2.3212,
      "step": 1930
    },
    {
      "epoch": 0.4736328125,
      "grad_norm": 5.70235013961792,
      "learning_rate": 4.211018880208333e-05,
      "loss": 2.5082,
      "step": 1940
    },
    {
      "epoch": 0.47607421875,
      "grad_norm": 4.489110469818115,
      "learning_rate": 4.206949869791667e-05,
      "loss": 2.2447,
      "step": 1950
    },
    {
      "epoch": 0.478515625,
      "grad_norm": 3.069579601287842,
      "learning_rate": 4.202880859375e-05,
      "loss": 2.2203,
      "step": 1960
    },
    {
      "epoch": 0.48095703125,
      "grad_norm": 5.658386707305908,
      "learning_rate": 4.198811848958333e-05,
      "loss": 2.215,
      "step": 1970
    },
    {
      "epoch": 0.4833984375,
      "grad_norm": 6.2945404052734375,
      "learning_rate": 4.194742838541667e-05,
      "loss": 2.2943,
      "step": 1980
    },
    {
      "epoch": 0.48583984375,
      "grad_norm": 4.861747741699219,
      "learning_rate": 4.190673828125e-05,
      "loss": 2.345,
      "step": 1990
    },
    {
      "epoch": 0.48828125,
      "grad_norm": 6.004611968994141,
      "learning_rate": 4.186604817708333e-05,
      "loss": 2.338,
      "step": 2000
    },
    {
      "epoch": 0.49072265625,
      "grad_norm": 4.378664493560791,
      "learning_rate": 4.182535807291667e-05,
      "loss": 2.1383,
      "step": 2010
    },
    {
      "epoch": 0.4931640625,
      "grad_norm": 3.088517904281616,
      "learning_rate": 4.178466796875e-05,
      "loss": 2.2536,
      "step": 2020
    },
    {
      "epoch": 0.49560546875,
      "grad_norm": 4.133410930633545,
      "learning_rate": 4.174397786458333e-05,
      "loss": 2.1691,
      "step": 2030
    },
    {
      "epoch": 0.498046875,
      "grad_norm": 4.506991863250732,
      "learning_rate": 4.170328776041667e-05,
      "loss": 2.3039,
      "step": 2040
    },
    {
      "epoch": 0.50048828125,
      "grad_norm": 4.871575355529785,
      "learning_rate": 4.166259765625e-05,
      "loss": 2.3793,
      "step": 2050
    },
    {
      "epoch": 0.5029296875,
      "grad_norm": 5.186428070068359,
      "learning_rate": 4.162190755208333e-05,
      "loss": 2.2671,
      "step": 2060
    },
    {
      "epoch": 0.50537109375,
      "grad_norm": 3.306185483932495,
      "learning_rate": 4.158121744791667e-05,
      "loss": 2.3511,
      "step": 2070
    },
    {
      "epoch": 0.5078125,
      "grad_norm": 6.339722633361816,
      "learning_rate": 4.154052734375e-05,
      "loss": 2.1051,
      "step": 2080
    },
    {
      "epoch": 0.51025390625,
      "grad_norm": 4.55454683303833,
      "learning_rate": 4.149983723958334e-05,
      "loss": 2.2679,
      "step": 2090
    },
    {
      "epoch": 0.5126953125,
      "grad_norm": 4.196094512939453,
      "learning_rate": 4.145914713541667e-05,
      "loss": 2.2519,
      "step": 2100
    },
    {
      "epoch": 0.51513671875,
      "grad_norm": 4.644556999206543,
      "learning_rate": 4.141845703125e-05,
      "loss": 2.304,
      "step": 2110
    },
    {
      "epoch": 0.517578125,
      "grad_norm": 4.161361217498779,
      "learning_rate": 4.137776692708334e-05,
      "loss": 2.226,
      "step": 2120
    },
    {
      "epoch": 0.52001953125,
      "grad_norm": 5.443729400634766,
      "learning_rate": 4.133707682291667e-05,
      "loss": 2.3428,
      "step": 2130
    },
    {
      "epoch": 0.5224609375,
      "grad_norm": 4.167183876037598,
      "learning_rate": 4.129638671875e-05,
      "loss": 2.1986,
      "step": 2140
    },
    {
      "epoch": 0.52490234375,
      "grad_norm": 3.353093147277832,
      "learning_rate": 4.1255696614583337e-05,
      "loss": 2.3217,
      "step": 2150
    },
    {
      "epoch": 0.52734375,
      "grad_norm": 3.6294362545013428,
      "learning_rate": 4.121500651041667e-05,
      "loss": 2.4125,
      "step": 2160
    },
    {
      "epoch": 0.52978515625,
      "grad_norm": 10.531597137451172,
      "learning_rate": 4.117431640625e-05,
      "loss": 2.1808,
      "step": 2170
    },
    {
      "epoch": 0.5322265625,
      "grad_norm": 4.6903300285339355,
      "learning_rate": 4.1133626302083336e-05,
      "loss": 2.4623,
      "step": 2180
    },
    {
      "epoch": 0.53466796875,
      "grad_norm": 4.45569372177124,
      "learning_rate": 4.109293619791667e-05,
      "loss": 2.2295,
      "step": 2190
    },
    {
      "epoch": 0.537109375,
      "grad_norm": 2.7419352531433105,
      "learning_rate": 4.1052246093750005e-05,
      "loss": 2.2647,
      "step": 2200
    },
    {
      "epoch": 0.53955078125,
      "grad_norm": 3.1438939571380615,
      "learning_rate": 4.1011555989583336e-05,
      "loss": 2.1911,
      "step": 2210
    },
    {
      "epoch": 0.5419921875,
      "grad_norm": 3.6840603351593018,
      "learning_rate": 4.097086588541667e-05,
      "loss": 2.2703,
      "step": 2220
    },
    {
      "epoch": 0.54443359375,
      "grad_norm": 4.283501625061035,
      "learning_rate": 4.0930175781250005e-05,
      "loss": 2.3284,
      "step": 2230
    },
    {
      "epoch": 0.546875,
      "grad_norm": 5.8845953941345215,
      "learning_rate": 4.0889485677083336e-05,
      "loss": 2.2697,
      "step": 2240
    },
    {
      "epoch": 0.54931640625,
      "grad_norm": 7.495868682861328,
      "learning_rate": 4.084879557291667e-05,
      "loss": 2.2722,
      "step": 2250
    },
    {
      "epoch": 0.5517578125,
      "grad_norm": 3.060051202774048,
      "learning_rate": 4.0808105468750005e-05,
      "loss": 2.2581,
      "step": 2260
    },
    {
      "epoch": 0.55419921875,
      "grad_norm": 4.001607418060303,
      "learning_rate": 4.0767415364583336e-05,
      "loss": 2.1921,
      "step": 2270
    },
    {
      "epoch": 0.556640625,
      "grad_norm": 4.092376708984375,
      "learning_rate": 4.072672526041667e-05,
      "loss": 2.2851,
      "step": 2280
    },
    {
      "epoch": 0.55908203125,
      "grad_norm": 9.464754104614258,
      "learning_rate": 4.0686035156250005e-05,
      "loss": 2.1667,
      "step": 2290
    },
    {
      "epoch": 0.5615234375,
      "grad_norm": 3.2729110717773438,
      "learning_rate": 4.0645345052083336e-05,
      "loss": 2.3275,
      "step": 2300
    },
    {
      "epoch": 0.56396484375,
      "grad_norm": 3.2527506351470947,
      "learning_rate": 4.0604654947916674e-05,
      "loss": 2.1125,
      "step": 2310
    },
    {
      "epoch": 0.56640625,
      "grad_norm": 4.5961127281188965,
      "learning_rate": 4.0563964843750005e-05,
      "loss": 2.1838,
      "step": 2320
    },
    {
      "epoch": 0.56884765625,
      "grad_norm": 4.362698078155518,
      "learning_rate": 4.0523274739583336e-05,
      "loss": 2.0681,
      "step": 2330
    },
    {
      "epoch": 0.5712890625,
      "grad_norm": 6.593459129333496,
      "learning_rate": 4.0482584635416674e-05,
      "loss": 2.1025,
      "step": 2340
    },
    {
      "epoch": 0.57373046875,
      "grad_norm": 3.336094379425049,
      "learning_rate": 4.0441894531250005e-05,
      "loss": 2.268,
      "step": 2350
    },
    {
      "epoch": 0.576171875,
      "grad_norm": 5.406242370605469,
      "learning_rate": 4.0401204427083336e-05,
      "loss": 2.1973,
      "step": 2360
    },
    {
      "epoch": 0.57861328125,
      "grad_norm": 3.3259499073028564,
      "learning_rate": 4.0360514322916674e-05,
      "loss": 2.2065,
      "step": 2370
    },
    {
      "epoch": 0.5810546875,
      "grad_norm": 3.1577577590942383,
      "learning_rate": 4.0319824218750005e-05,
      "loss": 2.1584,
      "step": 2380
    },
    {
      "epoch": 0.58349609375,
      "grad_norm": 5.780908107757568,
      "learning_rate": 4.0279134114583336e-05,
      "loss": 2.356,
      "step": 2390
    },
    {
      "epoch": 0.5859375,
      "grad_norm": 3.657688856124878,
      "learning_rate": 4.0238444010416673e-05,
      "loss": 2.2595,
      "step": 2400
    },
    {
      "epoch": 0.58837890625,
      "grad_norm": 4.297396183013916,
      "learning_rate": 4.0197753906250005e-05,
      "loss": 2.2821,
      "step": 2410
    },
    {
      "epoch": 0.5908203125,
      "grad_norm": 3.7100796699523926,
      "learning_rate": 4.0157063802083336e-05,
      "loss": 2.1697,
      "step": 2420
    },
    {
      "epoch": 0.59326171875,
      "grad_norm": 3.433680295944214,
      "learning_rate": 4.011637369791667e-05,
      "loss": 2.2664,
      "step": 2430
    },
    {
      "epoch": 0.595703125,
      "grad_norm": 3.0316171646118164,
      "learning_rate": 4.0075683593750004e-05,
      "loss": 2.3055,
      "step": 2440
    },
    {
      "epoch": 0.59814453125,
      "grad_norm": 5.022443771362305,
      "learning_rate": 4.0034993489583335e-05,
      "loss": 2.1787,
      "step": 2450
    },
    {
      "epoch": 0.6005859375,
      "grad_norm": 4.650871276855469,
      "learning_rate": 3.999430338541667e-05,
      "loss": 2.2109,
      "step": 2460
    },
    {
      "epoch": 0.60302734375,
      "grad_norm": 4.692988872528076,
      "learning_rate": 3.9953613281250004e-05,
      "loss": 2.2271,
      "step": 2470
    },
    {
      "epoch": 0.60546875,
      "grad_norm": 4.1225128173828125,
      "learning_rate": 3.9912923177083335e-05,
      "loss": 2.2704,
      "step": 2480
    },
    {
      "epoch": 0.60791015625,
      "grad_norm": 6.3109612464904785,
      "learning_rate": 3.987223307291667e-05,
      "loss": 2.2888,
      "step": 2490
    },
    {
      "epoch": 0.6103515625,
      "grad_norm": 3.7152297496795654,
      "learning_rate": 3.9831542968750004e-05,
      "loss": 2.2613,
      "step": 2500
    },
    {
      "epoch": 0.61279296875,
      "grad_norm": 4.445415496826172,
      "learning_rate": 3.9790852864583335e-05,
      "loss": 2.2341,
      "step": 2510
    },
    {
      "epoch": 0.615234375,
      "grad_norm": 2.7803561687469482,
      "learning_rate": 3.975016276041667e-05,
      "loss": 2.1654,
      "step": 2520
    },
    {
      "epoch": 0.61767578125,
      "grad_norm": 5.2791900634765625,
      "learning_rate": 3.9709472656250004e-05,
      "loss": 2.2133,
      "step": 2530
    },
    {
      "epoch": 0.6201171875,
      "grad_norm": 5.960577011108398,
      "learning_rate": 3.9668782552083335e-05,
      "loss": 2.2113,
      "step": 2540
    },
    {
      "epoch": 0.62255859375,
      "grad_norm": 5.653391361236572,
      "learning_rate": 3.962809244791667e-05,
      "loss": 2.1435,
      "step": 2550
    },
    {
      "epoch": 0.625,
      "grad_norm": 4.766706466674805,
      "learning_rate": 3.9587402343750004e-05,
      "loss": 2.1793,
      "step": 2560
    },
    {
      "epoch": 0.62744140625,
      "grad_norm": 4.893288612365723,
      "learning_rate": 3.9546712239583335e-05,
      "loss": 2.2509,
      "step": 2570
    },
    {
      "epoch": 0.6298828125,
      "grad_norm": 4.267228126525879,
      "learning_rate": 3.950602213541667e-05,
      "loss": 2.4838,
      "step": 2580
    },
    {
      "epoch": 0.63232421875,
      "grad_norm": 3.532087564468384,
      "learning_rate": 3.9465332031250004e-05,
      "loss": 2.2246,
      "step": 2590
    },
    {
      "epoch": 0.634765625,
      "grad_norm": 4.428231716156006,
      "learning_rate": 3.9424641927083335e-05,
      "loss": 2.2236,
      "step": 2600
    },
    {
      "epoch": 0.63720703125,
      "grad_norm": 4.364617347717285,
      "learning_rate": 3.938395182291667e-05,
      "loss": 2.1669,
      "step": 2610
    },
    {
      "epoch": 0.6396484375,
      "grad_norm": 5.609654903411865,
      "learning_rate": 3.9343261718750004e-05,
      "loss": 2.114,
      "step": 2620
    },
    {
      "epoch": 0.64208984375,
      "grad_norm": 5.303116798400879,
      "learning_rate": 3.9302571614583335e-05,
      "loss": 2.2456,
      "step": 2630
    },
    {
      "epoch": 0.64453125,
      "grad_norm": 4.2533793449401855,
      "learning_rate": 3.926188151041667e-05,
      "loss": 2.3392,
      "step": 2640
    },
    {
      "epoch": 0.64697265625,
      "grad_norm": 4.4002790451049805,
      "learning_rate": 3.9221191406250004e-05,
      "loss": 2.3491,
      "step": 2650
    },
    {
      "epoch": 0.6494140625,
      "grad_norm": 3.9163262844085693,
      "learning_rate": 3.9180501302083335e-05,
      "loss": 2.3115,
      "step": 2660
    },
    {
      "epoch": 0.65185546875,
      "grad_norm": 3.7389612197875977,
      "learning_rate": 3.913981119791667e-05,
      "loss": 2.127,
      "step": 2670
    },
    {
      "epoch": 0.654296875,
      "grad_norm": 4.633140563964844,
      "learning_rate": 3.9099121093750004e-05,
      "loss": 2.0464,
      "step": 2680
    },
    {
      "epoch": 0.65673828125,
      "grad_norm": 4.965242385864258,
      "learning_rate": 3.9058430989583335e-05,
      "loss": 2.2638,
      "step": 2690
    },
    {
      "epoch": 0.6591796875,
      "grad_norm": 3.6642982959747314,
      "learning_rate": 3.901774088541667e-05,
      "loss": 2.0863,
      "step": 2700
    },
    {
      "epoch": 0.66162109375,
      "grad_norm": 3.455315351486206,
      "learning_rate": 3.8977050781250003e-05,
      "loss": 2.2364,
      "step": 2710
    },
    {
      "epoch": 0.6640625,
      "grad_norm": 4.809534549713135,
      "learning_rate": 3.8936360677083334e-05,
      "loss": 2.3637,
      "step": 2720
    },
    {
      "epoch": 0.66650390625,
      "grad_norm": 3.1900434494018555,
      "learning_rate": 3.889567057291667e-05,
      "loss": 2.3302,
      "step": 2730
    },
    {
      "epoch": 0.6689453125,
      "grad_norm": 5.593765735626221,
      "learning_rate": 3.885498046875e-05,
      "loss": 2.169,
      "step": 2740
    },
    {
      "epoch": 0.67138671875,
      "grad_norm": 5.871394157409668,
      "learning_rate": 3.8814290364583334e-05,
      "loss": 2.1812,
      "step": 2750
    },
    {
      "epoch": 0.673828125,
      "grad_norm": 2.8641200065612793,
      "learning_rate": 3.877360026041667e-05,
      "loss": 2.0126,
      "step": 2760
    },
    {
      "epoch": 0.67626953125,
      "grad_norm": 5.128701686859131,
      "learning_rate": 3.873291015625e-05,
      "loss": 2.1151,
      "step": 2770
    },
    {
      "epoch": 0.6787109375,
      "grad_norm": 3.6028199195861816,
      "learning_rate": 3.8692220052083334e-05,
      "loss": 2.1806,
      "step": 2780
    },
    {
      "epoch": 0.68115234375,
      "grad_norm": 6.331089973449707,
      "learning_rate": 3.865152994791667e-05,
      "loss": 2.193,
      "step": 2790
    },
    {
      "epoch": 0.68359375,
      "grad_norm": 5.4412007331848145,
      "learning_rate": 3.861083984375e-05,
      "loss": 2.1591,
      "step": 2800
    },
    {
      "epoch": 0.68603515625,
      "grad_norm": 4.729854583740234,
      "learning_rate": 3.8570149739583334e-05,
      "loss": 2.1474,
      "step": 2810
    },
    {
      "epoch": 0.6884765625,
      "grad_norm": 5.030384540557861,
      "learning_rate": 3.852945963541667e-05,
      "loss": 2.2605,
      "step": 2820
    },
    {
      "epoch": 0.69091796875,
      "grad_norm": 5.0036940574646,
      "learning_rate": 3.848876953125e-05,
      "loss": 2.1425,
      "step": 2830
    },
    {
      "epoch": 0.693359375,
      "grad_norm": 5.039393901824951,
      "learning_rate": 3.8448079427083334e-05,
      "loss": 2.3373,
      "step": 2840
    },
    {
      "epoch": 0.69580078125,
      "grad_norm": 5.1649274826049805,
      "learning_rate": 3.840738932291667e-05,
      "loss": 2.3211,
      "step": 2850
    },
    {
      "epoch": 0.6982421875,
      "grad_norm": 3.0629522800445557,
      "learning_rate": 3.836669921875e-05,
      "loss": 2.2078,
      "step": 2860
    },
    {
      "epoch": 0.70068359375,
      "grad_norm": 9.298700332641602,
      "learning_rate": 3.8326009114583334e-05,
      "loss": 2.2007,
      "step": 2870
    },
    {
      "epoch": 0.703125,
      "grad_norm": 7.33323860168457,
      "learning_rate": 3.828531901041667e-05,
      "loss": 2.1874,
      "step": 2880
    },
    {
      "epoch": 0.70556640625,
      "grad_norm": 6.594947338104248,
      "learning_rate": 3.824462890625e-05,
      "loss": 2.2254,
      "step": 2890
    },
    {
      "epoch": 0.7080078125,
      "grad_norm": 3.841979503631592,
      "learning_rate": 3.8203938802083334e-05,
      "loss": 2.2985,
      "step": 2900
    },
    {
      "epoch": 0.71044921875,
      "grad_norm": 5.798023223876953,
      "learning_rate": 3.816324869791667e-05,
      "loss": 2.2195,
      "step": 2910
    },
    {
      "epoch": 0.712890625,
      "grad_norm": 4.641288757324219,
      "learning_rate": 3.812255859375e-05,
      "loss": 2.2269,
      "step": 2920
    },
    {
      "epoch": 0.71533203125,
      "grad_norm": 4.616421222686768,
      "learning_rate": 3.8081868489583334e-05,
      "loss": 2.1795,
      "step": 2930
    },
    {
      "epoch": 0.7177734375,
      "grad_norm": 4.730936050415039,
      "learning_rate": 3.804117838541667e-05,
      "loss": 2.2212,
      "step": 2940
    },
    {
      "epoch": 0.72021484375,
      "grad_norm": 3.3806278705596924,
      "learning_rate": 3.800048828125e-05,
      "loss": 2.2163,
      "step": 2950
    },
    {
      "epoch": 0.72265625,
      "grad_norm": 6.441685676574707,
      "learning_rate": 3.7959798177083334e-05,
      "loss": 2.2618,
      "step": 2960
    },
    {
      "epoch": 0.72509765625,
      "grad_norm": 4.985381126403809,
      "learning_rate": 3.791910807291667e-05,
      "loss": 2.0387,
      "step": 2970
    },
    {
      "epoch": 0.7275390625,
      "grad_norm": 5.14879846572876,
      "learning_rate": 3.787841796875e-05,
      "loss": 2.0649,
      "step": 2980
    },
    {
      "epoch": 0.72998046875,
      "grad_norm": 5.300271034240723,
      "learning_rate": 3.7837727864583334e-05,
      "loss": 2.1272,
      "step": 2990
    },
    {
      "epoch": 0.732421875,
      "grad_norm": 6.040872097015381,
      "learning_rate": 3.779703776041667e-05,
      "loss": 2.1629,
      "step": 3000
    },
    {
      "epoch": 0.73486328125,
      "grad_norm": 3.8494679927825928,
      "learning_rate": 3.775634765625e-05,
      "loss": 2.1639,
      "step": 3010
    },
    {
      "epoch": 0.7373046875,
      "grad_norm": 3.037343978881836,
      "learning_rate": 3.7715657552083333e-05,
      "loss": 2.1791,
      "step": 3020
    },
    {
      "epoch": 0.73974609375,
      "grad_norm": 3.0519750118255615,
      "learning_rate": 3.767496744791667e-05,
      "loss": 2.3366,
      "step": 3030
    },
    {
      "epoch": 0.7421875,
      "grad_norm": 5.8724541664123535,
      "learning_rate": 3.763427734375e-05,
      "loss": 2.201,
      "step": 3040
    },
    {
      "epoch": 0.74462890625,
      "grad_norm": 3.6399548053741455,
      "learning_rate": 3.759358723958333e-05,
      "loss": 2.2569,
      "step": 3050
    },
    {
      "epoch": 0.7470703125,
      "grad_norm": 3.840876817703247,
      "learning_rate": 3.755289713541667e-05,
      "loss": 2.292,
      "step": 3060
    },
    {
      "epoch": 0.74951171875,
      "grad_norm": 6.140596866607666,
      "learning_rate": 3.751220703125e-05,
      "loss": 2.2341,
      "step": 3070
    },
    {
      "epoch": 0.751953125,
      "grad_norm": 2.794973134994507,
      "learning_rate": 3.747151692708333e-05,
      "loss": 2.137,
      "step": 3080
    },
    {
      "epoch": 0.75439453125,
      "grad_norm": 4.186800956726074,
      "learning_rate": 3.743082682291667e-05,
      "loss": 2.1149,
      "step": 3090
    },
    {
      "epoch": 0.7568359375,
      "grad_norm": 3.298882246017456,
      "learning_rate": 3.739013671875e-05,
      "loss": 2.3688,
      "step": 3100
    },
    {
      "epoch": 0.75927734375,
      "grad_norm": 4.541698455810547,
      "learning_rate": 3.734944661458333e-05,
      "loss": 2.1117,
      "step": 3110
    },
    {
      "epoch": 0.76171875,
      "grad_norm": 5.847400665283203,
      "learning_rate": 3.730875651041667e-05,
      "loss": 2.3614,
      "step": 3120
    },
    {
      "epoch": 0.76416015625,
      "grad_norm": 5.301485061645508,
      "learning_rate": 3.726806640625e-05,
      "loss": 2.2193,
      "step": 3130
    },
    {
      "epoch": 0.7666015625,
      "grad_norm": 5.0946173667907715,
      "learning_rate": 3.722737630208333e-05,
      "loss": 2.3058,
      "step": 3140
    },
    {
      "epoch": 0.76904296875,
      "grad_norm": 6.606380939483643,
      "learning_rate": 3.718668619791667e-05,
      "loss": 2.0328,
      "step": 3150
    },
    {
      "epoch": 0.771484375,
      "grad_norm": 3.8174796104431152,
      "learning_rate": 3.714599609375e-05,
      "loss": 2.2695,
      "step": 3160
    },
    {
      "epoch": 0.77392578125,
      "grad_norm": 4.393121719360352,
      "learning_rate": 3.710530598958333e-05,
      "loss": 2.1655,
      "step": 3170
    },
    {
      "epoch": 0.7763671875,
      "grad_norm": 4.646409511566162,
      "learning_rate": 3.706461588541667e-05,
      "loss": 2.1511,
      "step": 3180
    },
    {
      "epoch": 0.77880859375,
      "grad_norm": 4.381148338317871,
      "learning_rate": 3.702392578125e-05,
      "loss": 2.2074,
      "step": 3190
    },
    {
      "epoch": 0.78125,
      "grad_norm": 4.58797025680542,
      "learning_rate": 3.698323567708333e-05,
      "loss": 2.2023,
      "step": 3200
    },
    {
      "epoch": 0.78369140625,
      "grad_norm": 5.946028232574463,
      "learning_rate": 3.694254557291667e-05,
      "loss": 2.2681,
      "step": 3210
    },
    {
      "epoch": 0.7861328125,
      "grad_norm": 4.007031440734863,
      "learning_rate": 3.690185546875e-05,
      "loss": 2.3148,
      "step": 3220
    },
    {
      "epoch": 0.78857421875,
      "grad_norm": 3.4223310947418213,
      "learning_rate": 3.686116536458333e-05,
      "loss": 2.2488,
      "step": 3230
    },
    {
      "epoch": 0.791015625,
      "grad_norm": 4.714056491851807,
      "learning_rate": 3.682047526041667e-05,
      "loss": 2.0631,
      "step": 3240
    },
    {
      "epoch": 0.79345703125,
      "grad_norm": 5.0123701095581055,
      "learning_rate": 3.677978515625e-05,
      "loss": 2.2626,
      "step": 3250
    },
    {
      "epoch": 0.7958984375,
      "grad_norm": 6.1692938804626465,
      "learning_rate": 3.673909505208333e-05,
      "loss": 2.2888,
      "step": 3260
    },
    {
      "epoch": 0.79833984375,
      "grad_norm": 4.957718372344971,
      "learning_rate": 3.669840494791667e-05,
      "loss": 2.3607,
      "step": 3270
    },
    {
      "epoch": 0.80078125,
      "grad_norm": 3.4431543350219727,
      "learning_rate": 3.665771484375e-05,
      "loss": 2.0874,
      "step": 3280
    },
    {
      "epoch": 0.80322265625,
      "grad_norm": 8.862403869628906,
      "learning_rate": 3.661702473958333e-05,
      "loss": 2.0375,
      "step": 3290
    },
    {
      "epoch": 0.8056640625,
      "grad_norm": 3.954059600830078,
      "learning_rate": 3.657633463541667e-05,
      "loss": 2.2063,
      "step": 3300
    },
    {
      "epoch": 0.80810546875,
      "grad_norm": 3.560831069946289,
      "learning_rate": 3.653564453125e-05,
      "loss": 2.1173,
      "step": 3310
    },
    {
      "epoch": 0.810546875,
      "grad_norm": 3.844233989715576,
      "learning_rate": 3.649495442708333e-05,
      "loss": 2.1181,
      "step": 3320
    },
    {
      "epoch": 0.81298828125,
      "grad_norm": 3.6797192096710205,
      "learning_rate": 3.645426432291667e-05,
      "loss": 2.0229,
      "step": 3330
    },
    {
      "epoch": 0.8154296875,
      "grad_norm": 6.621281623840332,
      "learning_rate": 3.641357421875e-05,
      "loss": 2.0729,
      "step": 3340
    },
    {
      "epoch": 0.81787109375,
      "grad_norm": 5.446695327758789,
      "learning_rate": 3.637288411458333e-05,
      "loss": 2.2228,
      "step": 3350
    },
    {
      "epoch": 0.8203125,
      "grad_norm": 5.2628068923950195,
      "learning_rate": 3.633219401041667e-05,
      "loss": 2.2237,
      "step": 3360
    },
    {
      "epoch": 0.82275390625,
      "grad_norm": 5.783728122711182,
      "learning_rate": 3.629150390625e-05,
      "loss": 2.1076,
      "step": 3370
    },
    {
      "epoch": 0.8251953125,
      "grad_norm": 6.355066776275635,
      "learning_rate": 3.625081380208333e-05,
      "loss": 2.2472,
      "step": 3380
    },
    {
      "epoch": 0.82763671875,
      "grad_norm": 6.442888259887695,
      "learning_rate": 3.621012369791667e-05,
      "loss": 2.0897,
      "step": 3390
    },
    {
      "epoch": 0.830078125,
      "grad_norm": 4.944504737854004,
      "learning_rate": 3.616943359375e-05,
      "loss": 2.1522,
      "step": 3400
    },
    {
      "epoch": 0.83251953125,
      "grad_norm": 4.541467189788818,
      "learning_rate": 3.612874348958333e-05,
      "loss": 1.9727,
      "step": 3410
    },
    {
      "epoch": 0.8349609375,
      "grad_norm": 4.761140823364258,
      "learning_rate": 3.608805338541667e-05,
      "loss": 2.0906,
      "step": 3420
    },
    {
      "epoch": 0.83740234375,
      "grad_norm": 3.826306104660034,
      "learning_rate": 3.604736328125e-05,
      "loss": 2.3082,
      "step": 3430
    },
    {
      "epoch": 0.83984375,
      "grad_norm": 6.4077372550964355,
      "learning_rate": 3.600667317708333e-05,
      "loss": 2.106,
      "step": 3440
    },
    {
      "epoch": 0.84228515625,
      "grad_norm": 4.099997043609619,
      "learning_rate": 3.596598307291667e-05,
      "loss": 2.1537,
      "step": 3450
    },
    {
      "epoch": 0.8447265625,
      "grad_norm": 5.0189127922058105,
      "learning_rate": 3.592529296875e-05,
      "loss": 2.2145,
      "step": 3460
    },
    {
      "epoch": 0.84716796875,
      "grad_norm": 6.449629783630371,
      "learning_rate": 3.588460286458333e-05,
      "loss": 1.9924,
      "step": 3470
    },
    {
      "epoch": 0.849609375,
      "grad_norm": 3.6714253425598145,
      "learning_rate": 3.584391276041667e-05,
      "loss": 2.1612,
      "step": 3480
    },
    {
      "epoch": 0.85205078125,
      "grad_norm": 4.826665878295898,
      "learning_rate": 3.580322265625e-05,
      "loss": 2.2628,
      "step": 3490
    },
    {
      "epoch": 0.8544921875,
      "grad_norm": 4.320293426513672,
      "learning_rate": 3.576253255208333e-05,
      "loss": 2.1214,
      "step": 3500
    },
    {
      "epoch": 0.85693359375,
      "grad_norm": 4.72427225112915,
      "learning_rate": 3.572184244791667e-05,
      "loss": 2.1064,
      "step": 3510
    },
    {
      "epoch": 0.859375,
      "grad_norm": 3.9332118034362793,
      "learning_rate": 3.568115234375e-05,
      "loss": 2.1499,
      "step": 3520
    },
    {
      "epoch": 0.86181640625,
      "grad_norm": 3.678755044937134,
      "learning_rate": 3.564046223958333e-05,
      "loss": 2.3229,
      "step": 3530
    },
    {
      "epoch": 0.8642578125,
      "grad_norm": 3.6456615924835205,
      "learning_rate": 3.559977213541667e-05,
      "loss": 2.124,
      "step": 3540
    },
    {
      "epoch": 0.86669921875,
      "grad_norm": 3.366668939590454,
      "learning_rate": 3.555908203125e-05,
      "loss": 2.1664,
      "step": 3550
    },
    {
      "epoch": 0.869140625,
      "grad_norm": 5.370581150054932,
      "learning_rate": 3.551839192708333e-05,
      "loss": 2.1301,
      "step": 3560
    },
    {
      "epoch": 0.87158203125,
      "grad_norm": 3.8927316665649414,
      "learning_rate": 3.547770182291667e-05,
      "loss": 2.2993,
      "step": 3570
    },
    {
      "epoch": 0.8740234375,
      "grad_norm": 6.179315567016602,
      "learning_rate": 3.543701171875e-05,
      "loss": 2.149,
      "step": 3580
    },
    {
      "epoch": 0.87646484375,
      "grad_norm": 5.546550750732422,
      "learning_rate": 3.539632161458333e-05,
      "loss": 2.152,
      "step": 3590
    },
    {
      "epoch": 0.87890625,
      "grad_norm": 5.675354957580566,
      "learning_rate": 3.535563151041667e-05,
      "loss": 1.9594,
      "step": 3600
    },
    {
      "epoch": 0.88134765625,
      "grad_norm": 5.6311187744140625,
      "learning_rate": 3.531494140625e-05,
      "loss": 2.2847,
      "step": 3610
    },
    {
      "epoch": 0.8837890625,
      "grad_norm": 7.757275104522705,
      "learning_rate": 3.527425130208333e-05,
      "loss": 2.1372,
      "step": 3620
    },
    {
      "epoch": 0.88623046875,
      "grad_norm": 4.5335798263549805,
      "learning_rate": 3.523356119791667e-05,
      "loss": 2.0736,
      "step": 3630
    },
    {
      "epoch": 0.888671875,
      "grad_norm": 6.831653118133545,
      "learning_rate": 3.519287109375e-05,
      "loss": 2.0761,
      "step": 3640
    },
    {
      "epoch": 0.89111328125,
      "grad_norm": 6.134949684143066,
      "learning_rate": 3.515218098958333e-05,
      "loss": 2.2088,
      "step": 3650
    },
    {
      "epoch": 0.8935546875,
      "grad_norm": 4.9401750564575195,
      "learning_rate": 3.511149088541667e-05,
      "loss": 2.0891,
      "step": 3660
    },
    {
      "epoch": 0.89599609375,
      "grad_norm": 4.513749122619629,
      "learning_rate": 3.507080078125e-05,
      "loss": 2.0743,
      "step": 3670
    },
    {
      "epoch": 0.8984375,
      "grad_norm": 3.5501081943511963,
      "learning_rate": 3.503011067708333e-05,
      "loss": 2.0986,
      "step": 3680
    },
    {
      "epoch": 0.90087890625,
      "grad_norm": 9.73003101348877,
      "learning_rate": 3.498942057291667e-05,
      "loss": 2.1659,
      "step": 3690
    },
    {
      "epoch": 0.9033203125,
      "grad_norm": 4.303472518920898,
      "learning_rate": 3.494873046875e-05,
      "loss": 2.2503,
      "step": 3700
    },
    {
      "epoch": 0.90576171875,
      "grad_norm": 4.756283283233643,
      "learning_rate": 3.490804036458333e-05,
      "loss": 2.2244,
      "step": 3710
    },
    {
      "epoch": 0.908203125,
      "grad_norm": 4.762638092041016,
      "learning_rate": 3.486735026041667e-05,
      "loss": 2.1912,
      "step": 3720
    },
    {
      "epoch": 0.91064453125,
      "grad_norm": 3.2064297199249268,
      "learning_rate": 3.482666015625e-05,
      "loss": 2.2337,
      "step": 3730
    },
    {
      "epoch": 0.9130859375,
      "grad_norm": 5.145088195800781,
      "learning_rate": 3.478597005208333e-05,
      "loss": 2.0193,
      "step": 3740
    },
    {
      "epoch": 0.91552734375,
      "grad_norm": 4.506618022918701,
      "learning_rate": 3.474527994791667e-05,
      "loss": 2.0955,
      "step": 3750
    },
    {
      "epoch": 0.91796875,
      "grad_norm": 3.8518805503845215,
      "learning_rate": 3.470458984375e-05,
      "loss": 2.169,
      "step": 3760
    },
    {
      "epoch": 0.92041015625,
      "grad_norm": 6.114134311676025,
      "learning_rate": 3.466389973958333e-05,
      "loss": 2.1319,
      "step": 3770
    },
    {
      "epoch": 0.9228515625,
      "grad_norm": 5.714572429656982,
      "learning_rate": 3.462320963541667e-05,
      "loss": 1.9979,
      "step": 3780
    },
    {
      "epoch": 0.92529296875,
      "grad_norm": 5.851072788238525,
      "learning_rate": 3.458251953125e-05,
      "loss": 2.3164,
      "step": 3790
    },
    {
      "epoch": 0.927734375,
      "grad_norm": 5.683563709259033,
      "learning_rate": 3.454182942708333e-05,
      "loss": 2.0679,
      "step": 3800
    },
    {
      "epoch": 0.93017578125,
      "grad_norm": 3.74586820602417,
      "learning_rate": 3.450113932291667e-05,
      "loss": 2.1568,
      "step": 3810
    },
    {
      "epoch": 0.9326171875,
      "grad_norm": 4.902795791625977,
      "learning_rate": 3.446044921875e-05,
      "loss": 2.1022,
      "step": 3820
    },
    {
      "epoch": 0.93505859375,
      "grad_norm": 4.937900066375732,
      "learning_rate": 3.441975911458333e-05,
      "loss": 2.2024,
      "step": 3830
    },
    {
      "epoch": 0.9375,
      "grad_norm": 4.882019519805908,
      "learning_rate": 3.437906901041667e-05,
      "loss": 2.0259,
      "step": 3840
    },
    {
      "epoch": 0.93994140625,
      "grad_norm": 7.486950874328613,
      "learning_rate": 3.433837890625e-05,
      "loss": 2.2649,
      "step": 3850
    },
    {
      "epoch": 0.9423828125,
      "grad_norm": 4.622385501861572,
      "learning_rate": 3.429768880208333e-05,
      "loss": 2.0968,
      "step": 3860
    },
    {
      "epoch": 0.94482421875,
      "grad_norm": 5.840235233306885,
      "learning_rate": 3.425699869791667e-05,
      "loss": 2.1849,
      "step": 3870
    },
    {
      "epoch": 0.947265625,
      "grad_norm": 5.723288059234619,
      "learning_rate": 3.421630859375e-05,
      "loss": 2.0147,
      "step": 3880
    },
    {
      "epoch": 0.94970703125,
      "grad_norm": 5.144820213317871,
      "learning_rate": 3.417561848958333e-05,
      "loss": 2.2083,
      "step": 3890
    },
    {
      "epoch": 0.9521484375,
      "grad_norm": 5.8366193771362305,
      "learning_rate": 3.413492838541667e-05,
      "loss": 2.1433,
      "step": 3900
    },
    {
      "epoch": 0.95458984375,
      "grad_norm": 5.713537216186523,
      "learning_rate": 3.409423828125e-05,
      "loss": 2.2895,
      "step": 3910
    },
    {
      "epoch": 0.95703125,
      "grad_norm": 8.695357322692871,
      "learning_rate": 3.405354817708333e-05,
      "loss": 2.1498,
      "step": 3920
    },
    {
      "epoch": 0.95947265625,
      "grad_norm": 4.9469122886657715,
      "learning_rate": 3.401285807291667e-05,
      "loss": 1.9791,
      "step": 3930
    },
    {
      "epoch": 0.9619140625,
      "grad_norm": 4.879045009613037,
      "learning_rate": 3.397216796875e-05,
      "loss": 2.0762,
      "step": 3940
    },
    {
      "epoch": 0.96435546875,
      "grad_norm": 3.8590292930603027,
      "learning_rate": 3.393147786458333e-05,
      "loss": 2.115,
      "step": 3950
    },
    {
      "epoch": 0.966796875,
      "grad_norm": 3.979827642440796,
      "learning_rate": 3.389078776041667e-05,
      "loss": 2.2118,
      "step": 3960
    },
    {
      "epoch": 0.96923828125,
      "grad_norm": 5.604823589324951,
      "learning_rate": 3.385009765625e-05,
      "loss": 2.0404,
      "step": 3970
    },
    {
      "epoch": 0.9716796875,
      "grad_norm": 4.872435092926025,
      "learning_rate": 3.380940755208333e-05,
      "loss": 2.394,
      "step": 3980
    },
    {
      "epoch": 0.97412109375,
      "grad_norm": 3.506906032562256,
      "learning_rate": 3.376871744791667e-05,
      "loss": 2.2548,
      "step": 3990
    },
    {
      "epoch": 0.9765625,
      "grad_norm": 3.880490779876709,
      "learning_rate": 3.372802734375e-05,
      "loss": 2.0268,
      "step": 4000
    },
    {
      "epoch": 0.97900390625,
      "grad_norm": 4.104421615600586,
      "learning_rate": 3.368733723958333e-05,
      "loss": 2.3117,
      "step": 4010
    },
    {
      "epoch": 0.9814453125,
      "grad_norm": 5.074425220489502,
      "learning_rate": 3.364664713541667e-05,
      "loss": 2.134,
      "step": 4020
    },
    {
      "epoch": 0.98388671875,
      "grad_norm": 4.272870063781738,
      "learning_rate": 3.360595703125e-05,
      "loss": 2.2257,
      "step": 4030
    },
    {
      "epoch": 0.986328125,
      "grad_norm": 5.92941427230835,
      "learning_rate": 3.356526692708333e-05,
      "loss": 2.0843,
      "step": 4040
    },
    {
      "epoch": 0.98876953125,
      "grad_norm": 3.3217427730560303,
      "learning_rate": 3.352457682291667e-05,
      "loss": 2.094,
      "step": 4050
    },
    {
      "epoch": 0.9912109375,
      "grad_norm": 5.802567958831787,
      "learning_rate": 3.348388671875e-05,
      "loss": 2.0637,
      "step": 4060
    },
    {
      "epoch": 0.99365234375,
      "grad_norm": 6.098359107971191,
      "learning_rate": 3.3443196614583336e-05,
      "loss": 1.9398,
      "step": 4070
    },
    {
      "epoch": 0.99609375,
      "grad_norm": 6.214400768280029,
      "learning_rate": 3.340250651041667e-05,
      "loss": 2.0832,
      "step": 4080
    },
    {
      "epoch": 0.99853515625,
      "grad_norm": 5.615668296813965,
      "learning_rate": 3.336181640625e-05,
      "loss": 2.0912,
      "step": 4090
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.9499292373657227,
      "eval_runtime": 5.5844,
      "eval_samples_per_second": 22.921,
      "eval_steps_per_second": 22.921,
      "step": 4096
    },
    {
      "epoch": 1.0009765625,
      "grad_norm": 3.8028571605682373,
      "learning_rate": 3.3321126302083336e-05,
      "loss": 2.046,
      "step": 4100
    },
    {
      "epoch": 1.00341796875,
      "grad_norm": 4.744617938995361,
      "learning_rate": 3.328043619791667e-05,
      "loss": 2.2059,
      "step": 4110
    },
    {
      "epoch": 1.005859375,
      "grad_norm": 3.8945696353912354,
      "learning_rate": 3.323974609375e-05,
      "loss": 2.0995,
      "step": 4120
    },
    {
      "epoch": 1.00830078125,
      "grad_norm": 6.2720866203308105,
      "learning_rate": 3.3199055989583336e-05,
      "loss": 2.0771,
      "step": 4130
    },
    {
      "epoch": 1.0107421875,
      "grad_norm": 4.820967197418213,
      "learning_rate": 3.315836588541667e-05,
      "loss": 2.0868,
      "step": 4140
    },
    {
      "epoch": 1.01318359375,
      "grad_norm": 4.339020729064941,
      "learning_rate": 3.311767578125e-05,
      "loss": 1.7594,
      "step": 4150
    },
    {
      "epoch": 1.015625,
      "grad_norm": 5.940157413482666,
      "learning_rate": 3.3076985677083336e-05,
      "loss": 2.2122,
      "step": 4160
    },
    {
      "epoch": 1.01806640625,
      "grad_norm": 4.277635097503662,
      "learning_rate": 3.303629557291667e-05,
      "loss": 2.0534,
      "step": 4170
    },
    {
      "epoch": 1.0205078125,
      "grad_norm": 5.918483257293701,
      "learning_rate": 3.2995605468750005e-05,
      "loss": 2.1058,
      "step": 4180
    },
    {
      "epoch": 1.02294921875,
      "grad_norm": 5.882425308227539,
      "learning_rate": 3.2954915364583336e-05,
      "loss": 2.1244,
      "step": 4190
    },
    {
      "epoch": 1.025390625,
      "grad_norm": 5.704862117767334,
      "learning_rate": 3.291422526041667e-05,
      "loss": 2.054,
      "step": 4200
    },
    {
      "epoch": 1.02783203125,
      "grad_norm": 5.744297027587891,
      "learning_rate": 3.2873535156250005e-05,
      "loss": 2.2398,
      "step": 4210
    },
    {
      "epoch": 1.0302734375,
      "grad_norm": 4.936196327209473,
      "learning_rate": 3.2832845052083336e-05,
      "loss": 2.0402,
      "step": 4220
    },
    {
      "epoch": 1.03271484375,
      "grad_norm": 4.138299942016602,
      "learning_rate": 3.279215494791667e-05,
      "loss": 2.266,
      "step": 4230
    },
    {
      "epoch": 1.03515625,
      "grad_norm": 4.005680084228516,
      "learning_rate": 3.2751464843750005e-05,
      "loss": 2.1323,
      "step": 4240
    },
    {
      "epoch": 1.03759765625,
      "grad_norm": 4.844576358795166,
      "learning_rate": 3.2710774739583336e-05,
      "loss": 2.1223,
      "step": 4250
    },
    {
      "epoch": 1.0400390625,
      "grad_norm": 5.331133842468262,
      "learning_rate": 3.267008463541667e-05,
      "loss": 2.1994,
      "step": 4260
    },
    {
      "epoch": 1.04248046875,
      "grad_norm": 5.722537040710449,
      "learning_rate": 3.2629394531250005e-05,
      "loss": 2.2286,
      "step": 4270
    },
    {
      "epoch": 1.044921875,
      "grad_norm": 4.159865856170654,
      "learning_rate": 3.2588704427083336e-05,
      "loss": 2.2343,
      "step": 4280
    },
    {
      "epoch": 1.04736328125,
      "grad_norm": 3.538085699081421,
      "learning_rate": 3.2548014322916673e-05,
      "loss": 1.9628,
      "step": 4290
    },
    {
      "epoch": 1.0498046875,
      "grad_norm": 3.601834774017334,
      "learning_rate": 3.2507324218750004e-05,
      "loss": 2.0611,
      "step": 4300
    },
    {
      "epoch": 1.05224609375,
      "grad_norm": 7.935354709625244,
      "learning_rate": 3.2466634114583336e-05,
      "loss": 2.0316,
      "step": 4310
    },
    {
      "epoch": 1.0546875,
      "grad_norm": 6.32449197769165,
      "learning_rate": 3.242594401041667e-05,
      "loss": 2.1514,
      "step": 4320
    },
    {
      "epoch": 1.05712890625,
      "grad_norm": 4.418630123138428,
      "learning_rate": 3.2385253906250004e-05,
      "loss": 1.8775,
      "step": 4330
    },
    {
      "epoch": 1.0595703125,
      "grad_norm": 3.2944133281707764,
      "learning_rate": 3.2344563802083335e-05,
      "loss": 2.1143,
      "step": 4340
    },
    {
      "epoch": 1.06201171875,
      "grad_norm": 5.372310638427734,
      "learning_rate": 3.230387369791667e-05,
      "loss": 1.9883,
      "step": 4350
    },
    {
      "epoch": 1.064453125,
      "grad_norm": 4.026620388031006,
      "learning_rate": 3.2263183593750004e-05,
      "loss": 2.1022,
      "step": 4360
    },
    {
      "epoch": 1.06689453125,
      "grad_norm": 4.241952896118164,
      "learning_rate": 3.2222493489583335e-05,
      "loss": 2.1856,
      "step": 4370
    },
    {
      "epoch": 1.0693359375,
      "grad_norm": 5.740052700042725,
      "learning_rate": 3.218180338541667e-05,
      "loss": 2.1323,
      "step": 4380
    },
    {
      "epoch": 1.07177734375,
      "grad_norm": 4.600122928619385,
      "learning_rate": 3.2141113281250004e-05,
      "loss": 2.1527,
      "step": 4390
    },
    {
      "epoch": 1.07421875,
      "grad_norm": 5.37387228012085,
      "learning_rate": 3.2100423177083335e-05,
      "loss": 2.2259,
      "step": 4400
    },
    {
      "epoch": 1.07666015625,
      "grad_norm": 3.6218996047973633,
      "learning_rate": 3.205973307291667e-05,
      "loss": 2.0849,
      "step": 4410
    },
    {
      "epoch": 1.0791015625,
      "grad_norm": 5.397819519042969,
      "learning_rate": 3.2019042968750004e-05,
      "loss": 2.0458,
      "step": 4420
    },
    {
      "epoch": 1.08154296875,
      "grad_norm": 5.564067363739014,
      "learning_rate": 3.1978352864583335e-05,
      "loss": 2.0152,
      "step": 4430
    },
    {
      "epoch": 1.083984375,
      "grad_norm": 5.219688415527344,
      "learning_rate": 3.193766276041667e-05,
      "loss": 2.0913,
      "step": 4440
    },
    {
      "epoch": 1.08642578125,
      "grad_norm": 3.804114818572998,
      "learning_rate": 3.1896972656250004e-05,
      "loss": 2.1591,
      "step": 4450
    },
    {
      "epoch": 1.0888671875,
      "grad_norm": 4.660396575927734,
      "learning_rate": 3.1856282552083335e-05,
      "loss": 2.0192,
      "step": 4460
    },
    {
      "epoch": 1.09130859375,
      "grad_norm": 6.286740303039551,
      "learning_rate": 3.181559244791667e-05,
      "loss": 2.136,
      "step": 4470
    },
    {
      "epoch": 1.09375,
      "grad_norm": 5.909531593322754,
      "learning_rate": 3.1774902343750004e-05,
      "loss": 2.0256,
      "step": 4480
    },
    {
      "epoch": 1.09619140625,
      "grad_norm": 3.398137331008911,
      "learning_rate": 3.1734212239583335e-05,
      "loss": 2.2186,
      "step": 4490
    },
    {
      "epoch": 1.0986328125,
      "grad_norm": 6.224480152130127,
      "learning_rate": 3.169352213541667e-05,
      "loss": 1.9908,
      "step": 4500
    },
    {
      "epoch": 1.10107421875,
      "grad_norm": 3.9754509925842285,
      "learning_rate": 3.1652832031250004e-05,
      "loss": 1.9835,
      "step": 4510
    },
    {
      "epoch": 1.103515625,
      "grad_norm": 5.386972904205322,
      "learning_rate": 3.1612141927083335e-05,
      "loss": 1.953,
      "step": 4520
    },
    {
      "epoch": 1.10595703125,
      "grad_norm": 4.520281791687012,
      "learning_rate": 3.157145182291667e-05,
      "loss": 2.0473,
      "step": 4530
    },
    {
      "epoch": 1.1083984375,
      "grad_norm": 6.2722039222717285,
      "learning_rate": 3.1530761718750004e-05,
      "loss": 2.1049,
      "step": 4540
    },
    {
      "epoch": 1.11083984375,
      "grad_norm": 5.103692531585693,
      "learning_rate": 3.1490071614583335e-05,
      "loss": 2.0394,
      "step": 4550
    },
    {
      "epoch": 1.11328125,
      "grad_norm": 5.1469831466674805,
      "learning_rate": 3.144938151041667e-05,
      "loss": 2.176,
      "step": 4560
    },
    {
      "epoch": 1.11572265625,
      "grad_norm": 5.631259918212891,
      "learning_rate": 3.1408691406250004e-05,
      "loss": 2.2127,
      "step": 4570
    },
    {
      "epoch": 1.1181640625,
      "grad_norm": 7.478880405426025,
      "learning_rate": 3.1368001302083335e-05,
      "loss": 2.0935,
      "step": 4580
    },
    {
      "epoch": 1.12060546875,
      "grad_norm": 5.39470911026001,
      "learning_rate": 3.132731119791667e-05,
      "loss": 2.017,
      "step": 4590
    },
    {
      "epoch": 1.123046875,
      "grad_norm": 5.280248165130615,
      "learning_rate": 3.1286621093750003e-05,
      "loss": 1.9433,
      "step": 4600
    },
    {
      "epoch": 1.12548828125,
      "grad_norm": 5.531657695770264,
      "learning_rate": 3.1245930989583334e-05,
      "loss": 2.0775,
      "step": 4610
    },
    {
      "epoch": 1.1279296875,
      "grad_norm": 3.3621957302093506,
      "learning_rate": 3.120524088541667e-05,
      "loss": 2.2932,
      "step": 4620
    },
    {
      "epoch": 1.13037109375,
      "grad_norm": 6.4848246574401855,
      "learning_rate": 3.116455078125e-05,
      "loss": 2.31,
      "step": 4630
    },
    {
      "epoch": 1.1328125,
      "grad_norm": 4.594645023345947,
      "learning_rate": 3.1123860677083334e-05,
      "loss": 2.0247,
      "step": 4640
    },
    {
      "epoch": 1.13525390625,
      "grad_norm": 3.4032232761383057,
      "learning_rate": 3.108317057291667e-05,
      "loss": 2.0192,
      "step": 4650
    },
    {
      "epoch": 1.1376953125,
      "grad_norm": 4.6440324783325195,
      "learning_rate": 3.104248046875e-05,
      "loss": 2.0441,
      "step": 4660
    },
    {
      "epoch": 1.14013671875,
      "grad_norm": 3.2916409969329834,
      "learning_rate": 3.1001790364583334e-05,
      "loss": 2.2148,
      "step": 4670
    },
    {
      "epoch": 1.142578125,
      "grad_norm": 3.90097975730896,
      "learning_rate": 3.096110026041667e-05,
      "loss": 2.055,
      "step": 4680
    },
    {
      "epoch": 1.14501953125,
      "grad_norm": 2.9745352268218994,
      "learning_rate": 3.092041015625e-05,
      "loss": 1.9622,
      "step": 4690
    },
    {
      "epoch": 1.1474609375,
      "grad_norm": 5.279110431671143,
      "learning_rate": 3.0879720052083334e-05,
      "loss": 2.075,
      "step": 4700
    },
    {
      "epoch": 1.14990234375,
      "grad_norm": 5.483469009399414,
      "learning_rate": 3.083902994791667e-05,
      "loss": 2.1547,
      "step": 4710
    },
    {
      "epoch": 1.15234375,
      "grad_norm": 4.0258965492248535,
      "learning_rate": 3.079833984375e-05,
      "loss": 2.1277,
      "step": 4720
    },
    {
      "epoch": 1.15478515625,
      "grad_norm": 4.80516242980957,
      "learning_rate": 3.0757649739583334e-05,
      "loss": 2.026,
      "step": 4730
    },
    {
      "epoch": 1.1572265625,
      "grad_norm": 9.013875961303711,
      "learning_rate": 3.071695963541667e-05,
      "loss": 2.3033,
      "step": 4740
    },
    {
      "epoch": 1.15966796875,
      "grad_norm": 4.718714714050293,
      "learning_rate": 3.067626953125e-05,
      "loss": 2.1824,
      "step": 4750
    },
    {
      "epoch": 1.162109375,
      "grad_norm": 3.6022794246673584,
      "learning_rate": 3.0635579427083334e-05,
      "loss": 2.2802,
      "step": 4760
    },
    {
      "epoch": 1.16455078125,
      "grad_norm": 8.0698881149292,
      "learning_rate": 3.059488932291667e-05,
      "loss": 2.2386,
      "step": 4770
    },
    {
      "epoch": 1.1669921875,
      "grad_norm": 5.863948822021484,
      "learning_rate": 3.055419921875e-05,
      "loss": 2.07,
      "step": 4780
    },
    {
      "epoch": 1.16943359375,
      "grad_norm": 4.96919059753418,
      "learning_rate": 3.0513509114583334e-05,
      "loss": 2.2408,
      "step": 4790
    },
    {
      "epoch": 1.171875,
      "grad_norm": 5.416713714599609,
      "learning_rate": 3.047281901041667e-05,
      "loss": 2.1277,
      "step": 4800
    },
    {
      "epoch": 1.17431640625,
      "grad_norm": 5.396754741668701,
      "learning_rate": 3.0432128906250003e-05,
      "loss": 2.0945,
      "step": 4810
    },
    {
      "epoch": 1.1767578125,
      "grad_norm": 5.531222820281982,
      "learning_rate": 3.0391438802083334e-05,
      "loss": 1.9968,
      "step": 4820
    },
    {
      "epoch": 1.17919921875,
      "grad_norm": 3.413843870162964,
      "learning_rate": 3.035074869791667e-05,
      "loss": 2.0329,
      "step": 4830
    },
    {
      "epoch": 1.181640625,
      "grad_norm": 6.359799385070801,
      "learning_rate": 3.0310058593750003e-05,
      "loss": 2.3913,
      "step": 4840
    },
    {
      "epoch": 1.18408203125,
      "grad_norm": 5.1488566398620605,
      "learning_rate": 3.0269368489583334e-05,
      "loss": 2.2147,
      "step": 4850
    },
    {
      "epoch": 1.1865234375,
      "grad_norm": 4.037153244018555,
      "learning_rate": 3.022867838541667e-05,
      "loss": 2.0948,
      "step": 4860
    },
    {
      "epoch": 1.18896484375,
      "grad_norm": 6.085233688354492,
      "learning_rate": 3.0187988281250002e-05,
      "loss": 2.0849,
      "step": 4870
    },
    {
      "epoch": 1.19140625,
      "grad_norm": 4.257367134094238,
      "learning_rate": 3.0147298177083333e-05,
      "loss": 1.9879,
      "step": 4880
    },
    {
      "epoch": 1.19384765625,
      "grad_norm": 5.69932746887207,
      "learning_rate": 3.010660807291667e-05,
      "loss": 1.9696,
      "step": 4890
    },
    {
      "epoch": 1.1962890625,
      "grad_norm": 3.777970314025879,
      "learning_rate": 3.0065917968750002e-05,
      "loss": 2.1336,
      "step": 4900
    },
    {
      "epoch": 1.19873046875,
      "grad_norm": 5.570672988891602,
      "learning_rate": 3.0025227864583333e-05,
      "loss": 2.0157,
      "step": 4910
    },
    {
      "epoch": 1.201171875,
      "grad_norm": 4.786208629608154,
      "learning_rate": 2.998453776041667e-05,
      "loss": 2.2082,
      "step": 4920
    },
    {
      "epoch": 1.20361328125,
      "grad_norm": 5.693769931793213,
      "learning_rate": 2.9943847656250002e-05,
      "loss": 2.2259,
      "step": 4930
    },
    {
      "epoch": 1.2060546875,
      "grad_norm": 3.4688608646392822,
      "learning_rate": 2.9903157552083333e-05,
      "loss": 2.1622,
      "step": 4940
    },
    {
      "epoch": 1.20849609375,
      "grad_norm": 7.562938213348389,
      "learning_rate": 2.986246744791667e-05,
      "loss": 1.9831,
      "step": 4950
    },
    {
      "epoch": 1.2109375,
      "grad_norm": 5.134135723114014,
      "learning_rate": 2.9821777343750002e-05,
      "loss": 2.1802,
      "step": 4960
    },
    {
      "epoch": 1.21337890625,
      "grad_norm": 5.205445289611816,
      "learning_rate": 2.9781087239583333e-05,
      "loss": 2.0042,
      "step": 4970
    },
    {
      "epoch": 1.2158203125,
      "grad_norm": 3.6134965419769287,
      "learning_rate": 2.974039713541667e-05,
      "loss": 2.0161,
      "step": 4980
    },
    {
      "epoch": 1.21826171875,
      "grad_norm": 4.677590847015381,
      "learning_rate": 2.9699707031250002e-05,
      "loss": 2.1048,
      "step": 4990
    },
    {
      "epoch": 1.220703125,
      "grad_norm": 5.849071979522705,
      "learning_rate": 2.9659016927083333e-05,
      "loss": 2.2422,
      "step": 5000
    },
    {
      "epoch": 1.22314453125,
      "grad_norm": 4.220547676086426,
      "learning_rate": 2.961832682291667e-05,
      "loss": 2.3811,
      "step": 5010
    },
    {
      "epoch": 1.2255859375,
      "grad_norm": 6.969424247741699,
      "learning_rate": 2.9577636718750002e-05,
      "loss": 2.0063,
      "step": 5020
    },
    {
      "epoch": 1.22802734375,
      "grad_norm": 5.115553379058838,
      "learning_rate": 2.9536946614583333e-05,
      "loss": 2.0598,
      "step": 5030
    },
    {
      "epoch": 1.23046875,
      "grad_norm": 4.023085117340088,
      "learning_rate": 2.949625651041667e-05,
      "loss": 2.1117,
      "step": 5040
    },
    {
      "epoch": 1.23291015625,
      "grad_norm": 4.902757167816162,
      "learning_rate": 2.9455566406250002e-05,
      "loss": 2.0662,
      "step": 5050
    },
    {
      "epoch": 1.2353515625,
      "grad_norm": 5.756074905395508,
      "learning_rate": 2.9414876302083333e-05,
      "loss": 2.1458,
      "step": 5060
    },
    {
      "epoch": 1.23779296875,
      "grad_norm": 5.582665920257568,
      "learning_rate": 2.937418619791667e-05,
      "loss": 1.9787,
      "step": 5070
    },
    {
      "epoch": 1.240234375,
      "grad_norm": 6.920862197875977,
      "learning_rate": 2.933349609375e-05,
      "loss": 2.0226,
      "step": 5080
    },
    {
      "epoch": 1.24267578125,
      "grad_norm": 6.324738502502441,
      "learning_rate": 2.9292805989583333e-05,
      "loss": 2.0867,
      "step": 5090
    },
    {
      "epoch": 1.2451171875,
      "grad_norm": 4.223093509674072,
      "learning_rate": 2.925211588541667e-05,
      "loss": 2.1352,
      "step": 5100
    },
    {
      "epoch": 1.24755859375,
      "grad_norm": 5.129189968109131,
      "learning_rate": 2.921142578125e-05,
      "loss": 2.1147,
      "step": 5110
    },
    {
      "epoch": 1.25,
      "grad_norm": 4.253347396850586,
      "learning_rate": 2.9170735677083333e-05,
      "loss": 2.0592,
      "step": 5120
    },
    {
      "epoch": 1.25244140625,
      "grad_norm": 5.63824462890625,
      "learning_rate": 2.913004557291667e-05,
      "loss": 2.1452,
      "step": 5130
    },
    {
      "epoch": 1.2548828125,
      "grad_norm": 5.734968662261963,
      "learning_rate": 2.908935546875e-05,
      "loss": 2.2564,
      "step": 5140
    },
    {
      "epoch": 1.25732421875,
      "grad_norm": 3.9435479640960693,
      "learning_rate": 2.9048665364583332e-05,
      "loss": 2.062,
      "step": 5150
    },
    {
      "epoch": 1.259765625,
      "grad_norm": 4.816163063049316,
      "learning_rate": 2.900797526041667e-05,
      "loss": 2.1204,
      "step": 5160
    },
    {
      "epoch": 1.26220703125,
      "grad_norm": 6.192443370819092,
      "learning_rate": 2.896728515625e-05,
      "loss": 2.1598,
      "step": 5170
    },
    {
      "epoch": 1.2646484375,
      "grad_norm": 6.170301914215088,
      "learning_rate": 2.8926595052083332e-05,
      "loss": 2.0609,
      "step": 5180
    },
    {
      "epoch": 1.26708984375,
      "grad_norm": 4.07689905166626,
      "learning_rate": 2.888590494791667e-05,
      "loss": 2.2389,
      "step": 5190
    },
    {
      "epoch": 1.26953125,
      "grad_norm": 7.666715621948242,
      "learning_rate": 2.884521484375e-05,
      "loss": 2.1618,
      "step": 5200
    },
    {
      "epoch": 1.27197265625,
      "grad_norm": 5.686585426330566,
      "learning_rate": 2.8804524739583332e-05,
      "loss": 2.1365,
      "step": 5210
    },
    {
      "epoch": 1.2744140625,
      "grad_norm": 4.690385818481445,
      "learning_rate": 2.876383463541667e-05,
      "loss": 1.913,
      "step": 5220
    },
    {
      "epoch": 1.27685546875,
      "grad_norm": 6.590240478515625,
      "learning_rate": 2.872314453125e-05,
      "loss": 2.221,
      "step": 5230
    },
    {
      "epoch": 1.279296875,
      "grad_norm": 7.2735114097595215,
      "learning_rate": 2.8682454427083332e-05,
      "loss": 2.06,
      "step": 5240
    },
    {
      "epoch": 1.28173828125,
      "grad_norm": 4.465279579162598,
      "learning_rate": 2.864176432291667e-05,
      "loss": 2.1213,
      "step": 5250
    },
    {
      "epoch": 1.2841796875,
      "grad_norm": 4.212214469909668,
      "learning_rate": 2.860107421875e-05,
      "loss": 2.0089,
      "step": 5260
    },
    {
      "epoch": 1.28662109375,
      "grad_norm": 3.7870328426361084,
      "learning_rate": 2.8560384114583332e-05,
      "loss": 2.0114,
      "step": 5270
    },
    {
      "epoch": 1.2890625,
      "grad_norm": 5.0216474533081055,
      "learning_rate": 2.851969401041667e-05,
      "loss": 2.1608,
      "step": 5280
    },
    {
      "epoch": 1.29150390625,
      "grad_norm": 6.641658782958984,
      "learning_rate": 2.847900390625e-05,
      "loss": 2.0781,
      "step": 5290
    },
    {
      "epoch": 1.2939453125,
      "grad_norm": 5.501990795135498,
      "learning_rate": 2.8438313802083332e-05,
      "loss": 2.1048,
      "step": 5300
    },
    {
      "epoch": 1.29638671875,
      "grad_norm": 5.597706317901611,
      "learning_rate": 2.839762369791667e-05,
      "loss": 2.1568,
      "step": 5310
    },
    {
      "epoch": 1.298828125,
      "grad_norm": 8.24314022064209,
      "learning_rate": 2.835693359375e-05,
      "loss": 1.9831,
      "step": 5320
    },
    {
      "epoch": 1.30126953125,
      "grad_norm": 6.870297431945801,
      "learning_rate": 2.8316243489583332e-05,
      "loss": 2.2554,
      "step": 5330
    },
    {
      "epoch": 1.3037109375,
      "grad_norm": 3.9684996604919434,
      "learning_rate": 2.827555338541667e-05,
      "loss": 2.0441,
      "step": 5340
    },
    {
      "epoch": 1.30615234375,
      "grad_norm": 3.8889400959014893,
      "learning_rate": 2.823486328125e-05,
      "loss": 2.0219,
      "step": 5350
    },
    {
      "epoch": 1.30859375,
      "grad_norm": 4.222235202789307,
      "learning_rate": 2.8194173177083332e-05,
      "loss": 2.0626,
      "step": 5360
    },
    {
      "epoch": 1.31103515625,
      "grad_norm": 4.856227397918701,
      "learning_rate": 2.815348307291667e-05,
      "loss": 1.9657,
      "step": 5370
    },
    {
      "epoch": 1.3134765625,
      "grad_norm": 6.135642051696777,
      "learning_rate": 2.811279296875e-05,
      "loss": 2.0856,
      "step": 5380
    },
    {
      "epoch": 1.31591796875,
      "grad_norm": 5.96342658996582,
      "learning_rate": 2.807210286458333e-05,
      "loss": 2.0243,
      "step": 5390
    },
    {
      "epoch": 1.318359375,
      "grad_norm": 4.579171180725098,
      "learning_rate": 2.803141276041667e-05,
      "loss": 2.0538,
      "step": 5400
    },
    {
      "epoch": 1.32080078125,
      "grad_norm": 4.241312026977539,
      "learning_rate": 2.799072265625e-05,
      "loss": 1.9955,
      "step": 5410
    },
    {
      "epoch": 1.3232421875,
      "grad_norm": 5.938989639282227,
      "learning_rate": 2.795003255208333e-05,
      "loss": 2.1326,
      "step": 5420
    },
    {
      "epoch": 1.32568359375,
      "grad_norm": 3.8056716918945312,
      "learning_rate": 2.790934244791667e-05,
      "loss": 2.0677,
      "step": 5430
    },
    {
      "epoch": 1.328125,
      "grad_norm": 5.455025672912598,
      "learning_rate": 2.786865234375e-05,
      "loss": 2.0226,
      "step": 5440
    },
    {
      "epoch": 1.33056640625,
      "grad_norm": 4.133018493652344,
      "learning_rate": 2.782796223958333e-05,
      "loss": 1.9999,
      "step": 5450
    },
    {
      "epoch": 1.3330078125,
      "grad_norm": 6.939018726348877,
      "learning_rate": 2.778727213541667e-05,
      "loss": 2.0929,
      "step": 5460
    },
    {
      "epoch": 1.33544921875,
      "grad_norm": 4.007261753082275,
      "learning_rate": 2.774658203125e-05,
      "loss": 2.129,
      "step": 5470
    },
    {
      "epoch": 1.337890625,
      "grad_norm": 6.245683193206787,
      "learning_rate": 2.770589192708333e-05,
      "loss": 2.0567,
      "step": 5480
    },
    {
      "epoch": 1.34033203125,
      "grad_norm": 3.877742052078247,
      "learning_rate": 2.766520182291667e-05,
      "loss": 2.0694,
      "step": 5490
    },
    {
      "epoch": 1.3427734375,
      "grad_norm": 3.891040325164795,
      "learning_rate": 2.762451171875e-05,
      "loss": 2.2443,
      "step": 5500
    },
    {
      "epoch": 1.34521484375,
      "grad_norm": 4.187836647033691,
      "learning_rate": 2.758382161458333e-05,
      "loss": 2.138,
      "step": 5510
    },
    {
      "epoch": 1.34765625,
      "grad_norm": 5.104190349578857,
      "learning_rate": 2.754313151041667e-05,
      "loss": 2.0301,
      "step": 5520
    },
    {
      "epoch": 1.35009765625,
      "grad_norm": 3.469799518585205,
      "learning_rate": 2.750244140625e-05,
      "loss": 2.0036,
      "step": 5530
    },
    {
      "epoch": 1.3525390625,
      "grad_norm": 5.63142728805542,
      "learning_rate": 2.7461751302083334e-05,
      "loss": 2.0349,
      "step": 5540
    },
    {
      "epoch": 1.35498046875,
      "grad_norm": 4.0620436668396,
      "learning_rate": 2.742106119791667e-05,
      "loss": 2.0408,
      "step": 5550
    },
    {
      "epoch": 1.357421875,
      "grad_norm": 5.164649963378906,
      "learning_rate": 2.738037109375e-05,
      "loss": 2.071,
      "step": 5560
    },
    {
      "epoch": 1.35986328125,
      "grad_norm": 7.814147472381592,
      "learning_rate": 2.7339680989583334e-05,
      "loss": 2.1519,
      "step": 5570
    },
    {
      "epoch": 1.3623046875,
      "grad_norm": 4.52758264541626,
      "learning_rate": 2.729899088541667e-05,
      "loss": 2.1371,
      "step": 5580
    },
    {
      "epoch": 1.36474609375,
      "grad_norm": 4.832249641418457,
      "learning_rate": 2.725830078125e-05,
      "loss": 2.1666,
      "step": 5590
    },
    {
      "epoch": 1.3671875,
      "grad_norm": 4.392660617828369,
      "learning_rate": 2.7217610677083334e-05,
      "loss": 2.151,
      "step": 5600
    },
    {
      "epoch": 1.36962890625,
      "grad_norm": 5.796478271484375,
      "learning_rate": 2.717692057291667e-05,
      "loss": 1.9575,
      "step": 5610
    },
    {
      "epoch": 1.3720703125,
      "grad_norm": 5.016470432281494,
      "learning_rate": 2.713623046875e-05,
      "loss": 1.9687,
      "step": 5620
    },
    {
      "epoch": 1.37451171875,
      "grad_norm": 4.742119789123535,
      "learning_rate": 2.7095540364583334e-05,
      "loss": 2.1541,
      "step": 5630
    },
    {
      "epoch": 1.376953125,
      "grad_norm": 5.391598224639893,
      "learning_rate": 2.705485026041667e-05,
      "loss": 2.0347,
      "step": 5640
    },
    {
      "epoch": 1.37939453125,
      "grad_norm": 5.368424892425537,
      "learning_rate": 2.7014160156250003e-05,
      "loss": 2.1373,
      "step": 5650
    },
    {
      "epoch": 1.3818359375,
      "grad_norm": 5.390387535095215,
      "learning_rate": 2.6973470052083334e-05,
      "loss": 2.2192,
      "step": 5660
    },
    {
      "epoch": 1.38427734375,
      "grad_norm": 7.482875823974609,
      "learning_rate": 2.693277994791667e-05,
      "loss": 2.156,
      "step": 5670
    },
    {
      "epoch": 1.38671875,
      "grad_norm": 5.834981918334961,
      "learning_rate": 2.6892089843750003e-05,
      "loss": 1.9295,
      "step": 5680
    },
    {
      "epoch": 1.38916015625,
      "grad_norm": 4.534455299377441,
      "learning_rate": 2.6851399739583334e-05,
      "loss": 2.2259,
      "step": 5690
    },
    {
      "epoch": 1.3916015625,
      "grad_norm": 5.06558084487915,
      "learning_rate": 2.681070963541667e-05,
      "loss": 2.1475,
      "step": 5700
    },
    {
      "epoch": 1.39404296875,
      "grad_norm": 3.988117218017578,
      "learning_rate": 2.6770019531250003e-05,
      "loss": 2.0986,
      "step": 5710
    },
    {
      "epoch": 1.396484375,
      "grad_norm": 5.959949970245361,
      "learning_rate": 2.6729329427083334e-05,
      "loss": 2.1644,
      "step": 5720
    },
    {
      "epoch": 1.39892578125,
      "grad_norm": 4.840933322906494,
      "learning_rate": 2.6688639322916668e-05,
      "loss": 2.3875,
      "step": 5730
    },
    {
      "epoch": 1.4013671875,
      "grad_norm": 4.71920108795166,
      "learning_rate": 2.6647949218750003e-05,
      "loss": 1.951,
      "step": 5740
    },
    {
      "epoch": 1.40380859375,
      "grad_norm": 6.020569801330566,
      "learning_rate": 2.6607259114583334e-05,
      "loss": 2.0964,
      "step": 5750
    },
    {
      "epoch": 1.40625,
      "grad_norm": 6.336606502532959,
      "learning_rate": 2.6566569010416668e-05,
      "loss": 2.2443,
      "step": 5760
    },
    {
      "epoch": 1.40869140625,
      "grad_norm": 5.085933208465576,
      "learning_rate": 2.6525878906250003e-05,
      "loss": 2.1091,
      "step": 5770
    },
    {
      "epoch": 1.4111328125,
      "grad_norm": 3.749662399291992,
      "learning_rate": 2.6485188802083334e-05,
      "loss": 2.0117,
      "step": 5780
    },
    {
      "epoch": 1.41357421875,
      "grad_norm": 5.775843620300293,
      "learning_rate": 2.644449869791667e-05,
      "loss": 1.9573,
      "step": 5790
    },
    {
      "epoch": 1.416015625,
      "grad_norm": 3.450472593307495,
      "learning_rate": 2.6403808593750002e-05,
      "loss": 2.0262,
      "step": 5800
    },
    {
      "epoch": 1.41845703125,
      "grad_norm": 6.0798563957214355,
      "learning_rate": 2.6363118489583333e-05,
      "loss": 2.0132,
      "step": 5810
    },
    {
      "epoch": 1.4208984375,
      "grad_norm": 3.9826855659484863,
      "learning_rate": 2.632242838541667e-05,
      "loss": 1.9798,
      "step": 5820
    },
    {
      "epoch": 1.42333984375,
      "grad_norm": 3.670128107070923,
      "learning_rate": 2.6281738281250002e-05,
      "loss": 2.2212,
      "step": 5830
    },
    {
      "epoch": 1.42578125,
      "grad_norm": 4.503645420074463,
      "learning_rate": 2.6241048177083333e-05,
      "loss": 2.05,
      "step": 5840
    },
    {
      "epoch": 1.42822265625,
      "grad_norm": 5.3430562019348145,
      "learning_rate": 2.620035807291667e-05,
      "loss": 1.9962,
      "step": 5850
    },
    {
      "epoch": 1.4306640625,
      "grad_norm": 5.568920612335205,
      "learning_rate": 2.6159667968750002e-05,
      "loss": 1.9019,
      "step": 5860
    },
    {
      "epoch": 1.43310546875,
      "grad_norm": 6.321529388427734,
      "learning_rate": 2.6118977864583333e-05,
      "loss": 2.036,
      "step": 5870
    },
    {
      "epoch": 1.435546875,
      "grad_norm": 7.273078918457031,
      "learning_rate": 2.607828776041667e-05,
      "loss": 2.0252,
      "step": 5880
    },
    {
      "epoch": 1.43798828125,
      "grad_norm": 4.44276762008667,
      "learning_rate": 2.6037597656250002e-05,
      "loss": 2.0531,
      "step": 5890
    },
    {
      "epoch": 1.4404296875,
      "grad_norm": 5.405457019805908,
      "learning_rate": 2.5996907552083333e-05,
      "loss": 1.9521,
      "step": 5900
    },
    {
      "epoch": 1.44287109375,
      "grad_norm": 4.827792644500732,
      "learning_rate": 2.595621744791667e-05,
      "loss": 2.0798,
      "step": 5910
    },
    {
      "epoch": 1.4453125,
      "grad_norm": 5.888503074645996,
      "learning_rate": 2.5915527343750002e-05,
      "loss": 2.1097,
      "step": 5920
    },
    {
      "epoch": 1.44775390625,
      "grad_norm": 3.972017288208008,
      "learning_rate": 2.5874837239583333e-05,
      "loss": 1.9675,
      "step": 5930
    },
    {
      "epoch": 1.4501953125,
      "grad_norm": 5.379756927490234,
      "learning_rate": 2.583414713541667e-05,
      "loss": 2.0958,
      "step": 5940
    },
    {
      "epoch": 1.45263671875,
      "grad_norm": 6.075850963592529,
      "learning_rate": 2.5793457031250002e-05,
      "loss": 2.1283,
      "step": 5950
    },
    {
      "epoch": 1.455078125,
      "grad_norm": 6.659461498260498,
      "learning_rate": 2.5752766927083333e-05,
      "loss": 1.8692,
      "step": 5960
    },
    {
      "epoch": 1.45751953125,
      "grad_norm": 6.173333644866943,
      "learning_rate": 2.571207682291667e-05,
      "loss": 2.0589,
      "step": 5970
    },
    {
      "epoch": 1.4599609375,
      "grad_norm": 5.188580513000488,
      "learning_rate": 2.5671386718750002e-05,
      "loss": 2.0943,
      "step": 5980
    },
    {
      "epoch": 1.46240234375,
      "grad_norm": 4.593975067138672,
      "learning_rate": 2.5630696614583333e-05,
      "loss": 2.0697,
      "step": 5990
    },
    {
      "epoch": 1.46484375,
      "grad_norm": 4.452277660369873,
      "learning_rate": 2.559000651041667e-05,
      "loss": 1.9353,
      "step": 6000
    },
    {
      "epoch": 1.46728515625,
      "grad_norm": 4.4525933265686035,
      "learning_rate": 2.554931640625e-05,
      "loss": 2.1202,
      "step": 6010
    },
    {
      "epoch": 1.4697265625,
      "grad_norm": 6.132906436920166,
      "learning_rate": 2.5508626302083333e-05,
      "loss": 2.0643,
      "step": 6020
    },
    {
      "epoch": 1.47216796875,
      "grad_norm": 4.9953484535217285,
      "learning_rate": 2.546793619791667e-05,
      "loss": 1.8305,
      "step": 6030
    },
    {
      "epoch": 1.474609375,
      "grad_norm": 3.680490732192993,
      "learning_rate": 2.542724609375e-05,
      "loss": 1.9873,
      "step": 6040
    },
    {
      "epoch": 1.47705078125,
      "grad_norm": 5.051680088043213,
      "learning_rate": 2.5386555989583333e-05,
      "loss": 2.0223,
      "step": 6050
    },
    {
      "epoch": 1.4794921875,
      "grad_norm": 3.701298236846924,
      "learning_rate": 2.534586588541667e-05,
      "loss": 2.0871,
      "step": 6060
    },
    {
      "epoch": 1.48193359375,
      "grad_norm": 6.131632328033447,
      "learning_rate": 2.530517578125e-05,
      "loss": 2.1247,
      "step": 6070
    },
    {
      "epoch": 1.484375,
      "grad_norm": 6.47396993637085,
      "learning_rate": 2.5264485677083333e-05,
      "loss": 2.0002,
      "step": 6080
    },
    {
      "epoch": 1.48681640625,
      "grad_norm": 7.483730316162109,
      "learning_rate": 2.522379557291667e-05,
      "loss": 1.8332,
      "step": 6090
    },
    {
      "epoch": 1.4892578125,
      "grad_norm": 5.936739444732666,
      "learning_rate": 2.518310546875e-05,
      "loss": 2.0162,
      "step": 6100
    },
    {
      "epoch": 1.49169921875,
      "grad_norm": 5.993351459503174,
      "learning_rate": 2.5142415364583332e-05,
      "loss": 2.0029,
      "step": 6110
    },
    {
      "epoch": 1.494140625,
      "grad_norm": 6.9295430183410645,
      "learning_rate": 2.510172526041667e-05,
      "loss": 2.0666,
      "step": 6120
    },
    {
      "epoch": 1.49658203125,
      "grad_norm": 12.586386680603027,
      "learning_rate": 2.506103515625e-05,
      "loss": 2.34,
      "step": 6130
    },
    {
      "epoch": 1.4990234375,
      "grad_norm": 4.260796070098877,
      "learning_rate": 2.5020345052083332e-05,
      "loss": 1.9524,
      "step": 6140
    },
    {
      "epoch": 1.50146484375,
      "grad_norm": 4.037591457366943,
      "learning_rate": 2.4979654947916667e-05,
      "loss": 2.2293,
      "step": 6150
    },
    {
      "epoch": 1.50390625,
      "grad_norm": 5.503134727478027,
      "learning_rate": 2.493896484375e-05,
      "loss": 1.9513,
      "step": 6160
    },
    {
      "epoch": 1.50634765625,
      "grad_norm": 7.478815078735352,
      "learning_rate": 2.4898274739583336e-05,
      "loss": 2.0923,
      "step": 6170
    },
    {
      "epoch": 1.5087890625,
      "grad_norm": 4.353801727294922,
      "learning_rate": 2.4857584635416667e-05,
      "loss": 2.0848,
      "step": 6180
    },
    {
      "epoch": 1.51123046875,
      "grad_norm": 3.179994821548462,
      "learning_rate": 2.481689453125e-05,
      "loss": 2.1313,
      "step": 6190
    },
    {
      "epoch": 1.513671875,
      "grad_norm": 4.348505020141602,
      "learning_rate": 2.4776204427083335e-05,
      "loss": 2.0973,
      "step": 6200
    },
    {
      "epoch": 1.51611328125,
      "grad_norm": 7.6291823387146,
      "learning_rate": 2.4735514322916667e-05,
      "loss": 2.0389,
      "step": 6210
    },
    {
      "epoch": 1.5185546875,
      "grad_norm": 5.068423748016357,
      "learning_rate": 2.469482421875e-05,
      "loss": 1.9802,
      "step": 6220
    },
    {
      "epoch": 1.52099609375,
      "grad_norm": 4.72747802734375,
      "learning_rate": 2.4654134114583335e-05,
      "loss": 2.1493,
      "step": 6230
    },
    {
      "epoch": 1.5234375,
      "grad_norm": 3.4256091117858887,
      "learning_rate": 2.4613444010416666e-05,
      "loss": 2.1807,
      "step": 6240
    },
    {
      "epoch": 1.52587890625,
      "grad_norm": 4.313953876495361,
      "learning_rate": 2.457275390625e-05,
      "loss": 1.9765,
      "step": 6250
    },
    {
      "epoch": 1.5283203125,
      "grad_norm": 5.018518447875977,
      "learning_rate": 2.4532063802083335e-05,
      "loss": 1.9795,
      "step": 6260
    },
    {
      "epoch": 1.53076171875,
      "grad_norm": 4.837173938751221,
      "learning_rate": 2.4491373697916666e-05,
      "loss": 1.9984,
      "step": 6270
    },
    {
      "epoch": 1.533203125,
      "grad_norm": 5.4210405349731445,
      "learning_rate": 2.445068359375e-05,
      "loss": 2.1722,
      "step": 6280
    },
    {
      "epoch": 1.53564453125,
      "grad_norm": 5.171753406524658,
      "learning_rate": 2.4409993489583335e-05,
      "loss": 2.2105,
      "step": 6290
    },
    {
      "epoch": 1.5380859375,
      "grad_norm": 4.922580718994141,
      "learning_rate": 2.4369303385416666e-05,
      "loss": 2.1471,
      "step": 6300
    },
    {
      "epoch": 1.54052734375,
      "grad_norm": 6.257869720458984,
      "learning_rate": 2.432861328125e-05,
      "loss": 2.3112,
      "step": 6310
    },
    {
      "epoch": 1.54296875,
      "grad_norm": 5.349978446960449,
      "learning_rate": 2.4287923177083335e-05,
      "loss": 2.0738,
      "step": 6320
    },
    {
      "epoch": 1.54541015625,
      "grad_norm": 6.208193778991699,
      "learning_rate": 2.4247233072916666e-05,
      "loss": 1.941,
      "step": 6330
    },
    {
      "epoch": 1.5478515625,
      "grad_norm": 5.544953346252441,
      "learning_rate": 2.420654296875e-05,
      "loss": 2.1304,
      "step": 6340
    },
    {
      "epoch": 1.55029296875,
      "grad_norm": 8.743782997131348,
      "learning_rate": 2.4165852864583335e-05,
      "loss": 2.1527,
      "step": 6350
    },
    {
      "epoch": 1.552734375,
      "grad_norm": 7.2964582443237305,
      "learning_rate": 2.4125162760416666e-05,
      "loss": 1.9503,
      "step": 6360
    },
    {
      "epoch": 1.55517578125,
      "grad_norm": 4.389743804931641,
      "learning_rate": 2.408447265625e-05,
      "loss": 1.8293,
      "step": 6370
    },
    {
      "epoch": 1.5576171875,
      "grad_norm": 7.674696445465088,
      "learning_rate": 2.4043782552083335e-05,
      "loss": 2.0775,
      "step": 6380
    },
    {
      "epoch": 1.56005859375,
      "grad_norm": 5.953409671783447,
      "learning_rate": 2.4003092447916666e-05,
      "loss": 2.195,
      "step": 6390
    },
    {
      "epoch": 1.5625,
      "grad_norm": 8.097368240356445,
      "learning_rate": 2.396240234375e-05,
      "loss": 2.0089,
      "step": 6400
    },
    {
      "epoch": 1.56494140625,
      "grad_norm": 6.682028293609619,
      "learning_rate": 2.3921712239583335e-05,
      "loss": 2.0526,
      "step": 6410
    },
    {
      "epoch": 1.5673828125,
      "grad_norm": 9.83466911315918,
      "learning_rate": 2.3881022135416666e-05,
      "loss": 2.0531,
      "step": 6420
    },
    {
      "epoch": 1.56982421875,
      "grad_norm": 5.633058071136475,
      "learning_rate": 2.384033203125e-05,
      "loss": 2.252,
      "step": 6430
    },
    {
      "epoch": 1.572265625,
      "grad_norm": 4.690502166748047,
      "learning_rate": 2.3799641927083335e-05,
      "loss": 1.9493,
      "step": 6440
    },
    {
      "epoch": 1.57470703125,
      "grad_norm": 6.484555244445801,
      "learning_rate": 2.3758951822916666e-05,
      "loss": 2.3293,
      "step": 6450
    },
    {
      "epoch": 1.5771484375,
      "grad_norm": 4.533258438110352,
      "learning_rate": 2.371826171875e-05,
      "loss": 2.1745,
      "step": 6460
    },
    {
      "epoch": 1.57958984375,
      "grad_norm": 5.600065231323242,
      "learning_rate": 2.3677571614583334e-05,
      "loss": 2.0456,
      "step": 6470
    },
    {
      "epoch": 1.58203125,
      "grad_norm": 4.703210353851318,
      "learning_rate": 2.3636881510416666e-05,
      "loss": 2.2414,
      "step": 6480
    },
    {
      "epoch": 1.58447265625,
      "grad_norm": 3.881014823913574,
      "learning_rate": 2.359619140625e-05,
      "loss": 1.976,
      "step": 6490
    },
    {
      "epoch": 1.5869140625,
      "grad_norm": 7.186013698577881,
      "learning_rate": 2.3555501302083334e-05,
      "loss": 2.2109,
      "step": 6500
    },
    {
      "epoch": 1.58935546875,
      "grad_norm": 4.267476558685303,
      "learning_rate": 2.3514811197916665e-05,
      "loss": 1.924,
      "step": 6510
    },
    {
      "epoch": 1.591796875,
      "grad_norm": 5.83447790145874,
      "learning_rate": 2.347412109375e-05,
      "loss": 1.9793,
      "step": 6520
    },
    {
      "epoch": 1.59423828125,
      "grad_norm": 6.461164951324463,
      "learning_rate": 2.3433430989583334e-05,
      "loss": 2.2047,
      "step": 6530
    },
    {
      "epoch": 1.5966796875,
      "grad_norm": 5.076656818389893,
      "learning_rate": 2.3392740885416665e-05,
      "loss": 1.9172,
      "step": 6540
    },
    {
      "epoch": 1.59912109375,
      "grad_norm": 6.667396545410156,
      "learning_rate": 2.335205078125e-05,
      "loss": 2.1137,
      "step": 6550
    },
    {
      "epoch": 1.6015625,
      "grad_norm": 3.8271114826202393,
      "learning_rate": 2.3311360677083334e-05,
      "loss": 2.0943,
      "step": 6560
    },
    {
      "epoch": 1.60400390625,
      "grad_norm": 3.6056370735168457,
      "learning_rate": 2.3270670572916665e-05,
      "loss": 2.1003,
      "step": 6570
    },
    {
      "epoch": 1.6064453125,
      "grad_norm": 6.656424045562744,
      "learning_rate": 2.322998046875e-05,
      "loss": 2.2568,
      "step": 6580
    },
    {
      "epoch": 1.60888671875,
      "grad_norm": 4.581127643585205,
      "learning_rate": 2.3189290364583334e-05,
      "loss": 2.0883,
      "step": 6590
    },
    {
      "epoch": 1.611328125,
      "grad_norm": 3.7677719593048096,
      "learning_rate": 2.314860026041667e-05,
      "loss": 1.8978,
      "step": 6600
    },
    {
      "epoch": 1.61376953125,
      "grad_norm": 5.394026756286621,
      "learning_rate": 2.310791015625e-05,
      "loss": 2.2247,
      "step": 6610
    },
    {
      "epoch": 1.6162109375,
      "grad_norm": 3.434581756591797,
      "learning_rate": 2.3067220052083334e-05,
      "loss": 1.903,
      "step": 6620
    },
    {
      "epoch": 1.61865234375,
      "grad_norm": 5.6122612953186035,
      "learning_rate": 2.302652994791667e-05,
      "loss": 1.979,
      "step": 6630
    },
    {
      "epoch": 1.62109375,
      "grad_norm": 6.47907018661499,
      "learning_rate": 2.298583984375e-05,
      "loss": 1.8817,
      "step": 6640
    },
    {
      "epoch": 1.62353515625,
      "grad_norm": 6.3772101402282715,
      "learning_rate": 2.2945149739583334e-05,
      "loss": 2.1233,
      "step": 6650
    },
    {
      "epoch": 1.6259765625,
      "grad_norm": 3.648397445678711,
      "learning_rate": 2.2904459635416668e-05,
      "loss": 2.0385,
      "step": 6660
    },
    {
      "epoch": 1.62841796875,
      "grad_norm": 7.137006759643555,
      "learning_rate": 2.2863769531250003e-05,
      "loss": 2.0179,
      "step": 6670
    },
    {
      "epoch": 1.630859375,
      "grad_norm": 6.802050590515137,
      "learning_rate": 2.2823079427083334e-05,
      "loss": 2.0265,
      "step": 6680
    },
    {
      "epoch": 1.63330078125,
      "grad_norm": 6.560830116271973,
      "learning_rate": 2.2782389322916668e-05,
      "loss": 2.0189,
      "step": 6690
    },
    {
      "epoch": 1.6357421875,
      "grad_norm": 4.771176815032959,
      "learning_rate": 2.2741699218750003e-05,
      "loss": 2.0362,
      "step": 6700
    },
    {
      "epoch": 1.63818359375,
      "grad_norm": 7.53814172744751,
      "learning_rate": 2.2701009114583337e-05,
      "loss": 1.9723,
      "step": 6710
    },
    {
      "epoch": 1.640625,
      "grad_norm": 7.802910804748535,
      "learning_rate": 2.2660319010416668e-05,
      "loss": 2.2101,
      "step": 6720
    },
    {
      "epoch": 1.64306640625,
      "grad_norm": 6.50121545791626,
      "learning_rate": 2.2619628906250002e-05,
      "loss": 1.9496,
      "step": 6730
    },
    {
      "epoch": 1.6455078125,
      "grad_norm": 7.180087089538574,
      "learning_rate": 2.2578938802083337e-05,
      "loss": 1.9742,
      "step": 6740
    },
    {
      "epoch": 1.64794921875,
      "grad_norm": 6.072566032409668,
      "learning_rate": 2.2538248697916668e-05,
      "loss": 1.9982,
      "step": 6750
    },
    {
      "epoch": 1.650390625,
      "grad_norm": 5.8263092041015625,
      "learning_rate": 2.2497558593750002e-05,
      "loss": 1.9654,
      "step": 6760
    },
    {
      "epoch": 1.65283203125,
      "grad_norm": 6.225966453552246,
      "learning_rate": 2.2456868489583337e-05,
      "loss": 2.0514,
      "step": 6770
    },
    {
      "epoch": 1.6552734375,
      "grad_norm": 4.285000801086426,
      "learning_rate": 2.2416178385416668e-05,
      "loss": 2.0791,
      "step": 6780
    },
    {
      "epoch": 1.65771484375,
      "grad_norm": 3.777883768081665,
      "learning_rate": 2.2375488281250002e-05,
      "loss": 2.1564,
      "step": 6790
    },
    {
      "epoch": 1.66015625,
      "grad_norm": 4.34833288192749,
      "learning_rate": 2.2334798177083337e-05,
      "loss": 2.0933,
      "step": 6800
    },
    {
      "epoch": 1.66259765625,
      "grad_norm": 4.502148628234863,
      "learning_rate": 2.2294108072916668e-05,
      "loss": 1.9291,
      "step": 6810
    },
    {
      "epoch": 1.6650390625,
      "grad_norm": 4.580027103424072,
      "learning_rate": 2.2253417968750002e-05,
      "loss": 2.0582,
      "step": 6820
    },
    {
      "epoch": 1.66748046875,
      "grad_norm": 3.4991395473480225,
      "learning_rate": 2.2212727864583337e-05,
      "loss": 2.1312,
      "step": 6830
    },
    {
      "epoch": 1.669921875,
      "grad_norm": 4.890451431274414,
      "learning_rate": 2.2172037760416668e-05,
      "loss": 1.9823,
      "step": 6840
    },
    {
      "epoch": 1.67236328125,
      "grad_norm": 3.5832552909851074,
      "learning_rate": 2.2131347656250002e-05,
      "loss": 1.9289,
      "step": 6850
    },
    {
      "epoch": 1.6748046875,
      "grad_norm": 4.043046951293945,
      "learning_rate": 2.2090657552083336e-05,
      "loss": 1.9996,
      "step": 6860
    },
    {
      "epoch": 1.67724609375,
      "grad_norm": 6.189732074737549,
      "learning_rate": 2.2049967447916668e-05,
      "loss": 2.1387,
      "step": 6870
    },
    {
      "epoch": 1.6796875,
      "grad_norm": 4.342010021209717,
      "learning_rate": 2.2009277343750002e-05,
      "loss": 2.1095,
      "step": 6880
    },
    {
      "epoch": 1.68212890625,
      "grad_norm": 4.5410308837890625,
      "learning_rate": 2.1968587239583336e-05,
      "loss": 2.0754,
      "step": 6890
    },
    {
      "epoch": 1.6845703125,
      "grad_norm": 5.061360836029053,
      "learning_rate": 2.1927897135416667e-05,
      "loss": 2.1253,
      "step": 6900
    },
    {
      "epoch": 1.68701171875,
      "grad_norm": 4.9340081214904785,
      "learning_rate": 2.1887207031250002e-05,
      "loss": 2.0272,
      "step": 6910
    },
    {
      "epoch": 1.689453125,
      "grad_norm": 4.077362060546875,
      "learning_rate": 2.1846516927083336e-05,
      "loss": 2.0452,
      "step": 6920
    },
    {
      "epoch": 1.69189453125,
      "grad_norm": 5.000301837921143,
      "learning_rate": 2.1805826822916667e-05,
      "loss": 1.9909,
      "step": 6930
    },
    {
      "epoch": 1.6943359375,
      "grad_norm": 3.8422951698303223,
      "learning_rate": 2.1765136718750002e-05,
      "loss": 2.0885,
      "step": 6940
    },
    {
      "epoch": 1.69677734375,
      "grad_norm": 3.5750017166137695,
      "learning_rate": 2.1724446614583336e-05,
      "loss": 1.9296,
      "step": 6950
    },
    {
      "epoch": 1.69921875,
      "grad_norm": 6.018822193145752,
      "learning_rate": 2.1683756510416667e-05,
      "loss": 1.9721,
      "step": 6960
    },
    {
      "epoch": 1.70166015625,
      "grad_norm": 7.915321350097656,
      "learning_rate": 2.164306640625e-05,
      "loss": 2.1065,
      "step": 6970
    },
    {
      "epoch": 1.7041015625,
      "grad_norm": 4.307515621185303,
      "learning_rate": 2.1602376302083336e-05,
      "loss": 2.0691,
      "step": 6980
    },
    {
      "epoch": 1.70654296875,
      "grad_norm": 6.74621057510376,
      "learning_rate": 2.1561686197916667e-05,
      "loss": 2.1379,
      "step": 6990
    },
    {
      "epoch": 1.708984375,
      "grad_norm": 5.8570475578308105,
      "learning_rate": 2.152099609375e-05,
      "loss": 2.1204,
      "step": 7000
    },
    {
      "epoch": 1.71142578125,
      "grad_norm": 5.3307576179504395,
      "learning_rate": 2.1480305989583336e-05,
      "loss": 1.9717,
      "step": 7010
    },
    {
      "epoch": 1.7138671875,
      "grad_norm": 4.548843860626221,
      "learning_rate": 2.1439615885416667e-05,
      "loss": 2.0517,
      "step": 7020
    },
    {
      "epoch": 1.71630859375,
      "grad_norm": 6.589061260223389,
      "learning_rate": 2.139892578125e-05,
      "loss": 1.9209,
      "step": 7030
    },
    {
      "epoch": 1.71875,
      "grad_norm": 4.851429462432861,
      "learning_rate": 2.1358235677083336e-05,
      "loss": 2.0046,
      "step": 7040
    },
    {
      "epoch": 1.72119140625,
      "grad_norm": 4.748821258544922,
      "learning_rate": 2.1317545572916667e-05,
      "loss": 1.9166,
      "step": 7050
    },
    {
      "epoch": 1.7236328125,
      "grad_norm": 4.810458183288574,
      "learning_rate": 2.127685546875e-05,
      "loss": 1.9974,
      "step": 7060
    },
    {
      "epoch": 1.72607421875,
      "grad_norm": 3.715740442276001,
      "learning_rate": 2.1236165364583336e-05,
      "loss": 1.9719,
      "step": 7070
    },
    {
      "epoch": 1.728515625,
      "grad_norm": 9.37076187133789,
      "learning_rate": 2.1195475260416667e-05,
      "loss": 2.1228,
      "step": 7080
    },
    {
      "epoch": 1.73095703125,
      "grad_norm": 5.261280059814453,
      "learning_rate": 2.115478515625e-05,
      "loss": 2.4811,
      "step": 7090
    },
    {
      "epoch": 1.7333984375,
      "grad_norm": 3.8586835861206055,
      "learning_rate": 2.1114095052083336e-05,
      "loss": 1.9196,
      "step": 7100
    },
    {
      "epoch": 1.73583984375,
      "grad_norm": 4.633889675140381,
      "learning_rate": 2.1073404947916667e-05,
      "loss": 2.1202,
      "step": 7110
    },
    {
      "epoch": 1.73828125,
      "grad_norm": 3.702242136001587,
      "learning_rate": 2.103271484375e-05,
      "loss": 1.9301,
      "step": 7120
    },
    {
      "epoch": 1.74072265625,
      "grad_norm": 4.695330619812012,
      "learning_rate": 2.0992024739583335e-05,
      "loss": 1.8995,
      "step": 7130
    },
    {
      "epoch": 1.7431640625,
      "grad_norm": 5.300631046295166,
      "learning_rate": 2.0951334635416667e-05,
      "loss": 2.1403,
      "step": 7140
    },
    {
      "epoch": 1.74560546875,
      "grad_norm": 4.976154804229736,
      "learning_rate": 2.091064453125e-05,
      "loss": 1.9788,
      "step": 7150
    },
    {
      "epoch": 1.748046875,
      "grad_norm": 7.255505084991455,
      "learning_rate": 2.0869954427083335e-05,
      "loss": 2.0937,
      "step": 7160
    },
    {
      "epoch": 1.75048828125,
      "grad_norm": 5.360969066619873,
      "learning_rate": 2.0829264322916666e-05,
      "loss": 1.9593,
      "step": 7170
    },
    {
      "epoch": 1.7529296875,
      "grad_norm": 5.486021995544434,
      "learning_rate": 2.078857421875e-05,
      "loss": 2.0487,
      "step": 7180
    },
    {
      "epoch": 1.75537109375,
      "grad_norm": 4.446864604949951,
      "learning_rate": 2.0747884114583335e-05,
      "loss": 2.0385,
      "step": 7190
    },
    {
      "epoch": 1.7578125,
      "grad_norm": 3.810530424118042,
      "learning_rate": 2.0707194010416666e-05,
      "loss": 2.0774,
      "step": 7200
    },
    {
      "epoch": 1.76025390625,
      "grad_norm": 5.065349102020264,
      "learning_rate": 2.066650390625e-05,
      "loss": 2.0711,
      "step": 7210
    },
    {
      "epoch": 1.7626953125,
      "grad_norm": 4.216716289520264,
      "learning_rate": 2.0625813802083335e-05,
      "loss": 2.1378,
      "step": 7220
    },
    {
      "epoch": 1.76513671875,
      "grad_norm": 6.655120849609375,
      "learning_rate": 2.0585123697916666e-05,
      "loss": 1.9339,
      "step": 7230
    },
    {
      "epoch": 1.767578125,
      "grad_norm": 4.800547122955322,
      "learning_rate": 2.054443359375e-05,
      "loss": 2.1914,
      "step": 7240
    },
    {
      "epoch": 1.77001953125,
      "grad_norm": 4.3525309562683105,
      "learning_rate": 2.0503743489583335e-05,
      "loss": 1.7815,
      "step": 7250
    },
    {
      "epoch": 1.7724609375,
      "grad_norm": 4.406849384307861,
      "learning_rate": 2.0463053385416666e-05,
      "loss": 1.9593,
      "step": 7260
    },
    {
      "epoch": 1.77490234375,
      "grad_norm": 4.967536449432373,
      "learning_rate": 2.042236328125e-05,
      "loss": 2.1513,
      "step": 7270
    },
    {
      "epoch": 1.77734375,
      "grad_norm": 5.835119724273682,
      "learning_rate": 2.0381673177083335e-05,
      "loss": 1.9882,
      "step": 7280
    },
    {
      "epoch": 1.77978515625,
      "grad_norm": 7.470624923706055,
      "learning_rate": 2.0340983072916666e-05,
      "loss": 2.0091,
      "step": 7290
    },
    {
      "epoch": 1.7822265625,
      "grad_norm": 5.719996452331543,
      "learning_rate": 2.030029296875e-05,
      "loss": 2.2716,
      "step": 7300
    },
    {
      "epoch": 1.78466796875,
      "grad_norm": 6.320882320404053,
      "learning_rate": 2.0259602864583335e-05,
      "loss": 2.2032,
      "step": 7310
    },
    {
      "epoch": 1.787109375,
      "grad_norm": 5.006093502044678,
      "learning_rate": 2.0218912760416666e-05,
      "loss": 2.0356,
      "step": 7320
    },
    {
      "epoch": 1.78955078125,
      "grad_norm": 4.445049285888672,
      "learning_rate": 2.017822265625e-05,
      "loss": 1.9626,
      "step": 7330
    },
    {
      "epoch": 1.7919921875,
      "grad_norm": 4.666658878326416,
      "learning_rate": 2.0137532552083335e-05,
      "loss": 2.0283,
      "step": 7340
    },
    {
      "epoch": 1.79443359375,
      "grad_norm": 5.134759902954102,
      "learning_rate": 2.0096842447916666e-05,
      "loss": 2.0848,
      "step": 7350
    },
    {
      "epoch": 1.796875,
      "grad_norm": 6.489743232727051,
      "learning_rate": 2.005615234375e-05,
      "loss": 1.9623,
      "step": 7360
    },
    {
      "epoch": 1.79931640625,
      "grad_norm": 6.2345967292785645,
      "learning_rate": 2.0015462239583335e-05,
      "loss": 1.9842,
      "step": 7370
    },
    {
      "epoch": 1.8017578125,
      "grad_norm": 5.1753082275390625,
      "learning_rate": 1.9974772135416666e-05,
      "loss": 2.0784,
      "step": 7380
    },
    {
      "epoch": 1.80419921875,
      "grad_norm": 2.9794275760650635,
      "learning_rate": 1.993408203125e-05,
      "loss": 2.0482,
      "step": 7390
    },
    {
      "epoch": 1.806640625,
      "grad_norm": 4.081007957458496,
      "learning_rate": 1.9893391927083335e-05,
      "loss": 1.883,
      "step": 7400
    },
    {
      "epoch": 1.80908203125,
      "grad_norm": 6.331088542938232,
      "learning_rate": 1.9852701822916666e-05,
      "loss": 2.0982,
      "step": 7410
    },
    {
      "epoch": 1.8115234375,
      "grad_norm": 5.402431488037109,
      "learning_rate": 1.981201171875e-05,
      "loss": 2.341,
      "step": 7420
    },
    {
      "epoch": 1.81396484375,
      "grad_norm": 6.107371807098389,
      "learning_rate": 1.9771321614583334e-05,
      "loss": 2.0303,
      "step": 7430
    },
    {
      "epoch": 1.81640625,
      "grad_norm": 4.527138710021973,
      "learning_rate": 1.9730631510416665e-05,
      "loss": 1.9879,
      "step": 7440
    },
    {
      "epoch": 1.81884765625,
      "grad_norm": 9.187224388122559,
      "learning_rate": 1.968994140625e-05,
      "loss": 2.0962,
      "step": 7450
    },
    {
      "epoch": 1.8212890625,
      "grad_norm": 5.693246364593506,
      "learning_rate": 1.9649251302083334e-05,
      "loss": 1.9089,
      "step": 7460
    },
    {
      "epoch": 1.82373046875,
      "grad_norm": 3.9885101318359375,
      "learning_rate": 1.9608561197916665e-05,
      "loss": 2.0444,
      "step": 7470
    },
    {
      "epoch": 1.826171875,
      "grad_norm": 4.017028331756592,
      "learning_rate": 1.956787109375e-05,
      "loss": 2.009,
      "step": 7480
    },
    {
      "epoch": 1.82861328125,
      "grad_norm": 6.372330188751221,
      "learning_rate": 1.9527180989583334e-05,
      "loss": 2.1022,
      "step": 7490
    },
    {
      "epoch": 1.8310546875,
      "grad_norm": 5.781981468200684,
      "learning_rate": 1.9486490885416665e-05,
      "loss": 2.2344,
      "step": 7500
    },
    {
      "epoch": 1.83349609375,
      "grad_norm": 5.639463901519775,
      "learning_rate": 1.944580078125e-05,
      "loss": 2.1808,
      "step": 7510
    },
    {
      "epoch": 1.8359375,
      "grad_norm": 4.39271879196167,
      "learning_rate": 1.9405110677083334e-05,
      "loss": 1.849,
      "step": 7520
    },
    {
      "epoch": 1.83837890625,
      "grad_norm": 3.8359320163726807,
      "learning_rate": 1.9364420572916665e-05,
      "loss": 2.0339,
      "step": 7530
    },
    {
      "epoch": 1.8408203125,
      "grad_norm": 4.532501220703125,
      "learning_rate": 1.932373046875e-05,
      "loss": 1.9218,
      "step": 7540
    },
    {
      "epoch": 1.84326171875,
      "grad_norm": 7.7643723487854,
      "learning_rate": 1.9283040364583334e-05,
      "loss": 2.023,
      "step": 7550
    },
    {
      "epoch": 1.845703125,
      "grad_norm": 7.047404766082764,
      "learning_rate": 1.9242350260416665e-05,
      "loss": 2.1941,
      "step": 7560
    },
    {
      "epoch": 1.84814453125,
      "grad_norm": 5.261781692504883,
      "learning_rate": 1.920166015625e-05,
      "loss": 1.9164,
      "step": 7570
    },
    {
      "epoch": 1.8505859375,
      "grad_norm": 5.649815559387207,
      "learning_rate": 1.9160970052083334e-05,
      "loss": 2.0557,
      "step": 7580
    },
    {
      "epoch": 1.85302734375,
      "grad_norm": 5.264492034912109,
      "learning_rate": 1.9120279947916668e-05,
      "loss": 2.0473,
      "step": 7590
    },
    {
      "epoch": 1.85546875,
      "grad_norm": 5.277257442474365,
      "learning_rate": 1.907958984375e-05,
      "loss": 2.1156,
      "step": 7600
    },
    {
      "epoch": 1.85791015625,
      "grad_norm": 4.126400947570801,
      "learning_rate": 1.9038899739583334e-05,
      "loss": 2.1603,
      "step": 7610
    },
    {
      "epoch": 1.8603515625,
      "grad_norm": 5.599890232086182,
      "learning_rate": 1.8998209635416668e-05,
      "loss": 2.0984,
      "step": 7620
    },
    {
      "epoch": 1.86279296875,
      "grad_norm": 8.427067756652832,
      "learning_rate": 1.895751953125e-05,
      "loss": 2.0263,
      "step": 7630
    },
    {
      "epoch": 1.865234375,
      "grad_norm": 6.238686561584473,
      "learning_rate": 1.8916829427083334e-05,
      "loss": 1.9928,
      "step": 7640
    },
    {
      "epoch": 1.86767578125,
      "grad_norm": 6.782978057861328,
      "learning_rate": 1.8876139322916668e-05,
      "loss": 2.3031,
      "step": 7650
    },
    {
      "epoch": 1.8701171875,
      "grad_norm": 5.184013843536377,
      "learning_rate": 1.8835449218750002e-05,
      "loss": 2.0906,
      "step": 7660
    },
    {
      "epoch": 1.87255859375,
      "grad_norm": 4.796077728271484,
      "learning_rate": 1.8794759114583334e-05,
      "loss": 2.1425,
      "step": 7670
    },
    {
      "epoch": 1.875,
      "grad_norm": 4.625560283660889,
      "learning_rate": 1.8754069010416668e-05,
      "loss": 2.1621,
      "step": 7680
    },
    {
      "epoch": 1.87744140625,
      "grad_norm": 6.404147624969482,
      "learning_rate": 1.8713378906250002e-05,
      "loss": 2.0543,
      "step": 7690
    },
    {
      "epoch": 1.8798828125,
      "grad_norm": 4.970512390136719,
      "learning_rate": 1.8672688802083337e-05,
      "loss": 1.994,
      "step": 7700
    },
    {
      "epoch": 1.88232421875,
      "grad_norm": 3.98443865776062,
      "learning_rate": 1.8631998697916668e-05,
      "loss": 2.1084,
      "step": 7710
    },
    {
      "epoch": 1.884765625,
      "grad_norm": 5.874026775360107,
      "learning_rate": 1.8591308593750002e-05,
      "loss": 2.1433,
      "step": 7720
    },
    {
      "epoch": 1.88720703125,
      "grad_norm": 5.466376304626465,
      "learning_rate": 1.8550618489583337e-05,
      "loss": 2.2018,
      "step": 7730
    },
    {
      "epoch": 1.8896484375,
      "grad_norm": 4.35684061050415,
      "learning_rate": 1.8509928385416668e-05,
      "loss": 2.1174,
      "step": 7740
    },
    {
      "epoch": 1.89208984375,
      "grad_norm": 3.878108263015747,
      "learning_rate": 1.8469238281250002e-05,
      "loss": 2.0112,
      "step": 7750
    },
    {
      "epoch": 1.89453125,
      "grad_norm": 5.020873546600342,
      "learning_rate": 1.8428548177083337e-05,
      "loss": 2.041,
      "step": 7760
    },
    {
      "epoch": 1.89697265625,
      "grad_norm": 3.7292683124542236,
      "learning_rate": 1.8387858072916668e-05,
      "loss": 2.1535,
      "step": 7770
    },
    {
      "epoch": 1.8994140625,
      "grad_norm": 5.8533453941345215,
      "learning_rate": 1.8347167968750002e-05,
      "loss": 1.9428,
      "step": 7780
    },
    {
      "epoch": 1.90185546875,
      "grad_norm": 8.211009979248047,
      "learning_rate": 1.8306477864583336e-05,
      "loss": 1.9623,
      "step": 7790
    },
    {
      "epoch": 1.904296875,
      "grad_norm": 7.767279624938965,
      "learning_rate": 1.8265787760416668e-05,
      "loss": 1.9732,
      "step": 7800
    },
    {
      "epoch": 1.90673828125,
      "grad_norm": 6.205027103424072,
      "learning_rate": 1.8225097656250002e-05,
      "loss": 2.0645,
      "step": 7810
    },
    {
      "epoch": 1.9091796875,
      "grad_norm": 4.466982364654541,
      "learning_rate": 1.8184407552083336e-05,
      "loss": 2.0707,
      "step": 7820
    },
    {
      "epoch": 1.91162109375,
      "grad_norm": 5.3191304206848145,
      "learning_rate": 1.8143717447916667e-05,
      "loss": 1.8857,
      "step": 7830
    },
    {
      "epoch": 1.9140625,
      "grad_norm": 5.962928771972656,
      "learning_rate": 1.8103027343750002e-05,
      "loss": 2.1891,
      "step": 7840
    },
    {
      "epoch": 1.91650390625,
      "grad_norm": 4.553837776184082,
      "learning_rate": 1.8062337239583336e-05,
      "loss": 1.8999,
      "step": 7850
    },
    {
      "epoch": 1.9189453125,
      "grad_norm": 3.1221694946289062,
      "learning_rate": 1.8021647135416667e-05,
      "loss": 2.0749,
      "step": 7860
    },
    {
      "epoch": 1.92138671875,
      "grad_norm": 6.867712497711182,
      "learning_rate": 1.7980957031250002e-05,
      "loss": 2.0858,
      "step": 7870
    },
    {
      "epoch": 1.923828125,
      "grad_norm": 3.9793050289154053,
      "learning_rate": 1.7940266927083336e-05,
      "loss": 2.1302,
      "step": 7880
    },
    {
      "epoch": 1.92626953125,
      "grad_norm": 6.078869819641113,
      "learning_rate": 1.7899576822916667e-05,
      "loss": 1.8833,
      "step": 7890
    },
    {
      "epoch": 1.9287109375,
      "grad_norm": 3.948713779449463,
      "learning_rate": 1.785888671875e-05,
      "loss": 1.8831,
      "step": 7900
    },
    {
      "epoch": 1.93115234375,
      "grad_norm": 5.958511829376221,
      "learning_rate": 1.7818196614583336e-05,
      "loss": 1.8414,
      "step": 7910
    },
    {
      "epoch": 1.93359375,
      "grad_norm": 5.874477386474609,
      "learning_rate": 1.7777506510416667e-05,
      "loss": 2.0161,
      "step": 7920
    },
    {
      "epoch": 1.93603515625,
      "grad_norm": 4.177259922027588,
      "learning_rate": 1.773681640625e-05,
      "loss": 1.9658,
      "step": 7930
    },
    {
      "epoch": 1.9384765625,
      "grad_norm": 6.6858906745910645,
      "learning_rate": 1.7696126302083336e-05,
      "loss": 2.0911,
      "step": 7940
    },
    {
      "epoch": 1.94091796875,
      "grad_norm": 4.65249490737915,
      "learning_rate": 1.7655436197916667e-05,
      "loss": 2.0063,
      "step": 7950
    },
    {
      "epoch": 1.943359375,
      "grad_norm": 5.41144323348999,
      "learning_rate": 1.761474609375e-05,
      "loss": 1.995,
      "step": 7960
    },
    {
      "epoch": 1.94580078125,
      "grad_norm": 5.102119445800781,
      "learning_rate": 1.7574055989583336e-05,
      "loss": 1.93,
      "step": 7970
    },
    {
      "epoch": 1.9482421875,
      "grad_norm": 5.336393356323242,
      "learning_rate": 1.7533365885416667e-05,
      "loss": 1.9722,
      "step": 7980
    },
    {
      "epoch": 1.95068359375,
      "grad_norm": 6.155048370361328,
      "learning_rate": 1.749267578125e-05,
      "loss": 2.0654,
      "step": 7990
    },
    {
      "epoch": 1.953125,
      "grad_norm": 5.891607761383057,
      "learning_rate": 1.7451985677083336e-05,
      "loss": 2.0826,
      "step": 8000
    },
    {
      "epoch": 1.95556640625,
      "grad_norm": 4.054538726806641,
      "learning_rate": 1.7411295572916667e-05,
      "loss": 1.9947,
      "step": 8010
    },
    {
      "epoch": 1.9580078125,
      "grad_norm": 6.280575752258301,
      "learning_rate": 1.737060546875e-05,
      "loss": 2.0529,
      "step": 8020
    },
    {
      "epoch": 1.96044921875,
      "grad_norm": 3.993140459060669,
      "learning_rate": 1.7329915364583336e-05,
      "loss": 1.7922,
      "step": 8030
    },
    {
      "epoch": 1.962890625,
      "grad_norm": 5.473353862762451,
      "learning_rate": 1.7289225260416667e-05,
      "loss": 2.0735,
      "step": 8040
    },
    {
      "epoch": 1.96533203125,
      "grad_norm": 5.370128631591797,
      "learning_rate": 1.724853515625e-05,
      "loss": 2.0798,
      "step": 8050
    },
    {
      "epoch": 1.9677734375,
      "grad_norm": 4.209381580352783,
      "learning_rate": 1.7207845052083336e-05,
      "loss": 1.9353,
      "step": 8060
    },
    {
      "epoch": 1.97021484375,
      "grad_norm": 6.902242183685303,
      "learning_rate": 1.7167154947916667e-05,
      "loss": 2.0479,
      "step": 8070
    },
    {
      "epoch": 1.97265625,
      "grad_norm": 4.113428592681885,
      "learning_rate": 1.712646484375e-05,
      "loss": 1.9548,
      "step": 8080
    },
    {
      "epoch": 1.97509765625,
      "grad_norm": 5.8362321853637695,
      "learning_rate": 1.7085774739583335e-05,
      "loss": 1.9552,
      "step": 8090
    },
    {
      "epoch": 1.9775390625,
      "grad_norm": 7.754044532775879,
      "learning_rate": 1.7045084635416666e-05,
      "loss": 2.038,
      "step": 8100
    },
    {
      "epoch": 1.97998046875,
      "grad_norm": 5.820178985595703,
      "learning_rate": 1.700439453125e-05,
      "loss": 1.914,
      "step": 8110
    },
    {
      "epoch": 1.982421875,
      "grad_norm": 3.6089539527893066,
      "learning_rate": 1.6963704427083335e-05,
      "loss": 2.1634,
      "step": 8120
    },
    {
      "epoch": 1.98486328125,
      "grad_norm": 5.260838031768799,
      "learning_rate": 1.6923014322916666e-05,
      "loss": 1.8847,
      "step": 8130
    },
    {
      "epoch": 1.9873046875,
      "grad_norm": 5.193350315093994,
      "learning_rate": 1.688232421875e-05,
      "loss": 1.9009,
      "step": 8140
    },
    {
      "epoch": 1.98974609375,
      "grad_norm": 5.549376010894775,
      "learning_rate": 1.6841634114583335e-05,
      "loss": 1.8597,
      "step": 8150
    },
    {
      "epoch": 1.9921875,
      "grad_norm": 4.73309326171875,
      "learning_rate": 1.6800944010416666e-05,
      "loss": 1.9843,
      "step": 8160
    },
    {
      "epoch": 1.99462890625,
      "grad_norm": 6.067076206207275,
      "learning_rate": 1.676025390625e-05,
      "loss": 2.1255,
      "step": 8170
    },
    {
      "epoch": 1.9970703125,
      "grad_norm": 4.795539379119873,
      "learning_rate": 1.6719563802083335e-05,
      "loss": 1.8875,
      "step": 8180
    },
    {
      "epoch": 1.99951171875,
      "grad_norm": 4.851809978485107,
      "learning_rate": 1.6678873697916666e-05,
      "loss": 1.9521,
      "step": 8190
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.8188118934631348,
      "eval_runtime": 6.0936,
      "eval_samples_per_second": 21.006,
      "eval_steps_per_second": 21.006,
      "step": 8192
    },
    {
      "epoch": 2.001953125,
      "grad_norm": 4.744487285614014,
      "learning_rate": 1.663818359375e-05,
      "loss": 1.9052,
      "step": 8200
    },
    {
      "epoch": 2.00439453125,
      "grad_norm": 5.029151916503906,
      "learning_rate": 1.6597493489583335e-05,
      "loss": 1.8952,
      "step": 8210
    },
    {
      "epoch": 2.0068359375,
      "grad_norm": 8.462079048156738,
      "learning_rate": 1.6556803385416666e-05,
      "loss": 2.0463,
      "step": 8220
    },
    {
      "epoch": 2.00927734375,
      "grad_norm": 6.088096618652344,
      "learning_rate": 1.651611328125e-05,
      "loss": 2.031,
      "step": 8230
    },
    {
      "epoch": 2.01171875,
      "grad_norm": 7.437417984008789,
      "learning_rate": 1.6475423177083335e-05,
      "loss": 2.0169,
      "step": 8240
    },
    {
      "epoch": 2.01416015625,
      "grad_norm": 6.106995105743408,
      "learning_rate": 1.6434733072916666e-05,
      "loss": 1.8996,
      "step": 8250
    },
    {
      "epoch": 2.0166015625,
      "grad_norm": 3.3534581661224365,
      "learning_rate": 1.639404296875e-05,
      "loss": 2.1081,
      "step": 8260
    },
    {
      "epoch": 2.01904296875,
      "grad_norm": 3.9065117835998535,
      "learning_rate": 1.6353352864583335e-05,
      "loss": 2.0079,
      "step": 8270
    },
    {
      "epoch": 2.021484375,
      "grad_norm": 4.371288299560547,
      "learning_rate": 1.6312662760416666e-05,
      "loss": 2.039,
      "step": 8280
    },
    {
      "epoch": 2.02392578125,
      "grad_norm": 5.164384365081787,
      "learning_rate": 1.627197265625e-05,
      "loss": 2.016,
      "step": 8290
    },
    {
      "epoch": 2.0263671875,
      "grad_norm": 4.032451152801514,
      "learning_rate": 1.6231282552083335e-05,
      "loss": 1.9281,
      "step": 8300
    },
    {
      "epoch": 2.02880859375,
      "grad_norm": 3.7060129642486572,
      "learning_rate": 1.6190592447916666e-05,
      "loss": 1.803,
      "step": 8310
    },
    {
      "epoch": 2.03125,
      "grad_norm": 5.879978179931641,
      "learning_rate": 1.614990234375e-05,
      "loss": 2.0526,
      "step": 8320
    },
    {
      "epoch": 2.03369140625,
      "grad_norm": 3.881561040878296,
      "learning_rate": 1.6109212239583335e-05,
      "loss": 2.04,
      "step": 8330
    },
    {
      "epoch": 2.0361328125,
      "grad_norm": 7.512825012207031,
      "learning_rate": 1.6068522135416666e-05,
      "loss": 1.8953,
      "step": 8340
    },
    {
      "epoch": 2.03857421875,
      "grad_norm": 4.7909111976623535,
      "learning_rate": 1.602783203125e-05,
      "loss": 1.8805,
      "step": 8350
    },
    {
      "epoch": 2.041015625,
      "grad_norm": 5.8690972328186035,
      "learning_rate": 1.5987141927083334e-05,
      "loss": 2.018,
      "step": 8360
    },
    {
      "epoch": 2.04345703125,
      "grad_norm": 3.8585169315338135,
      "learning_rate": 1.5946451822916665e-05,
      "loss": 2.0596,
      "step": 8370
    },
    {
      "epoch": 2.0458984375,
      "grad_norm": 3.7488455772399902,
      "learning_rate": 1.590576171875e-05,
      "loss": 1.9203,
      "step": 8380
    },
    {
      "epoch": 2.04833984375,
      "grad_norm": 6.867895603179932,
      "learning_rate": 1.5865071614583334e-05,
      "loss": 2.1447,
      "step": 8390
    },
    {
      "epoch": 2.05078125,
      "grad_norm": 4.081091403961182,
      "learning_rate": 1.5824381510416665e-05,
      "loss": 1.916,
      "step": 8400
    },
    {
      "epoch": 2.05322265625,
      "grad_norm": 6.147821426391602,
      "learning_rate": 1.578369140625e-05,
      "loss": 1.9531,
      "step": 8410
    },
    {
      "epoch": 2.0556640625,
      "grad_norm": 4.1247711181640625,
      "learning_rate": 1.5743001302083334e-05,
      "loss": 1.9928,
      "step": 8420
    },
    {
      "epoch": 2.05810546875,
      "grad_norm": 6.0708794593811035,
      "learning_rate": 1.5702311197916665e-05,
      "loss": 1.984,
      "step": 8430
    },
    {
      "epoch": 2.060546875,
      "grad_norm": 3.7966322898864746,
      "learning_rate": 1.566162109375e-05,
      "loss": 2.0386,
      "step": 8440
    },
    {
      "epoch": 2.06298828125,
      "grad_norm": 6.21096658706665,
      "learning_rate": 1.5620930989583334e-05,
      "loss": 1.9717,
      "step": 8450
    },
    {
      "epoch": 2.0654296875,
      "grad_norm": 9.656933784484863,
      "learning_rate": 1.5580240885416665e-05,
      "loss": 1.9966,
      "step": 8460
    },
    {
      "epoch": 2.06787109375,
      "grad_norm": 7.376509666442871,
      "learning_rate": 1.553955078125e-05,
      "loss": 2.0976,
      "step": 8470
    },
    {
      "epoch": 2.0703125,
      "grad_norm": 6.128950119018555,
      "learning_rate": 1.5498860677083334e-05,
      "loss": 2.107,
      "step": 8480
    },
    {
      "epoch": 2.07275390625,
      "grad_norm": 5.332475662231445,
      "learning_rate": 1.5458170572916665e-05,
      "loss": 1.8623,
      "step": 8490
    },
    {
      "epoch": 2.0751953125,
      "grad_norm": 6.806912899017334,
      "learning_rate": 1.541748046875e-05,
      "loss": 2.0828,
      "step": 8500
    },
    {
      "epoch": 2.07763671875,
      "grad_norm": 6.40390157699585,
      "learning_rate": 1.5376790364583334e-05,
      "loss": 2.0396,
      "step": 8510
    },
    {
      "epoch": 2.080078125,
      "grad_norm": 4.533731460571289,
      "learning_rate": 1.5336100260416665e-05,
      "loss": 2.1157,
      "step": 8520
    },
    {
      "epoch": 2.08251953125,
      "grad_norm": 3.9940619468688965,
      "learning_rate": 1.529541015625e-05,
      "loss": 1.939,
      "step": 8530
    },
    {
      "epoch": 2.0849609375,
      "grad_norm": 5.129738807678223,
      "learning_rate": 1.5254720052083335e-05,
      "loss": 2.0194,
      "step": 8540
    },
    {
      "epoch": 2.08740234375,
      "grad_norm": 5.047642707824707,
      "learning_rate": 1.5214029947916667e-05,
      "loss": 1.9826,
      "step": 8550
    },
    {
      "epoch": 2.08984375,
      "grad_norm": 7.211142063140869,
      "learning_rate": 1.5173339843750001e-05,
      "loss": 2.0144,
      "step": 8560
    },
    {
      "epoch": 2.09228515625,
      "grad_norm": 4.733123779296875,
      "learning_rate": 1.5132649739583335e-05,
      "loss": 1.8415,
      "step": 8570
    },
    {
      "epoch": 2.0947265625,
      "grad_norm": 3.3651041984558105,
      "learning_rate": 1.5091959635416666e-05,
      "loss": 1.7318,
      "step": 8580
    },
    {
      "epoch": 2.09716796875,
      "grad_norm": 7.107393264770508,
      "learning_rate": 1.505126953125e-05,
      "loss": 1.8722,
      "step": 8590
    },
    {
      "epoch": 2.099609375,
      "grad_norm": 3.6697213649749756,
      "learning_rate": 1.5010579427083335e-05,
      "loss": 2.0419,
      "step": 8600
    },
    {
      "epoch": 2.10205078125,
      "grad_norm": 4.872371673583984,
      "learning_rate": 1.4969889322916666e-05,
      "loss": 1.9311,
      "step": 8610
    },
    {
      "epoch": 2.1044921875,
      "grad_norm": 4.059453964233398,
      "learning_rate": 1.492919921875e-05,
      "loss": 2.0006,
      "step": 8620
    },
    {
      "epoch": 2.10693359375,
      "grad_norm": 7.345384120941162,
      "learning_rate": 1.4888509114583335e-05,
      "loss": 2.1774,
      "step": 8630
    },
    {
      "epoch": 2.109375,
      "grad_norm": 5.665154933929443,
      "learning_rate": 1.4847819010416666e-05,
      "loss": 2.1692,
      "step": 8640
    },
    {
      "epoch": 2.11181640625,
      "grad_norm": 7.9591498374938965,
      "learning_rate": 1.480712890625e-05,
      "loss": 2.0108,
      "step": 8650
    },
    {
      "epoch": 2.1142578125,
      "grad_norm": 5.748767375946045,
      "learning_rate": 1.4766438802083335e-05,
      "loss": 1.9664,
      "step": 8660
    },
    {
      "epoch": 2.11669921875,
      "grad_norm": 5.453625202178955,
      "learning_rate": 1.4725748697916666e-05,
      "loss": 2.192,
      "step": 8670
    },
    {
      "epoch": 2.119140625,
      "grad_norm": 5.163515567779541,
      "learning_rate": 1.468505859375e-05,
      "loss": 2.0229,
      "step": 8680
    },
    {
      "epoch": 2.12158203125,
      "grad_norm": 3.300718307495117,
      "learning_rate": 1.4644368489583335e-05,
      "loss": 2.0567,
      "step": 8690
    },
    {
      "epoch": 2.1240234375,
      "grad_norm": 4.230897903442383,
      "learning_rate": 1.4603678385416666e-05,
      "loss": 1.954,
      "step": 8700
    },
    {
      "epoch": 2.12646484375,
      "grad_norm": 4.415432929992676,
      "learning_rate": 1.456298828125e-05,
      "loss": 1.9732,
      "step": 8710
    },
    {
      "epoch": 2.12890625,
      "grad_norm": 5.121025562286377,
      "learning_rate": 1.4522298177083335e-05,
      "loss": 1.8791,
      "step": 8720
    },
    {
      "epoch": 2.13134765625,
      "grad_norm": 3.997540235519409,
      "learning_rate": 1.4481608072916666e-05,
      "loss": 2.0386,
      "step": 8730
    },
    {
      "epoch": 2.1337890625,
      "grad_norm": 6.232773780822754,
      "learning_rate": 1.444091796875e-05,
      "loss": 1.9159,
      "step": 8740
    },
    {
      "epoch": 2.13623046875,
      "grad_norm": 7.84451961517334,
      "learning_rate": 1.4400227864583335e-05,
      "loss": 2.3604,
      "step": 8750
    },
    {
      "epoch": 2.138671875,
      "grad_norm": 5.439448833465576,
      "learning_rate": 1.4359537760416666e-05,
      "loss": 2.0483,
      "step": 8760
    },
    {
      "epoch": 2.14111328125,
      "grad_norm": 5.644936561584473,
      "learning_rate": 1.431884765625e-05,
      "loss": 2.1474,
      "step": 8770
    },
    {
      "epoch": 2.1435546875,
      "grad_norm": 4.724425315856934,
      "learning_rate": 1.4278157552083335e-05,
      "loss": 1.9067,
      "step": 8780
    },
    {
      "epoch": 2.14599609375,
      "grad_norm": 5.495434761047363,
      "learning_rate": 1.4237467447916666e-05,
      "loss": 2.0276,
      "step": 8790
    },
    {
      "epoch": 2.1484375,
      "grad_norm": 4.459500789642334,
      "learning_rate": 1.419677734375e-05,
      "loss": 1.9889,
      "step": 8800
    },
    {
      "epoch": 2.15087890625,
      "grad_norm": 4.815464019775391,
      "learning_rate": 1.4156087239583334e-05,
      "loss": 2.0981,
      "step": 8810
    },
    {
      "epoch": 2.1533203125,
      "grad_norm": 6.541299819946289,
      "learning_rate": 1.4115397135416667e-05,
      "loss": 1.8904,
      "step": 8820
    },
    {
      "epoch": 2.15576171875,
      "grad_norm": 6.248340129852295,
      "learning_rate": 1.407470703125e-05,
      "loss": 2.14,
      "step": 8830
    },
    {
      "epoch": 2.158203125,
      "grad_norm": 4.049793720245361,
      "learning_rate": 1.4034016927083334e-05,
      "loss": 2.1293,
      "step": 8840
    },
    {
      "epoch": 2.16064453125,
      "grad_norm": 6.2126383781433105,
      "learning_rate": 1.3993326822916667e-05,
      "loss": 1.9795,
      "step": 8850
    },
    {
      "epoch": 2.1630859375,
      "grad_norm": 5.522219657897949,
      "learning_rate": 1.395263671875e-05,
      "loss": 1.9363,
      "step": 8860
    },
    {
      "epoch": 2.16552734375,
      "grad_norm": 6.143759250640869,
      "learning_rate": 1.3911946614583334e-05,
      "loss": 2.0595,
      "step": 8870
    },
    {
      "epoch": 2.16796875,
      "grad_norm": 7.751782417297363,
      "learning_rate": 1.3871256510416667e-05,
      "loss": 1.9097,
      "step": 8880
    },
    {
      "epoch": 2.17041015625,
      "grad_norm": 6.41556453704834,
      "learning_rate": 1.3830566406250001e-05,
      "loss": 2.0474,
      "step": 8890
    },
    {
      "epoch": 2.1728515625,
      "grad_norm": 4.544334411621094,
      "learning_rate": 1.3789876302083334e-05,
      "loss": 1.9893,
      "step": 8900
    },
    {
      "epoch": 2.17529296875,
      "grad_norm": 4.509627342224121,
      "learning_rate": 1.3749186197916667e-05,
      "loss": 2.0344,
      "step": 8910
    },
    {
      "epoch": 2.177734375,
      "grad_norm": 6.487776279449463,
      "learning_rate": 1.3708496093750001e-05,
      "loss": 1.9849,
      "step": 8920
    },
    {
      "epoch": 2.18017578125,
      "grad_norm": 5.930000305175781,
      "learning_rate": 1.3667805989583336e-05,
      "loss": 2.0391,
      "step": 8930
    },
    {
      "epoch": 2.1826171875,
      "grad_norm": 4.990492343902588,
      "learning_rate": 1.3627115885416667e-05,
      "loss": 1.8755,
      "step": 8940
    },
    {
      "epoch": 2.18505859375,
      "grad_norm": 6.8554534912109375,
      "learning_rate": 1.3586425781250001e-05,
      "loss": 2.0852,
      "step": 8950
    },
    {
      "epoch": 2.1875,
      "grad_norm": 6.820478916168213,
      "learning_rate": 1.3545735677083336e-05,
      "loss": 1.8808,
      "step": 8960
    },
    {
      "epoch": 2.18994140625,
      "grad_norm": 4.887932300567627,
      "learning_rate": 1.3505045572916667e-05,
      "loss": 2.0632,
      "step": 8970
    },
    {
      "epoch": 2.1923828125,
      "grad_norm": 6.4691033363342285,
      "learning_rate": 1.3464355468750001e-05,
      "loss": 1.9244,
      "step": 8980
    },
    {
      "epoch": 2.19482421875,
      "grad_norm": 5.291258335113525,
      "learning_rate": 1.3423665364583336e-05,
      "loss": 1.8561,
      "step": 8990
    },
    {
      "epoch": 2.197265625,
      "grad_norm": 5.774433135986328,
      "learning_rate": 1.3382975260416667e-05,
      "loss": 2.1689,
      "step": 9000
    },
    {
      "epoch": 2.19970703125,
      "grad_norm": 6.106510639190674,
      "learning_rate": 1.3342285156250001e-05,
      "loss": 2.0413,
      "step": 9010
    },
    {
      "epoch": 2.2021484375,
      "grad_norm": 6.005353927612305,
      "learning_rate": 1.3301595052083335e-05,
      "loss": 1.8855,
      "step": 9020
    },
    {
      "epoch": 2.20458984375,
      "grad_norm": 6.319979190826416,
      "learning_rate": 1.3260904947916666e-05,
      "loss": 2.0864,
      "step": 9030
    },
    {
      "epoch": 2.20703125,
      "grad_norm": 3.387054681777954,
      "learning_rate": 1.3220214843750001e-05,
      "loss": 1.7871,
      "step": 9040
    },
    {
      "epoch": 2.20947265625,
      "grad_norm": 4.1615095138549805,
      "learning_rate": 1.3179524739583335e-05,
      "loss": 2.0433,
      "step": 9050
    },
    {
      "epoch": 2.2119140625,
      "grad_norm": 5.40675163269043,
      "learning_rate": 1.3138834635416666e-05,
      "loss": 1.94,
      "step": 9060
    },
    {
      "epoch": 2.21435546875,
      "grad_norm": 5.28299617767334,
      "learning_rate": 1.309814453125e-05,
      "loss": 2.1041,
      "step": 9070
    },
    {
      "epoch": 2.216796875,
      "grad_norm": 6.209596157073975,
      "learning_rate": 1.3057454427083335e-05,
      "loss": 2.256,
      "step": 9080
    },
    {
      "epoch": 2.21923828125,
      "grad_norm": 6.45443058013916,
      "learning_rate": 1.3016764322916666e-05,
      "loss": 1.9728,
      "step": 9090
    },
    {
      "epoch": 2.2216796875,
      "grad_norm": 6.576704025268555,
      "learning_rate": 1.297607421875e-05,
      "loss": 1.991,
      "step": 9100
    },
    {
      "epoch": 2.22412109375,
      "grad_norm": 5.453228950500488,
      "learning_rate": 1.2935384114583335e-05,
      "loss": 2.0697,
      "step": 9110
    },
    {
      "epoch": 2.2265625,
      "grad_norm": 4.49083137512207,
      "learning_rate": 1.2894694010416666e-05,
      "loss": 2.1102,
      "step": 9120
    },
    {
      "epoch": 2.22900390625,
      "grad_norm": 6.226383209228516,
      "learning_rate": 1.285400390625e-05,
      "loss": 2.1784,
      "step": 9130
    },
    {
      "epoch": 2.2314453125,
      "grad_norm": 5.51116418838501,
      "learning_rate": 1.2813313802083335e-05,
      "loss": 1.8128,
      "step": 9140
    },
    {
      "epoch": 2.23388671875,
      "grad_norm": 5.599817276000977,
      "learning_rate": 1.2772623697916666e-05,
      "loss": 2.1844,
      "step": 9150
    },
    {
      "epoch": 2.236328125,
      "grad_norm": 4.706624984741211,
      "learning_rate": 1.273193359375e-05,
      "loss": 2.033,
      "step": 9160
    },
    {
      "epoch": 2.23876953125,
      "grad_norm": 4.275507926940918,
      "learning_rate": 1.2691243489583335e-05,
      "loss": 1.8945,
      "step": 9170
    },
    {
      "epoch": 2.2412109375,
      "grad_norm": 4.906450271606445,
      "learning_rate": 1.2650553385416666e-05,
      "loss": 1.89,
      "step": 9180
    },
    {
      "epoch": 2.24365234375,
      "grad_norm": 3.461561441421509,
      "learning_rate": 1.260986328125e-05,
      "loss": 1.9084,
      "step": 9190
    },
    {
      "epoch": 2.24609375,
      "grad_norm": 7.080495834350586,
      "learning_rate": 1.2569173177083335e-05,
      "loss": 2.0396,
      "step": 9200
    },
    {
      "epoch": 2.24853515625,
      "grad_norm": 5.9744648933410645,
      "learning_rate": 1.2528483072916666e-05,
      "loss": 2.0803,
      "step": 9210
    },
    {
      "epoch": 2.2509765625,
      "grad_norm": 6.833141803741455,
      "learning_rate": 1.248779296875e-05,
      "loss": 1.8253,
      "step": 9220
    },
    {
      "epoch": 2.25341796875,
      "grad_norm": 4.197854518890381,
      "learning_rate": 1.2447102864583333e-05,
      "loss": 2.0287,
      "step": 9230
    },
    {
      "epoch": 2.255859375,
      "grad_norm": 5.00985050201416,
      "learning_rate": 1.2406412760416667e-05,
      "loss": 2.0057,
      "step": 9240
    },
    {
      "epoch": 2.25830078125,
      "grad_norm": 5.500946044921875,
      "learning_rate": 1.236572265625e-05,
      "loss": 2.0583,
      "step": 9250
    },
    {
      "epoch": 2.2607421875,
      "grad_norm": 5.420984745025635,
      "learning_rate": 1.2325032552083333e-05,
      "loss": 1.9973,
      "step": 9260
    },
    {
      "epoch": 2.26318359375,
      "grad_norm": 7.947021007537842,
      "learning_rate": 1.2284342447916667e-05,
      "loss": 1.9827,
      "step": 9270
    },
    {
      "epoch": 2.265625,
      "grad_norm": 5.132359027862549,
      "learning_rate": 1.224365234375e-05,
      "loss": 1.9982,
      "step": 9280
    },
    {
      "epoch": 2.26806640625,
      "grad_norm": 4.111363410949707,
      "learning_rate": 1.2202962239583333e-05,
      "loss": 1.9069,
      "step": 9290
    },
    {
      "epoch": 2.2705078125,
      "grad_norm": 9.16122817993164,
      "learning_rate": 1.2162272135416667e-05,
      "loss": 2.1421,
      "step": 9300
    },
    {
      "epoch": 2.27294921875,
      "grad_norm": 6.895666122436523,
      "learning_rate": 1.212158203125e-05,
      "loss": 2.1807,
      "step": 9310
    },
    {
      "epoch": 2.275390625,
      "grad_norm": 8.784300804138184,
      "learning_rate": 1.2080891927083333e-05,
      "loss": 2.2221,
      "step": 9320
    },
    {
      "epoch": 2.27783203125,
      "grad_norm": 5.388009548187256,
      "learning_rate": 1.2040201822916667e-05,
      "loss": 2.0771,
      "step": 9330
    },
    {
      "epoch": 2.2802734375,
      "grad_norm": 4.913763046264648,
      "learning_rate": 1.199951171875e-05,
      "loss": 1.9272,
      "step": 9340
    },
    {
      "epoch": 2.28271484375,
      "grad_norm": 4.847507476806641,
      "learning_rate": 1.1958821614583334e-05,
      "loss": 1.8967,
      "step": 9350
    },
    {
      "epoch": 2.28515625,
      "grad_norm": 9.652101516723633,
      "learning_rate": 1.1918131510416667e-05,
      "loss": 2.1115,
      "step": 9360
    },
    {
      "epoch": 2.28759765625,
      "grad_norm": 5.842996120452881,
      "learning_rate": 1.1877441406250001e-05,
      "loss": 1.8308,
      "step": 9370
    },
    {
      "epoch": 2.2900390625,
      "grad_norm": 5.012149333953857,
      "learning_rate": 1.1836751302083334e-05,
      "loss": 2.0541,
      "step": 9380
    },
    {
      "epoch": 2.29248046875,
      "grad_norm": 5.5236310958862305,
      "learning_rate": 1.1796061197916667e-05,
      "loss": 1.8219,
      "step": 9390
    },
    {
      "epoch": 2.294921875,
      "grad_norm": 5.781736373901367,
      "learning_rate": 1.1755371093750001e-05,
      "loss": 2.1527,
      "step": 9400
    },
    {
      "epoch": 2.29736328125,
      "grad_norm": 5.2444071769714355,
      "learning_rate": 1.1714680989583334e-05,
      "loss": 1.9151,
      "step": 9410
    },
    {
      "epoch": 2.2998046875,
      "grad_norm": 3.305372714996338,
      "learning_rate": 1.1673990885416668e-05,
      "loss": 1.9098,
      "step": 9420
    },
    {
      "epoch": 2.30224609375,
      "grad_norm": 8.690587997436523,
      "learning_rate": 1.1633300781250001e-05,
      "loss": 1.968,
      "step": 9430
    },
    {
      "epoch": 2.3046875,
      "grad_norm": 4.581580638885498,
      "learning_rate": 1.1592610677083334e-05,
      "loss": 2.0253,
      "step": 9440
    },
    {
      "epoch": 2.30712890625,
      "grad_norm": 4.579745292663574,
      "learning_rate": 1.1551920572916668e-05,
      "loss": 2.063,
      "step": 9450
    },
    {
      "epoch": 2.3095703125,
      "grad_norm": 3.6133151054382324,
      "learning_rate": 1.1511230468750001e-05,
      "loss": 2.0913,
      "step": 9460
    },
    {
      "epoch": 2.31201171875,
      "grad_norm": 5.148662090301514,
      "learning_rate": 1.1470540364583334e-05,
      "loss": 2.0406,
      "step": 9470
    },
    {
      "epoch": 2.314453125,
      "grad_norm": 5.1530609130859375,
      "learning_rate": 1.1429850260416668e-05,
      "loss": 2.1751,
      "step": 9480
    },
    {
      "epoch": 2.31689453125,
      "grad_norm": 4.656540393829346,
      "learning_rate": 1.1389160156250001e-05,
      "loss": 2.0833,
      "step": 9490
    },
    {
      "epoch": 2.3193359375,
      "grad_norm": 4.637571334838867,
      "learning_rate": 1.1348470052083334e-05,
      "loss": 1.9822,
      "step": 9500
    },
    {
      "epoch": 2.32177734375,
      "grad_norm": 10.608325958251953,
      "learning_rate": 1.1307779947916668e-05,
      "loss": 2.0498,
      "step": 9510
    },
    {
      "epoch": 2.32421875,
      "grad_norm": 4.781363010406494,
      "learning_rate": 1.1267089843750001e-05,
      "loss": 2.0288,
      "step": 9520
    },
    {
      "epoch": 2.32666015625,
      "grad_norm": 6.876678466796875,
      "learning_rate": 1.1226399739583334e-05,
      "loss": 2.0146,
      "step": 9530
    },
    {
      "epoch": 2.3291015625,
      "grad_norm": 4.935571193695068,
      "learning_rate": 1.1185709635416668e-05,
      "loss": 2.1015,
      "step": 9540
    },
    {
      "epoch": 2.33154296875,
      "grad_norm": 4.863860130310059,
      "learning_rate": 1.114501953125e-05,
      "loss": 2.1147,
      "step": 9550
    },
    {
      "epoch": 2.333984375,
      "grad_norm": 3.6984951496124268,
      "learning_rate": 1.1104329427083333e-05,
      "loss": 2.1674,
      "step": 9560
    },
    {
      "epoch": 2.33642578125,
      "grad_norm": 3.5350215435028076,
      "learning_rate": 1.1063639322916668e-05,
      "loss": 1.9983,
      "step": 9570
    },
    {
      "epoch": 2.3388671875,
      "grad_norm": 4.067290306091309,
      "learning_rate": 1.102294921875e-05,
      "loss": 2.199,
      "step": 9580
    },
    {
      "epoch": 2.34130859375,
      "grad_norm": 5.359466075897217,
      "learning_rate": 1.0982259114583333e-05,
      "loss": 2.1021,
      "step": 9590
    },
    {
      "epoch": 2.34375,
      "grad_norm": 5.612009525299072,
      "learning_rate": 1.0941569010416668e-05,
      "loss": 2.1989,
      "step": 9600
    },
    {
      "epoch": 2.34619140625,
      "grad_norm": 4.788543701171875,
      "learning_rate": 1.090087890625e-05,
      "loss": 2.0528,
      "step": 9610
    },
    {
      "epoch": 2.3486328125,
      "grad_norm": 7.063022613525391,
      "learning_rate": 1.0860188802083333e-05,
      "loss": 1.9395,
      "step": 9620
    },
    {
      "epoch": 2.35107421875,
      "grad_norm": 5.427948474884033,
      "learning_rate": 1.0819498697916668e-05,
      "loss": 1.962,
      "step": 9630
    },
    {
      "epoch": 2.353515625,
      "grad_norm": 6.773391246795654,
      "learning_rate": 1.077880859375e-05,
      "loss": 2.1559,
      "step": 9640
    },
    {
      "epoch": 2.35595703125,
      "grad_norm": 7.314776420593262,
      "learning_rate": 1.0738118489583333e-05,
      "loss": 1.9512,
      "step": 9650
    },
    {
      "epoch": 2.3583984375,
      "grad_norm": 4.036967754364014,
      "learning_rate": 1.0697428385416668e-05,
      "loss": 1.8154,
      "step": 9660
    },
    {
      "epoch": 2.36083984375,
      "grad_norm": 7.701634407043457,
      "learning_rate": 1.065673828125e-05,
      "loss": 1.9482,
      "step": 9670
    },
    {
      "epoch": 2.36328125,
      "grad_norm": 4.280491828918457,
      "learning_rate": 1.0616048177083333e-05,
      "loss": 1.7899,
      "step": 9680
    },
    {
      "epoch": 2.36572265625,
      "grad_norm": 4.244207859039307,
      "learning_rate": 1.0575358072916667e-05,
      "loss": 1.9209,
      "step": 9690
    },
    {
      "epoch": 2.3681640625,
      "grad_norm": 6.511090278625488,
      "learning_rate": 1.053466796875e-05,
      "loss": 2.1794,
      "step": 9700
    },
    {
      "epoch": 2.37060546875,
      "grad_norm": 6.06095552444458,
      "learning_rate": 1.0493977864583333e-05,
      "loss": 2.0869,
      "step": 9710
    },
    {
      "epoch": 2.373046875,
      "grad_norm": 5.908383369445801,
      "learning_rate": 1.0453287760416667e-05,
      "loss": 2.0591,
      "step": 9720
    },
    {
      "epoch": 2.37548828125,
      "grad_norm": 6.861988067626953,
      "learning_rate": 1.041259765625e-05,
      "loss": 2.0136,
      "step": 9730
    },
    {
      "epoch": 2.3779296875,
      "grad_norm": 5.847772598266602,
      "learning_rate": 1.0371907552083333e-05,
      "loss": 2.2464,
      "step": 9740
    },
    {
      "epoch": 2.38037109375,
      "grad_norm": 5.2227935791015625,
      "learning_rate": 1.0331217447916667e-05,
      "loss": 1.851,
      "step": 9750
    },
    {
      "epoch": 2.3828125,
      "grad_norm": 8.749781608581543,
      "learning_rate": 1.029052734375e-05,
      "loss": 1.9338,
      "step": 9760
    },
    {
      "epoch": 2.38525390625,
      "grad_norm": 6.735744953155518,
      "learning_rate": 1.0249837239583333e-05,
      "loss": 1.8018,
      "step": 9770
    },
    {
      "epoch": 2.3876953125,
      "grad_norm": 5.095259666442871,
      "learning_rate": 1.0209147135416667e-05,
      "loss": 2.0535,
      "step": 9780
    },
    {
      "epoch": 2.39013671875,
      "grad_norm": 6.156435489654541,
      "learning_rate": 1.016845703125e-05,
      "loss": 1.7797,
      "step": 9790
    },
    {
      "epoch": 2.392578125,
      "grad_norm": 5.092252254486084,
      "learning_rate": 1.0127766927083333e-05,
      "loss": 2.0929,
      "step": 9800
    },
    {
      "epoch": 2.39501953125,
      "grad_norm": 6.069978713989258,
      "learning_rate": 1.0087076822916667e-05,
      "loss": 2.1825,
      "step": 9810
    },
    {
      "epoch": 2.3974609375,
      "grad_norm": 3.8613877296447754,
      "learning_rate": 1.004638671875e-05,
      "loss": 2.0323,
      "step": 9820
    },
    {
      "epoch": 2.39990234375,
      "grad_norm": 6.763985633850098,
      "learning_rate": 1.0005696614583333e-05,
      "loss": 1.9546,
      "step": 9830
    },
    {
      "epoch": 2.40234375,
      "grad_norm": 6.697855472564697,
      "learning_rate": 9.965006510416667e-06,
      "loss": 1.9975,
      "step": 9840
    },
    {
      "epoch": 2.40478515625,
      "grad_norm": 6.452691555023193,
      "learning_rate": 9.92431640625e-06,
      "loss": 2.0167,
      "step": 9850
    },
    {
      "epoch": 2.4072265625,
      "grad_norm": 3.6209399700164795,
      "learning_rate": 9.883626302083334e-06,
      "loss": 2.0211,
      "step": 9860
    },
    {
      "epoch": 2.40966796875,
      "grad_norm": 3.8073065280914307,
      "learning_rate": 9.842936197916667e-06,
      "loss": 1.9548,
      "step": 9870
    },
    {
      "epoch": 2.412109375,
      "grad_norm": 7.338233470916748,
      "learning_rate": 9.802246093750001e-06,
      "loss": 2.0088,
      "step": 9880
    },
    {
      "epoch": 2.41455078125,
      "grad_norm": 7.004356861114502,
      "learning_rate": 9.761555989583334e-06,
      "loss": 1.934,
      "step": 9890
    },
    {
      "epoch": 2.4169921875,
      "grad_norm": 6.204686641693115,
      "learning_rate": 9.720865885416668e-06,
      "loss": 1.8721,
      "step": 9900
    },
    {
      "epoch": 2.41943359375,
      "grad_norm": 4.060643196105957,
      "learning_rate": 9.680175781250001e-06,
      "loss": 2.0792,
      "step": 9910
    },
    {
      "epoch": 2.421875,
      "grad_norm": 3.8070812225341797,
      "learning_rate": 9.639485677083334e-06,
      "loss": 1.8819,
      "step": 9920
    },
    {
      "epoch": 2.42431640625,
      "grad_norm": 8.256393432617188,
      "learning_rate": 9.598795572916668e-06,
      "loss": 2.0882,
      "step": 9930
    },
    {
      "epoch": 2.4267578125,
      "grad_norm": 6.569866180419922,
      "learning_rate": 9.558105468750001e-06,
      "loss": 1.9899,
      "step": 9940
    },
    {
      "epoch": 2.42919921875,
      "grad_norm": 4.404758930206299,
      "learning_rate": 9.517415364583334e-06,
      "loss": 2.0316,
      "step": 9950
    },
    {
      "epoch": 2.431640625,
      "grad_norm": 6.812569618225098,
      "learning_rate": 9.476725260416668e-06,
      "loss": 2.1164,
      "step": 9960
    },
    {
      "epoch": 2.43408203125,
      "grad_norm": 4.43760347366333,
      "learning_rate": 9.436035156250001e-06,
      "loss": 1.9514,
      "step": 9970
    },
    {
      "epoch": 2.4365234375,
      "grad_norm": 3.4763684272766113,
      "learning_rate": 9.395345052083334e-06,
      "loss": 1.9632,
      "step": 9980
    },
    {
      "epoch": 2.43896484375,
      "grad_norm": 5.601917266845703,
      "learning_rate": 9.354654947916668e-06,
      "loss": 2.1092,
      "step": 9990
    },
    {
      "epoch": 2.44140625,
      "grad_norm": 5.677037239074707,
      "learning_rate": 9.31396484375e-06,
      "loss": 1.9904,
      "step": 10000
    },
    {
      "epoch": 2.44384765625,
      "grad_norm": 6.735728740692139,
      "learning_rate": 9.273274739583334e-06,
      "loss": 1.8377,
      "step": 10010
    },
    {
      "epoch": 2.4462890625,
      "grad_norm": 4.7107110023498535,
      "learning_rate": 9.232584635416668e-06,
      "loss": 1.9965,
      "step": 10020
    },
    {
      "epoch": 2.44873046875,
      "grad_norm": 4.997391700744629,
      "learning_rate": 9.19189453125e-06,
      "loss": 2.003,
      "step": 10030
    },
    {
      "epoch": 2.451171875,
      "grad_norm": 7.2595720291137695,
      "learning_rate": 9.151204427083333e-06,
      "loss": 1.9868,
      "step": 10040
    },
    {
      "epoch": 2.45361328125,
      "grad_norm": 6.341078758239746,
      "learning_rate": 9.110514322916668e-06,
      "loss": 1.944,
      "step": 10050
    },
    {
      "epoch": 2.4560546875,
      "grad_norm": 6.391170978546143,
      "learning_rate": 9.06982421875e-06,
      "loss": 2.02,
      "step": 10060
    },
    {
      "epoch": 2.45849609375,
      "grad_norm": 5.440094470977783,
      "learning_rate": 9.029134114583333e-06,
      "loss": 1.9286,
      "step": 10070
    },
    {
      "epoch": 2.4609375,
      "grad_norm": 6.199402332305908,
      "learning_rate": 8.988444010416668e-06,
      "loss": 1.9781,
      "step": 10080
    },
    {
      "epoch": 2.46337890625,
      "grad_norm": 6.962412357330322,
      "learning_rate": 8.94775390625e-06,
      "loss": 1.9455,
      "step": 10090
    },
    {
      "epoch": 2.4658203125,
      "grad_norm": 4.587612628936768,
      "learning_rate": 8.907063802083333e-06,
      "loss": 1.9889,
      "step": 10100
    },
    {
      "epoch": 2.46826171875,
      "grad_norm": 6.718532085418701,
      "learning_rate": 8.866373697916668e-06,
      "loss": 2.0267,
      "step": 10110
    },
    {
      "epoch": 2.470703125,
      "grad_norm": 6.816495418548584,
      "learning_rate": 8.82568359375e-06,
      "loss": 2.0775,
      "step": 10120
    },
    {
      "epoch": 2.47314453125,
      "grad_norm": 5.793878078460693,
      "learning_rate": 8.784993489583333e-06,
      "loss": 2.3214,
      "step": 10130
    },
    {
      "epoch": 2.4755859375,
      "grad_norm": 4.662170886993408,
      "learning_rate": 8.744303385416668e-06,
      "loss": 1.9515,
      "step": 10140
    },
    {
      "epoch": 2.47802734375,
      "grad_norm": 8.079020500183105,
      "learning_rate": 8.70361328125e-06,
      "loss": 1.8292,
      "step": 10150
    },
    {
      "epoch": 2.48046875,
      "grad_norm": 5.281771183013916,
      "learning_rate": 8.662923177083333e-06,
      "loss": 1.9028,
      "step": 10160
    },
    {
      "epoch": 2.48291015625,
      "grad_norm": 6.620693683624268,
      "learning_rate": 8.622233072916667e-06,
      "loss": 1.9966,
      "step": 10170
    },
    {
      "epoch": 2.4853515625,
      "grad_norm": 5.0245184898376465,
      "learning_rate": 8.58154296875e-06,
      "loss": 1.9779,
      "step": 10180
    },
    {
      "epoch": 2.48779296875,
      "grad_norm": 4.090142726898193,
      "learning_rate": 8.540852864583333e-06,
      "loss": 1.908,
      "step": 10190
    },
    {
      "epoch": 2.490234375,
      "grad_norm": 6.388958930969238,
      "learning_rate": 8.500162760416667e-06,
      "loss": 2.1003,
      "step": 10200
    },
    {
      "epoch": 2.49267578125,
      "grad_norm": 7.449470043182373,
      "learning_rate": 8.45947265625e-06,
      "loss": 2.0235,
      "step": 10210
    },
    {
      "epoch": 2.4951171875,
      "grad_norm": 5.751414775848389,
      "learning_rate": 8.418782552083333e-06,
      "loss": 2.0044,
      "step": 10220
    },
    {
      "epoch": 2.49755859375,
      "grad_norm": 5.780592441558838,
      "learning_rate": 8.378092447916667e-06,
      "loss": 2.0299,
      "step": 10230
    },
    {
      "epoch": 2.5,
      "grad_norm": 4.209904193878174,
      "learning_rate": 8.33740234375e-06,
      "loss": 2.0146,
      "step": 10240
    },
    {
      "epoch": 2.50244140625,
      "grad_norm": 5.235332012176514,
      "learning_rate": 8.296712239583333e-06,
      "loss": 2.1417,
      "step": 10250
    },
    {
      "epoch": 2.5048828125,
      "grad_norm": 5.563718795776367,
      "learning_rate": 8.256022135416667e-06,
      "loss": 2.1294,
      "step": 10260
    },
    {
      "epoch": 2.50732421875,
      "grad_norm": 3.918823719024658,
      "learning_rate": 8.21533203125e-06,
      "loss": 1.8122,
      "step": 10270
    },
    {
      "epoch": 2.509765625,
      "grad_norm": 4.491087436676025,
      "learning_rate": 8.174641927083333e-06,
      "loss": 1.9968,
      "step": 10280
    },
    {
      "epoch": 2.51220703125,
      "grad_norm": 4.32786750793457,
      "learning_rate": 8.133951822916667e-06,
      "loss": 1.9887,
      "step": 10290
    },
    {
      "epoch": 2.5146484375,
      "grad_norm": 5.597733020782471,
      "learning_rate": 8.09326171875e-06,
      "loss": 2.0574,
      "step": 10300
    },
    {
      "epoch": 2.51708984375,
      "grad_norm": 6.738620758056641,
      "learning_rate": 8.052571614583332e-06,
      "loss": 2.1488,
      "step": 10310
    },
    {
      "epoch": 2.51953125,
      "grad_norm": 7.023026943206787,
      "learning_rate": 8.011881510416667e-06,
      "loss": 1.9693,
      "step": 10320
    },
    {
      "epoch": 2.52197265625,
      "grad_norm": 4.298691272735596,
      "learning_rate": 7.97119140625e-06,
      "loss": 1.9259,
      "step": 10330
    },
    {
      "epoch": 2.5244140625,
      "grad_norm": 6.254760265350342,
      "learning_rate": 7.930501302083334e-06,
      "loss": 1.9786,
      "step": 10340
    },
    {
      "epoch": 2.52685546875,
      "grad_norm": 3.7960731983184814,
      "learning_rate": 7.889811197916667e-06,
      "loss": 2.0028,
      "step": 10350
    },
    {
      "epoch": 2.529296875,
      "grad_norm": 3.5823192596435547,
      "learning_rate": 7.849121093750001e-06,
      "loss": 1.967,
      "step": 10360
    },
    {
      "epoch": 2.53173828125,
      "grad_norm": 7.86301326751709,
      "learning_rate": 7.808430989583334e-06,
      "loss": 2.2335,
      "step": 10370
    },
    {
      "epoch": 2.5341796875,
      "grad_norm": 6.069089889526367,
      "learning_rate": 7.767740885416667e-06,
      "loss": 2.0734,
      "step": 10380
    },
    {
      "epoch": 2.53662109375,
      "grad_norm": 4.5858283042907715,
      "learning_rate": 7.727050781250001e-06,
      "loss": 2.0069,
      "step": 10390
    },
    {
      "epoch": 2.5390625,
      "grad_norm": 6.961788177490234,
      "learning_rate": 7.686360677083334e-06,
      "loss": 2.077,
      "step": 10400
    },
    {
      "epoch": 2.54150390625,
      "grad_norm": 4.180336952209473,
      "learning_rate": 7.645670572916668e-06,
      "loss": 1.8555,
      "step": 10410
    },
    {
      "epoch": 2.5439453125,
      "grad_norm": 4.120978832244873,
      "learning_rate": 7.60498046875e-06,
      "loss": 1.7582,
      "step": 10420
    },
    {
      "epoch": 2.54638671875,
      "grad_norm": 5.62636137008667,
      "learning_rate": 7.564290364583333e-06,
      "loss": 2.0022,
      "step": 10430
    },
    {
      "epoch": 2.548828125,
      "grad_norm": 4.985145568847656,
      "learning_rate": 7.523600260416667e-06,
      "loss": 1.968,
      "step": 10440
    },
    {
      "epoch": 2.55126953125,
      "grad_norm": 4.010902404785156,
      "learning_rate": 7.48291015625e-06,
      "loss": 2.0023,
      "step": 10450
    },
    {
      "epoch": 2.5537109375,
      "grad_norm": 5.908644199371338,
      "learning_rate": 7.442220052083334e-06,
      "loss": 1.9528,
      "step": 10460
    },
    {
      "epoch": 2.55615234375,
      "grad_norm": 5.726779460906982,
      "learning_rate": 7.401529947916667e-06,
      "loss": 1.842,
      "step": 10470
    },
    {
      "epoch": 2.55859375,
      "grad_norm": 4.675449848175049,
      "learning_rate": 7.36083984375e-06,
      "loss": 2.0912,
      "step": 10480
    },
    {
      "epoch": 2.56103515625,
      "grad_norm": 5.475020885467529,
      "learning_rate": 7.3201497395833335e-06,
      "loss": 2.0608,
      "step": 10490
    },
    {
      "epoch": 2.5634765625,
      "grad_norm": 6.1954803466796875,
      "learning_rate": 7.279459635416667e-06,
      "loss": 1.8385,
      "step": 10500
    },
    {
      "epoch": 2.56591796875,
      "grad_norm": 3.522416830062866,
      "learning_rate": 7.238769531250001e-06,
      "loss": 1.9771,
      "step": 10510
    },
    {
      "epoch": 2.568359375,
      "grad_norm": 8.083277702331543,
      "learning_rate": 7.198079427083333e-06,
      "loss": 1.9854,
      "step": 10520
    },
    {
      "epoch": 2.57080078125,
      "grad_norm": 4.817133903503418,
      "learning_rate": 7.157389322916668e-06,
      "loss": 2.0832,
      "step": 10530
    },
    {
      "epoch": 2.5732421875,
      "grad_norm": 6.264163494110107,
      "learning_rate": 7.1166992187500006e-06,
      "loss": 1.8328,
      "step": 10540
    },
    {
      "epoch": 2.57568359375,
      "grad_norm": 6.257239818572998,
      "learning_rate": 7.076009114583333e-06,
      "loss": 1.9838,
      "step": 10550
    },
    {
      "epoch": 2.578125,
      "grad_norm": 4.475339412689209,
      "learning_rate": 7.035319010416668e-06,
      "loss": 1.8329,
      "step": 10560
    },
    {
      "epoch": 2.58056640625,
      "grad_norm": 7.095290660858154,
      "learning_rate": 6.9946289062500005e-06,
      "loss": 2.0213,
      "step": 10570
    },
    {
      "epoch": 2.5830078125,
      "grad_norm": 4.72643518447876,
      "learning_rate": 6.953938802083333e-06,
      "loss": 1.9188,
      "step": 10580
    },
    {
      "epoch": 2.58544921875,
      "grad_norm": 6.145205497741699,
      "learning_rate": 6.913248697916668e-06,
      "loss": 2.0281,
      "step": 10590
    },
    {
      "epoch": 2.587890625,
      "grad_norm": 3.5683248043060303,
      "learning_rate": 6.87255859375e-06,
      "loss": 1.9925,
      "step": 10600
    },
    {
      "epoch": 2.59033203125,
      "grad_norm": 5.619402885437012,
      "learning_rate": 6.831868489583333e-06,
      "loss": 1.8647,
      "step": 10610
    },
    {
      "epoch": 2.5927734375,
      "grad_norm": 7.949748992919922,
      "learning_rate": 6.7911783854166675e-06,
      "loss": 2.0469,
      "step": 10620
    },
    {
      "epoch": 2.59521484375,
      "grad_norm": 3.589031457901001,
      "learning_rate": 6.75048828125e-06,
      "loss": 2.0399,
      "step": 10630
    },
    {
      "epoch": 2.59765625,
      "grad_norm": 5.420596599578857,
      "learning_rate": 6.709798177083333e-06,
      "loss": 2.0035,
      "step": 10640
    },
    {
      "epoch": 2.60009765625,
      "grad_norm": 4.6517252922058105,
      "learning_rate": 6.669108072916667e-06,
      "loss": 1.9904,
      "step": 10650
    },
    {
      "epoch": 2.6025390625,
      "grad_norm": 5.805283546447754,
      "learning_rate": 6.62841796875e-06,
      "loss": 2.01,
      "step": 10660
    },
    {
      "epoch": 2.60498046875,
      "grad_norm": 5.575065612792969,
      "learning_rate": 6.587727864583333e-06,
      "loss": 2.0356,
      "step": 10670
    },
    {
      "epoch": 2.607421875,
      "grad_norm": 5.200705528259277,
      "learning_rate": 6.547037760416667e-06,
      "loss": 2.0419,
      "step": 10680
    },
    {
      "epoch": 2.60986328125,
      "grad_norm": 6.946002960205078,
      "learning_rate": 6.50634765625e-06,
      "loss": 2.035,
      "step": 10690
    },
    {
      "epoch": 2.6123046875,
      "grad_norm": 6.290890216827393,
      "learning_rate": 6.465657552083334e-06,
      "loss": 2.0629,
      "step": 10700
    },
    {
      "epoch": 2.61474609375,
      "grad_norm": 5.789004325866699,
      "learning_rate": 6.424967447916667e-06,
      "loss": 1.9737,
      "step": 10710
    },
    {
      "epoch": 2.6171875,
      "grad_norm": 3.5119292736053467,
      "learning_rate": 6.38427734375e-06,
      "loss": 2.0155,
      "step": 10720
    },
    {
      "epoch": 2.61962890625,
      "grad_norm": 6.236867427825928,
      "learning_rate": 6.3435872395833335e-06,
      "loss": 1.9514,
      "step": 10730
    },
    {
      "epoch": 2.6220703125,
      "grad_norm": 5.4666337966918945,
      "learning_rate": 6.302897135416667e-06,
      "loss": 1.8552,
      "step": 10740
    },
    {
      "epoch": 2.62451171875,
      "grad_norm": 5.919288635253906,
      "learning_rate": 6.262207031250001e-06,
      "loss": 2.0078,
      "step": 10750
    },
    {
      "epoch": 2.626953125,
      "grad_norm": 4.273507595062256,
      "learning_rate": 6.221516927083334e-06,
      "loss": 1.9926,
      "step": 10760
    },
    {
      "epoch": 2.62939453125,
      "grad_norm": 5.978726387023926,
      "learning_rate": 6.180826822916667e-06,
      "loss": 1.8821,
      "step": 10770
    },
    {
      "epoch": 2.6318359375,
      "grad_norm": 5.123662948608398,
      "learning_rate": 6.1401367187500005e-06,
      "loss": 2.1246,
      "step": 10780
    },
    {
      "epoch": 2.63427734375,
      "grad_norm": 3.826441526412964,
      "learning_rate": 6.099446614583334e-06,
      "loss": 2.0987,
      "step": 10790
    },
    {
      "epoch": 2.63671875,
      "grad_norm": 4.614815711975098,
      "learning_rate": 6.058756510416667e-06,
      "loss": 2.094,
      "step": 10800
    },
    {
      "epoch": 2.63916015625,
      "grad_norm": 8.851347923278809,
      "learning_rate": 6.0180664062500004e-06,
      "loss": 2.1398,
      "step": 10810
    },
    {
      "epoch": 2.6416015625,
      "grad_norm": 3.6591289043426514,
      "learning_rate": 5.977376302083334e-06,
      "loss": 1.9724,
      "step": 10820
    },
    {
      "epoch": 2.64404296875,
      "grad_norm": 6.112117767333984,
      "learning_rate": 5.936686197916667e-06,
      "loss": 2.0145,
      "step": 10830
    },
    {
      "epoch": 2.646484375,
      "grad_norm": 5.101861476898193,
      "learning_rate": 5.89599609375e-06,
      "loss": 2.0415,
      "step": 10840
    },
    {
      "epoch": 2.64892578125,
      "grad_norm": 3.7915799617767334,
      "learning_rate": 5.855305989583334e-06,
      "loss": 1.8954,
      "step": 10850
    },
    {
      "epoch": 2.6513671875,
      "grad_norm": 3.4999868869781494,
      "learning_rate": 5.814615885416667e-06,
      "loss": 2.122,
      "step": 10860
    },
    {
      "epoch": 2.65380859375,
      "grad_norm": 6.336495399475098,
      "learning_rate": 5.77392578125e-06,
      "loss": 1.936,
      "step": 10870
    },
    {
      "epoch": 2.65625,
      "grad_norm": 8.763243675231934,
      "learning_rate": 5.733235677083334e-06,
      "loss": 1.9044,
      "step": 10880
    },
    {
      "epoch": 2.65869140625,
      "grad_norm": 4.213160514831543,
      "learning_rate": 5.6925455729166665e-06,
      "loss": 1.8816,
      "step": 10890
    },
    {
      "epoch": 2.6611328125,
      "grad_norm": 5.8786797523498535,
      "learning_rate": 5.65185546875e-06,
      "loss": 2.083,
      "step": 10900
    },
    {
      "epoch": 2.66357421875,
      "grad_norm": 5.856537818908691,
      "learning_rate": 5.611165364583334e-06,
      "loss": 2.2187,
      "step": 10910
    },
    {
      "epoch": 2.666015625,
      "grad_norm": 5.702938079833984,
      "learning_rate": 5.570475260416666e-06,
      "loss": 1.8122,
      "step": 10920
    },
    {
      "epoch": 2.66845703125,
      "grad_norm": 4.368640899658203,
      "learning_rate": 5.52978515625e-06,
      "loss": 2.0346,
      "step": 10930
    },
    {
      "epoch": 2.6708984375,
      "grad_norm": 7.139517784118652,
      "learning_rate": 5.489095052083334e-06,
      "loss": 1.935,
      "step": 10940
    },
    {
      "epoch": 2.67333984375,
      "grad_norm": 4.591310024261475,
      "learning_rate": 5.448404947916666e-06,
      "loss": 1.9738,
      "step": 10950
    },
    {
      "epoch": 2.67578125,
      "grad_norm": 5.078165531158447,
      "learning_rate": 5.40771484375e-06,
      "loss": 2.054,
      "step": 10960
    },
    {
      "epoch": 2.67822265625,
      "grad_norm": 3.3456625938415527,
      "learning_rate": 5.3670247395833335e-06,
      "loss": 2.0851,
      "step": 10970
    },
    {
      "epoch": 2.6806640625,
      "grad_norm": 8.24483585357666,
      "learning_rate": 5.326334635416667e-06,
      "loss": 1.972,
      "step": 10980
    },
    {
      "epoch": 2.68310546875,
      "grad_norm": 4.72097110748291,
      "learning_rate": 5.285644531250001e-06,
      "loss": 2.0158,
      "step": 10990
    },
    {
      "epoch": 2.685546875,
      "grad_norm": 7.152531147003174,
      "learning_rate": 5.244954427083334e-06,
      "loss": 2.0472,
      "step": 11000
    },
    {
      "epoch": 2.68798828125,
      "grad_norm": 5.619648456573486,
      "learning_rate": 5.204264322916667e-06,
      "loss": 1.9841,
      "step": 11010
    },
    {
      "epoch": 2.6904296875,
      "grad_norm": 4.901437759399414,
      "learning_rate": 5.1635742187500005e-06,
      "loss": 2.0189,
      "step": 11020
    },
    {
      "epoch": 2.69287109375,
      "grad_norm": 4.447957515716553,
      "learning_rate": 5.122884114583334e-06,
      "loss": 2.0111,
      "step": 11030
    },
    {
      "epoch": 2.6953125,
      "grad_norm": 5.237621784210205,
      "learning_rate": 5.082194010416667e-06,
      "loss": 1.9365,
      "step": 11040
    },
    {
      "epoch": 2.69775390625,
      "grad_norm": 5.441109657287598,
      "learning_rate": 5.04150390625e-06,
      "loss": 2.0068,
      "step": 11050
    },
    {
      "epoch": 2.7001953125,
      "grad_norm": 6.509562015533447,
      "learning_rate": 5.000813802083334e-06,
      "loss": 2.0015,
      "step": 11060
    },
    {
      "epoch": 2.70263671875,
      "grad_norm": 6.2029242515563965,
      "learning_rate": 4.960123697916667e-06,
      "loss": 2.015,
      "step": 11070
    },
    {
      "epoch": 2.705078125,
      "grad_norm": 3.88456392288208,
      "learning_rate": 4.91943359375e-06,
      "loss": 1.9791,
      "step": 11080
    },
    {
      "epoch": 2.70751953125,
      "grad_norm": 3.695838451385498,
      "learning_rate": 4.878743489583334e-06,
      "loss": 1.9417,
      "step": 11090
    },
    {
      "epoch": 2.7099609375,
      "grad_norm": 4.9101948738098145,
      "learning_rate": 4.838053385416667e-06,
      "loss": 1.7563,
      "step": 11100
    },
    {
      "epoch": 2.71240234375,
      "grad_norm": 8.061932563781738,
      "learning_rate": 4.79736328125e-06,
      "loss": 1.9549,
      "step": 11110
    },
    {
      "epoch": 2.71484375,
      "grad_norm": 5.437298774719238,
      "learning_rate": 4.756673177083334e-06,
      "loss": 2.0305,
      "step": 11120
    },
    {
      "epoch": 2.71728515625,
      "grad_norm": 5.496481418609619,
      "learning_rate": 4.7159830729166665e-06,
      "loss": 1.8517,
      "step": 11130
    },
    {
      "epoch": 2.7197265625,
      "grad_norm": 4.618467330932617,
      "learning_rate": 4.67529296875e-06,
      "loss": 1.9728,
      "step": 11140
    },
    {
      "epoch": 2.72216796875,
      "grad_norm": 8.009786605834961,
      "learning_rate": 4.634602864583334e-06,
      "loss": 1.9858,
      "step": 11150
    },
    {
      "epoch": 2.724609375,
      "grad_norm": 9.700533866882324,
      "learning_rate": 4.593912760416666e-06,
      "loss": 2.0707,
      "step": 11160
    },
    {
      "epoch": 2.72705078125,
      "grad_norm": 8.706886291503906,
      "learning_rate": 4.55322265625e-06,
      "loss": 1.9899,
      "step": 11170
    },
    {
      "epoch": 2.7294921875,
      "grad_norm": 6.63736629486084,
      "learning_rate": 4.5125325520833336e-06,
      "loss": 1.9252,
      "step": 11180
    },
    {
      "epoch": 2.73193359375,
      "grad_norm": 4.636203289031982,
      "learning_rate": 4.471842447916666e-06,
      "loss": 1.937,
      "step": 11190
    },
    {
      "epoch": 2.734375,
      "grad_norm": 6.203013896942139,
      "learning_rate": 4.43115234375e-06,
      "loss": 2.1069,
      "step": 11200
    },
    {
      "epoch": 2.73681640625,
      "grad_norm": 5.967626571655273,
      "learning_rate": 4.3904622395833334e-06,
      "loss": 1.9952,
      "step": 11210
    },
    {
      "epoch": 2.7392578125,
      "grad_norm": 4.748725891113281,
      "learning_rate": 4.349772135416667e-06,
      "loss": 1.9477,
      "step": 11220
    },
    {
      "epoch": 2.74169921875,
      "grad_norm": 6.124764442443848,
      "learning_rate": 4.309082031250001e-06,
      "loss": 1.9218,
      "step": 11230
    },
    {
      "epoch": 2.744140625,
      "grad_norm": 7.5870161056518555,
      "learning_rate": 4.268391927083334e-06,
      "loss": 1.8947,
      "step": 11240
    },
    {
      "epoch": 2.74658203125,
      "grad_norm": 7.75913667678833,
      "learning_rate": 4.227701822916667e-06,
      "loss": 2.0649,
      "step": 11250
    },
    {
      "epoch": 2.7490234375,
      "grad_norm": 3.8951971530914307,
      "learning_rate": 4.1870117187500005e-06,
      "loss": 1.8351,
      "step": 11260
    },
    {
      "epoch": 2.75146484375,
      "grad_norm": 4.6714253425598145,
      "learning_rate": 4.146321614583334e-06,
      "loss": 2.0948,
      "step": 11270
    },
    {
      "epoch": 2.75390625,
      "grad_norm": 9.724705696105957,
      "learning_rate": 4.105631510416667e-06,
      "loss": 1.9324,
      "step": 11280
    },
    {
      "epoch": 2.75634765625,
      "grad_norm": 4.257323741912842,
      "learning_rate": 4.06494140625e-06,
      "loss": 1.9246,
      "step": 11290
    },
    {
      "epoch": 2.7587890625,
      "grad_norm": 4.557126045227051,
      "learning_rate": 4.024251302083334e-06,
      "loss": 2.0654,
      "step": 11300
    },
    {
      "epoch": 2.76123046875,
      "grad_norm": 5.685171127319336,
      "learning_rate": 3.983561197916667e-06,
      "loss": 1.8771,
      "step": 11310
    },
    {
      "epoch": 2.763671875,
      "grad_norm": 7.061281681060791,
      "learning_rate": 3.94287109375e-06,
      "loss": 1.8935,
      "step": 11320
    },
    {
      "epoch": 2.76611328125,
      "grad_norm": 6.36629581451416,
      "learning_rate": 3.902180989583334e-06,
      "loss": 2.077,
      "step": 11330
    },
    {
      "epoch": 2.7685546875,
      "grad_norm": 6.707566738128662,
      "learning_rate": 3.861490885416667e-06,
      "loss": 1.9853,
      "step": 11340
    },
    {
      "epoch": 2.77099609375,
      "grad_norm": 4.281193733215332,
      "learning_rate": 3.82080078125e-06,
      "loss": 2.0438,
      "step": 11350
    },
    {
      "epoch": 2.7734375,
      "grad_norm": 3.5622572898864746,
      "learning_rate": 3.7801106770833338e-06,
      "loss": 2.0077,
      "step": 11360
    },
    {
      "epoch": 2.77587890625,
      "grad_norm": 5.959658622741699,
      "learning_rate": 3.7394205729166665e-06,
      "loss": 1.9114,
      "step": 11370
    },
    {
      "epoch": 2.7783203125,
      "grad_norm": 5.958677768707275,
      "learning_rate": 3.69873046875e-06,
      "loss": 1.8551,
      "step": 11380
    },
    {
      "epoch": 2.78076171875,
      "grad_norm": 6.159235954284668,
      "learning_rate": 3.6580403645833336e-06,
      "loss": 1.8046,
      "step": 11390
    },
    {
      "epoch": 2.783203125,
      "grad_norm": 6.050356864929199,
      "learning_rate": 3.617350260416667e-06,
      "loss": 1.9978,
      "step": 11400
    },
    {
      "epoch": 2.78564453125,
      "grad_norm": 5.30659818649292,
      "learning_rate": 3.5766601562500004e-06,
      "loss": 2.1431,
      "step": 11410
    },
    {
      "epoch": 2.7880859375,
      "grad_norm": 5.487672328948975,
      "learning_rate": 3.5359700520833335e-06,
      "loss": 2.0853,
      "step": 11420
    },
    {
      "epoch": 2.79052734375,
      "grad_norm": 6.276288032531738,
      "learning_rate": 3.4952799479166667e-06,
      "loss": 1.9247,
      "step": 11430
    },
    {
      "epoch": 2.79296875,
      "grad_norm": 5.130101203918457,
      "learning_rate": 3.4545898437500003e-06,
      "loss": 1.9041,
      "step": 11440
    },
    {
      "epoch": 2.79541015625,
      "grad_norm": 5.452718257904053,
      "learning_rate": 3.413899739583334e-06,
      "loss": 1.982,
      "step": 11450
    },
    {
      "epoch": 2.7978515625,
      "grad_norm": 3.738420009613037,
      "learning_rate": 3.3732096354166666e-06,
      "loss": 1.8441,
      "step": 11460
    },
    {
      "epoch": 2.80029296875,
      "grad_norm": 6.914186954498291,
      "learning_rate": 3.33251953125e-06,
      "loss": 1.9858,
      "step": 11470
    },
    {
      "epoch": 2.802734375,
      "grad_norm": 5.61909818649292,
      "learning_rate": 3.2918294270833337e-06,
      "loss": 2.064,
      "step": 11480
    },
    {
      "epoch": 2.80517578125,
      "grad_norm": 6.464170455932617,
      "learning_rate": 3.2511393229166665e-06,
      "loss": 1.9743,
      "step": 11490
    },
    {
      "epoch": 2.8076171875,
      "grad_norm": 3.794811725616455,
      "learning_rate": 3.21044921875e-06,
      "loss": 1.9238,
      "step": 11500
    },
    {
      "epoch": 2.81005859375,
      "grad_norm": 5.824609279632568,
      "learning_rate": 3.1697591145833336e-06,
      "loss": 1.8758,
      "step": 11510
    },
    {
      "epoch": 2.8125,
      "grad_norm": 5.872136116027832,
      "learning_rate": 3.129069010416667e-06,
      "loss": 2.1383,
      "step": 11520
    },
    {
      "epoch": 2.81494140625,
      "grad_norm": 6.269739151000977,
      "learning_rate": 3.08837890625e-06,
      "loss": 1.9211,
      "step": 11530
    },
    {
      "epoch": 2.8173828125,
      "grad_norm": 4.551607608795166,
      "learning_rate": 3.0476888020833335e-06,
      "loss": 1.9056,
      "step": 11540
    },
    {
      "epoch": 2.81982421875,
      "grad_norm": 6.993317127227783,
      "learning_rate": 3.006998697916667e-06,
      "loss": 1.913,
      "step": 11550
    },
    {
      "epoch": 2.822265625,
      "grad_norm": 4.592174530029297,
      "learning_rate": 2.9663085937500003e-06,
      "loss": 1.9413,
      "step": 11560
    },
    {
      "epoch": 2.82470703125,
      "grad_norm": 6.538798809051514,
      "learning_rate": 2.9256184895833334e-06,
      "loss": 1.9826,
      "step": 11570
    },
    {
      "epoch": 2.8271484375,
      "grad_norm": 3.7552597522735596,
      "learning_rate": 2.884928385416667e-06,
      "loss": 2.0284,
      "step": 11580
    },
    {
      "epoch": 2.82958984375,
      "grad_norm": 6.6525797843933105,
      "learning_rate": 2.84423828125e-06,
      "loss": 1.7379,
      "step": 11590
    },
    {
      "epoch": 2.83203125,
      "grad_norm": 5.440038681030273,
      "learning_rate": 2.8035481770833333e-06,
      "loss": 2.0615,
      "step": 11600
    },
    {
      "epoch": 2.83447265625,
      "grad_norm": 4.907205104827881,
      "learning_rate": 2.762858072916667e-06,
      "loss": 2.0466,
      "step": 11610
    },
    {
      "epoch": 2.8369140625,
      "grad_norm": 5.238824844360352,
      "learning_rate": 2.72216796875e-06,
      "loss": 1.9841,
      "step": 11620
    },
    {
      "epoch": 2.83935546875,
      "grad_norm": 6.402015209197998,
      "learning_rate": 2.681477864583333e-06,
      "loss": 1.8516,
      "step": 11630
    },
    {
      "epoch": 2.841796875,
      "grad_norm": 6.270359039306641,
      "learning_rate": 2.6407877604166668e-06,
      "loss": 2.2621,
      "step": 11640
    },
    {
      "epoch": 2.84423828125,
      "grad_norm": 6.962217330932617,
      "learning_rate": 2.60009765625e-06,
      "loss": 2.148,
      "step": 11650
    },
    {
      "epoch": 2.8466796875,
      "grad_norm": 6.235287189483643,
      "learning_rate": 2.5594075520833335e-06,
      "loss": 1.9192,
      "step": 11660
    },
    {
      "epoch": 2.84912109375,
      "grad_norm": 4.414429187774658,
      "learning_rate": 2.518717447916667e-06,
      "loss": 2.0364,
      "step": 11670
    },
    {
      "epoch": 2.8515625,
      "grad_norm": 3.30438494682312,
      "learning_rate": 2.4780273437500003e-06,
      "loss": 1.8738,
      "step": 11680
    },
    {
      "epoch": 2.85400390625,
      "grad_norm": 4.9447855949401855,
      "learning_rate": 2.4373372395833334e-06,
      "loss": 1.9324,
      "step": 11690
    },
    {
      "epoch": 2.8564453125,
      "grad_norm": 4.913999080657959,
      "learning_rate": 2.396647135416667e-06,
      "loss": 1.8531,
      "step": 11700
    },
    {
      "epoch": 2.85888671875,
      "grad_norm": 6.486142635345459,
      "learning_rate": 2.35595703125e-06,
      "loss": 2.1035,
      "step": 11710
    },
    {
      "epoch": 2.861328125,
      "grad_norm": 6.034538745880127,
      "learning_rate": 2.3152669270833333e-06,
      "loss": 2.1567,
      "step": 11720
    },
    {
      "epoch": 2.86376953125,
      "grad_norm": 7.790440559387207,
      "learning_rate": 2.274576822916667e-06,
      "loss": 1.9725,
      "step": 11730
    },
    {
      "epoch": 2.8662109375,
      "grad_norm": 3.5322341918945312,
      "learning_rate": 2.23388671875e-06,
      "loss": 1.9662,
      "step": 11740
    },
    {
      "epoch": 2.86865234375,
      "grad_norm": 5.653836727142334,
      "learning_rate": 2.193196614583333e-06,
      "loss": 1.7675,
      "step": 11750
    },
    {
      "epoch": 2.87109375,
      "grad_norm": 5.779114723205566,
      "learning_rate": 2.1525065104166668e-06,
      "loss": 1.9779,
      "step": 11760
    },
    {
      "epoch": 2.87353515625,
      "grad_norm": 7.006382465362549,
      "learning_rate": 2.11181640625e-06,
      "loss": 1.8939,
      "step": 11770
    },
    {
      "epoch": 2.8759765625,
      "grad_norm": 4.627727508544922,
      "learning_rate": 2.0711263020833335e-06,
      "loss": 2.094,
      "step": 11780
    },
    {
      "epoch": 2.87841796875,
      "grad_norm": 6.963385105133057,
      "learning_rate": 2.030436197916667e-06,
      "loss": 2.0158,
      "step": 11790
    },
    {
      "epoch": 2.880859375,
      "grad_norm": 4.127467155456543,
      "learning_rate": 1.9897460937500002e-06,
      "loss": 1.967,
      "step": 11800
    },
    {
      "epoch": 2.88330078125,
      "grad_norm": 4.083718776702881,
      "learning_rate": 1.9490559895833334e-06,
      "loss": 1.8204,
      "step": 11810
    },
    {
      "epoch": 2.8857421875,
      "grad_norm": 7.183277606964111,
      "learning_rate": 1.908365885416667e-06,
      "loss": 1.9392,
      "step": 11820
    },
    {
      "epoch": 2.88818359375,
      "grad_norm": 4.141117572784424,
      "learning_rate": 1.8676757812500001e-06,
      "loss": 1.8614,
      "step": 11830
    },
    {
      "epoch": 2.890625,
      "grad_norm": 7.382767200469971,
      "learning_rate": 1.8269856770833333e-06,
      "loss": 2.1325,
      "step": 11840
    },
    {
      "epoch": 2.89306640625,
      "grad_norm": 4.194200038909912,
      "learning_rate": 1.7862955729166669e-06,
      "loss": 1.9425,
      "step": 11850
    },
    {
      "epoch": 2.8955078125,
      "grad_norm": 4.191664218902588,
      "learning_rate": 1.74560546875e-06,
      "loss": 1.9466,
      "step": 11860
    },
    {
      "epoch": 2.89794921875,
      "grad_norm": 6.489378929138184,
      "learning_rate": 1.7049153645833334e-06,
      "loss": 1.8508,
      "step": 11870
    },
    {
      "epoch": 2.900390625,
      "grad_norm": 5.568162441253662,
      "learning_rate": 1.6642252604166668e-06,
      "loss": 2.0301,
      "step": 11880
    },
    {
      "epoch": 2.90283203125,
      "grad_norm": 5.5627288818359375,
      "learning_rate": 1.6235351562500001e-06,
      "loss": 1.9958,
      "step": 11890
    },
    {
      "epoch": 2.9052734375,
      "grad_norm": 7.091019630432129,
      "learning_rate": 1.5828450520833333e-06,
      "loss": 1.9083,
      "step": 11900
    },
    {
      "epoch": 2.90771484375,
      "grad_norm": 4.118227005004883,
      "learning_rate": 1.5421549479166667e-06,
      "loss": 1.8959,
      "step": 11910
    },
    {
      "epoch": 2.91015625,
      "grad_norm": 9.683269500732422,
      "learning_rate": 1.50146484375e-06,
      "loss": 2.0425,
      "step": 11920
    },
    {
      "epoch": 2.91259765625,
      "grad_norm": 8.14868450164795,
      "learning_rate": 1.4607747395833334e-06,
      "loss": 1.8233,
      "step": 11930
    },
    {
      "epoch": 2.9150390625,
      "grad_norm": 7.555147647857666,
      "learning_rate": 1.4200846354166668e-06,
      "loss": 2.078,
      "step": 11940
    },
    {
      "epoch": 2.91748046875,
      "grad_norm": 6.229890823364258,
      "learning_rate": 1.3793945312500001e-06,
      "loss": 1.8894,
      "step": 11950
    },
    {
      "epoch": 2.919921875,
      "grad_norm": 4.453264236450195,
      "learning_rate": 1.3387044270833335e-06,
      "loss": 2.0422,
      "step": 11960
    },
    {
      "epoch": 2.92236328125,
      "grad_norm": 6.027439117431641,
      "learning_rate": 1.2980143229166666e-06,
      "loss": 1.8054,
      "step": 11970
    },
    {
      "epoch": 2.9248046875,
      "grad_norm": 6.588044166564941,
      "learning_rate": 1.25732421875e-06,
      "loss": 2.0612,
      "step": 11980
    },
    {
      "epoch": 2.92724609375,
      "grad_norm": 6.134800910949707,
      "learning_rate": 1.2166341145833334e-06,
      "loss": 1.9953,
      "step": 11990
    },
    {
      "epoch": 2.9296875,
      "grad_norm": 6.7300801277160645,
      "learning_rate": 1.1759440104166668e-06,
      "loss": 1.9049,
      "step": 12000
    },
    {
      "epoch": 2.93212890625,
      "grad_norm": 4.643308162689209,
      "learning_rate": 1.1352539062500001e-06,
      "loss": 1.9759,
      "step": 12010
    },
    {
      "epoch": 2.9345703125,
      "grad_norm": 6.7529778480529785,
      "learning_rate": 1.0945638020833335e-06,
      "loss": 1.9546,
      "step": 12020
    },
    {
      "epoch": 2.93701171875,
      "grad_norm": 7.287583827972412,
      "learning_rate": 1.0538736979166666e-06,
      "loss": 2.131,
      "step": 12030
    },
    {
      "epoch": 2.939453125,
      "grad_norm": 8.013957023620605,
      "learning_rate": 1.01318359375e-06,
      "loss": 1.9797,
      "step": 12040
    },
    {
      "epoch": 2.94189453125,
      "grad_norm": 5.766831874847412,
      "learning_rate": 9.724934895833334e-07,
      "loss": 2.0263,
      "step": 12050
    },
    {
      "epoch": 2.9443359375,
      "grad_norm": 8.266229629516602,
      "learning_rate": 9.318033854166666e-07,
      "loss": 1.9139,
      "step": 12060
    },
    {
      "epoch": 2.94677734375,
      "grad_norm": 13.936187744140625,
      "learning_rate": 8.9111328125e-07,
      "loss": 1.9181,
      "step": 12070
    },
    {
      "epoch": 2.94921875,
      "grad_norm": 5.750120639801025,
      "learning_rate": 8.504231770833335e-07,
      "loss": 1.9743,
      "step": 12080
    },
    {
      "epoch": 2.95166015625,
      "grad_norm": 5.724064350128174,
      "learning_rate": 8.097330729166666e-07,
      "loss": 2.0303,
      "step": 12090
    },
    {
      "epoch": 2.9541015625,
      "grad_norm": 4.531826019287109,
      "learning_rate": 7.6904296875e-07,
      "loss": 1.9291,
      "step": 12100
    },
    {
      "epoch": 2.95654296875,
      "grad_norm": 5.750802516937256,
      "learning_rate": 7.283528645833334e-07,
      "loss": 2.0171,
      "step": 12110
    },
    {
      "epoch": 2.958984375,
      "grad_norm": 4.048965930938721,
      "learning_rate": 6.876627604166667e-07,
      "loss": 1.7367,
      "step": 12120
    },
    {
      "epoch": 2.96142578125,
      "grad_norm": 7.2666449546813965,
      "learning_rate": 6.4697265625e-07,
      "loss": 1.9877,
      "step": 12130
    },
    {
      "epoch": 2.9638671875,
      "grad_norm": 4.041323661804199,
      "learning_rate": 6.062825520833334e-07,
      "loss": 2.133,
      "step": 12140
    },
    {
      "epoch": 2.96630859375,
      "grad_norm": 5.260027885437012,
      "learning_rate": 5.655924479166667e-07,
      "loss": 1.9329,
      "step": 12150
    },
    {
      "epoch": 2.96875,
      "grad_norm": 4.404916763305664,
      "learning_rate": 5.2490234375e-07,
      "loss": 1.8591,
      "step": 12160
    },
    {
      "epoch": 2.97119140625,
      "grad_norm": 3.9609625339508057,
      "learning_rate": 4.842122395833334e-07,
      "loss": 2.1246,
      "step": 12170
    },
    {
      "epoch": 2.9736328125,
      "grad_norm": 6.876498699188232,
      "learning_rate": 4.4352213541666674e-07,
      "loss": 1.9129,
      "step": 12180
    },
    {
      "epoch": 2.97607421875,
      "grad_norm": 5.439281463623047,
      "learning_rate": 4.0283203125e-07,
      "loss": 1.9357,
      "step": 12190
    },
    {
      "epoch": 2.978515625,
      "grad_norm": 5.739552974700928,
      "learning_rate": 3.6214192708333337e-07,
      "loss": 1.9486,
      "step": 12200
    },
    {
      "epoch": 2.98095703125,
      "grad_norm": 5.236711025238037,
      "learning_rate": 3.214518229166667e-07,
      "loss": 2.0497,
      "step": 12210
    },
    {
      "epoch": 2.9833984375,
      "grad_norm": 4.622352123260498,
      "learning_rate": 2.8076171875e-07,
      "loss": 1.9788,
      "step": 12220
    },
    {
      "epoch": 2.98583984375,
      "grad_norm": 5.3521013259887695,
      "learning_rate": 2.4007161458333336e-07,
      "loss": 2.0054,
      "step": 12230
    },
    {
      "epoch": 2.98828125,
      "grad_norm": 4.132733345031738,
      "learning_rate": 1.9938151041666665e-07,
      "loss": 1.95,
      "step": 12240
    },
    {
      "epoch": 2.99072265625,
      "grad_norm": 6.90182638168335,
      "learning_rate": 1.5869140625000002e-07,
      "loss": 2.0,
      "step": 12250
    },
    {
      "epoch": 2.9931640625,
      "grad_norm": 4.348954677581787,
      "learning_rate": 1.1800130208333333e-07,
      "loss": 1.9933,
      "step": 12260
    },
    {
      "epoch": 2.99560546875,
      "grad_norm": 5.565003395080566,
      "learning_rate": 7.731119791666666e-08,
      "loss": 1.8457,
      "step": 12270
    },
    {
      "epoch": 2.998046875,
      "grad_norm": 4.8259053230285645,
      "learning_rate": 3.6621093750000003e-08,
      "loss": 2.0478,
      "step": 12280
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.7818667888641357,
      "eval_runtime": 6.4035,
      "eval_samples_per_second": 19.989,
      "eval_steps_per_second": 19.989,
      "step": 12288
    }
  ],
  "logging_steps": 10,
  "max_steps": 12288,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 810357249540096.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
